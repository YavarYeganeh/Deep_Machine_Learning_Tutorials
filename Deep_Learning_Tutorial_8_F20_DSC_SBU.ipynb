{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep-Learning_Tutorial_8_F20_DSC_SBU",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "14d9c825ff904a08a26bb2e8d6468489": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c505701b382d4d0795d6685d00d638be",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a41f180f30714b6c9b79bc4881e4b18d",
              "IPY_MODEL_f17dc10c6d3f4792b70902e169b641c1"
            ]
          },
          "model_module_version": "1.5.0"
        },
        "c505701b382d4d0795d6685d00d638be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "a41f180f30714b6c9b79bc4881e4b18d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cb92e19d82844e0fb1a5b699288ef503",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 46827520,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 46827520,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1935d5ffd0974463929d2bd88ddc9460"
          },
          "model_module_version": "1.5.0"
        },
        "f17dc10c6d3f4792b70902e169b641c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e478f8c136c14d269ebaa4e4f99f403e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 44.7M/44.7M [20:39&lt;00:00, 37.8kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a3fafc2491e24860a8628d8f33709690"
          },
          "model_module_version": "1.5.0"
        },
        "cb92e19d82844e0fb1a5b699288ef503": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "1935d5ffd0974463929d2bd88ddc9460": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "e478f8c136c14d269ebaa4e4f99f403e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "a3fafc2491e24860a8628d8f33709690": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "d6022294f4ff46d6a25bc019ddfc1577": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6cb2892014824418bf412e6c05535d8f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_404c943b2fc4413cbfdf75497b762824",
              "IPY_MODEL_e1965b1062bb438aabfb36eeae01b3d2"
            ]
          },
          "model_module_version": "1.5.0"
        },
        "6cb2892014824418bf412e6c05535d8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "404c943b2fc4413cbfdf75497b762824": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b6981c121fa246e899c809fe1e49064a",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 244418560,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 244418560,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_149d8f9007d141668222b6f0b1b23053"
          },
          "model_module_version": "1.5.0"
        },
        "e1965b1062bb438aabfb36eeae01b3d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_19ebbc413c824e3caa8474b49c292fab",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 233M/233M [00:42&lt;00:00, 5.70MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4e578dbdcecd43e9912dd1e787ebd58e"
          },
          "model_module_version": "1.5.0"
        },
        "b6981c121fa246e899c809fe1e49064a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "149d8f9007d141668222b6f0b1b23053": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        },
        "19ebbc413c824e3caa8474b49c292fab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          },
          "model_module_version": "1.5.0"
        },
        "4e578dbdcecd43e9912dd1e787ebd58e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          },
          "model_module_version": "1.2.0"
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9ccTlE-X9Io"
      },
      "source": [
        "**Deep Learning Tutorial**, Aban 20 at 18:00 via Skype \n",
        "Class Room, DL Recitation, Session Eight\n",
        "\n",
        "[Deep Learning](https://github.com/hhaji/Deep-Learning), Data Science Center, Shahid Beheshti University\n",
        "\n",
        "Presented and prepared by [‌‌Yavar Yeganeh](https://github.com/YavarYeganeh)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMe8PdaaZfdK"
      },
      "source": [
        "\n",
        "\n",
        "**Acknowledgment and References:**\n",
        "\n",
        "* [Pytorch](www.Pytorch.org)\n",
        "\n",
        "* Others listed throughout the notebook\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVRUS6JANEa6"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=https://colab.research.google.com/drive/1cass1LqzpmTs1dnasSzgHnvuSnHdODZQ?usp=sharing\"><img src=\"https://colab.research.google.com/img/colab_favicon_256px.png\" height=\"100\" width=\"100\" /> <br>  Run in Google Colab</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMkvYXd0Lm1V"
      },
      "source": [
        "\n",
        "We will discuss:\n",
        "\n",
        "- [torchvision.models](https://pytorch.org/docs/stable/torchvision/models.html)\n",
        "- Regularization for Deep Learning (Part two): Bagging and Dropout\n",
        "- Optimization for Training Deep Models: Different Optimizers and Batch Normalization\n",
        "- **Training** a fully connected neural networks without using machine learning libraries in Python (I'll send a tutorial for it)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERnF0jls3Mvy"
      },
      "source": [
        "A great repository for Pytorch related stuff:\n",
        "\n",
        "* [**the-incredible-pytorch**](https://www.ritchieng.com/the-incredible-pytorch/#Optimization): https://github.com/ritchieng/the-incredible-pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_8vhmpMlGBI"
      },
      "source": [
        "[**torchvision.models**]([torchvision.models](https://pytorch.org/docs/stable/torchvision/models.html)): \n",
        "\n",
        "The models subpackage contains definitions of models for addressing different tasks, including: image classification, pixelwise semantic segmentation, object detection, instance segmentation, person keypoint detection and video classification.\n",
        "\n",
        "Classification: The models subpackage contains definitions for the following model architectures for image classification:\n",
        "\n",
        "    AlexNet\n",
        "\n",
        "    VGG\n",
        "\n",
        "    ResNet\n",
        "\n",
        "    SqueezeNet\n",
        "\n",
        "    DenseNet\n",
        "\n",
        "    Inception v3\n",
        "\n",
        "    GoogLeNet\n",
        "\n",
        "    ShuffleNet v2\n",
        "\n",
        "    MobileNet v2\n",
        "\n",
        "    ResNeXt\n",
        "\n",
        "    Wide ResNet\n",
        "\n",
        "    MNASNet\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd1meroBo850"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUTfEHCso8FZ",
        "outputId": "037a6e9e-b08b-46fb-c2df-70cbb358ed6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "14d9c825ff904a08a26bb2e8d6468489",
            "c505701b382d4d0795d6685d00d638be",
            "a41f180f30714b6c9b79bc4881e4b18d",
            "f17dc10c6d3f4792b70902e169b641c1",
            "cb92e19d82844e0fb1a5b699288ef503",
            "1935d5ffd0974463929d2bd88ddc9460",
            "e478f8c136c14d269ebaa4e4f99f403e",
            "a3fafc2491e24860a8628d8f33709690"
          ]
        }
      },
      "source": [
        "import torchvision.models as models\n",
        "resnet18 = models.resnet18()\n",
        "resnet18_pt = models.resnet18(pretrained=True)\n",
        "#alexnet = models.alexnet()\n",
        "#vgg16 = models.vgg16()\n",
        "#squeezenet = models.squeezenet1_0()\n",
        "#densenet = models.densenet161()\n",
        "#inception = models.inception_v3()\n",
        "#googlenet = models.googlenet()\n",
        "#shufflenet = models.shufflenet_v2_x1_0()\n",
        "#mobilenet = models.mobilenet_v2()\n",
        "#resnext50_32x4d = models.resnext50_32x4d()\n",
        "#wide_resnet50_2 = models.wide_resnet50_2()\n",
        "#mnasnet = models.mnasnet1_0()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14d9c825ff904a08a26bb2e8d6468489",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uNSE1FxmrR_",
        "outputId": "7ceb23df-fa99-4e26-882b-548506f7b9a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "resnet18"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeZHH1RamwyX",
        "outputId": "d66fa84b-19e8-4aa0-822d-ae9ba9a8a438",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "resnet18_pt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt7UI1Djm2TK",
        "outputId": "5a2d8672-68e0-4502-fd59-b8c2d5698b06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "d6022294f4ff46d6a25bc019ddfc1577",
            "6cb2892014824418bf412e6c05535d8f",
            "404c943b2fc4413cbfdf75497b762824",
            "e1965b1062bb438aabfb36eeae01b3d2",
            "b6981c121fa246e899c809fe1e49064a",
            "149d8f9007d141668222b6f0b1b23053",
            "19ebbc413c824e3caa8474b49c292fab",
            "4e578dbdcecd43e9912dd1e787ebd58e"
          ]
        }
      },
      "source": [
        "alexnet = models.alexnet()\n",
        "alexnet_pt = models.alexnet(pretrained=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-4df8aa71.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6022294f4ff46d6a25bc019ddfc1577",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=244418560.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C77FrRLDol54",
        "outputId": "74a8cced-f415-4ab3-bd81-05a06d37d965",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "alexnet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34Jmfny0oklH",
        "outputId": "5e0287c9-f8d3-4087-ef00-803478c47897",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "alexnet_pt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6RGJlGrrYFt"
      },
      "source": [
        "sample_input=torch.rand(1,3,224,224)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKbHcZUUsF1-",
        "outputId": "2a2f5d1b-aaa5-481f-a8cb-a4e0023c7f08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(resnet18(sample_input))\n",
        "print(resnet18_pt(sample_input))\n",
        "print(alexnet(sample_input))\n",
        "print(alexnet_pt(sample_input))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-3.7480e-01,  1.7617e-01, -5.9083e-01, -4.7130e-01, -6.8953e-01,\n",
            "          9.0782e-02, -4.4005e-02, -4.7955e-01, -2.5937e-01,  2.0701e-01,\n",
            "          1.7559e-01, -1.0990e+00,  6.9854e-01, -1.7944e+00, -9.0110e-01,\n",
            "         -9.4805e-01, -9.3713e-01, -5.1444e-01,  1.8877e-01, -7.2581e-02,\n",
            "         -3.5521e-01, -3.7642e-01,  3.6675e-01,  4.4044e-01,  5.4703e-03,\n",
            "         -5.7668e-01,  5.8538e-01, -1.2001e+00, -2.2794e-01,  5.8323e-01,\n",
            "         -1.5892e-01, -2.2846e-02, -7.6490e-01, -9.5614e-01,  8.2606e-01,\n",
            "          1.9579e-01, -4.0407e-01, -2.6577e-01,  3.5535e-01, -2.9903e-01,\n",
            "         -1.1595e+00, -1.5173e-01, -1.8430e-01,  1.4570e-01, -3.2693e-01,\n",
            "          6.6518e-01,  7.7033e-01,  1.1847e-01,  3.7505e-01, -5.9252e-01,\n",
            "         -4.1773e-03,  4.5659e-01,  2.3347e-01, -1.0798e-01, -4.1171e-01,\n",
            "          8.8393e-01, -3.8764e-01,  2.4220e-01, -4.6403e-01,  9.5299e-01,\n",
            "          3.5307e-02,  3.0320e-01,  3.9352e-01, -3.9524e-01,  1.9584e-01,\n",
            "          2.4106e-02,  1.4316e-01,  6.8018e-01,  1.6121e-01,  9.3077e-02,\n",
            "         -8.2159e-01,  9.9246e-01,  5.6225e-01,  7.6056e-01,  6.6588e-01,\n",
            "          8.6197e-01,  2.4204e-01,  1.4577e-01,  2.5069e-01,  6.5059e-01,\n",
            "         -3.2229e-01,  1.0866e-01,  7.3283e-01, -8.4559e-01, -3.5744e-01,\n",
            "         -8.7290e-01,  3.5732e-02,  2.8626e-01, -3.4907e-01,  2.0428e-01,\n",
            "         -4.2446e-01, -7.2198e-02, -8.7906e-01, -7.8200e-01,  5.0339e-02,\n",
            "         -6.1634e-01, -1.3822e-01, -5.5911e-01,  9.6586e-01,  4.1494e-01,\n",
            "          7.9085e-01, -1.8149e-01,  7.6510e-01, -3.4195e-01, -4.3857e-01,\n",
            "         -5.4260e-01,  4.6283e-01,  3.4545e-01,  4.6278e-01, -2.2001e-02,\n",
            "          7.3361e-01, -8.9260e-01, -1.0515e-01,  2.2845e-01, -2.8788e-01,\n",
            "         -1.9606e-01, -4.8433e-01, -4.1775e-01,  7.1746e-02,  3.8223e-01,\n",
            "         -6.4396e-01,  6.8465e-01,  4.2376e-01, -5.5450e-01, -2.7501e-01,\n",
            "         -8.6252e-02,  7.6682e-01, -1.7933e-01,  2.6134e-01,  5.9667e-01,\n",
            "         -1.3882e-01, -4.6617e-01,  1.4170e+00, -3.3128e-01,  6.1633e-01,\n",
            "          3.2676e-02,  1.4134e-01,  1.7914e-01, -4.1463e-01,  6.6888e-01,\n",
            "          2.9591e-01,  8.1245e-02, -1.3089e-01, -1.1862e-01, -4.8460e-01,\n",
            "          3.6805e-01, -1.0900e-01, -1.1434e-01,  1.2625e-01, -3.7831e-01,\n",
            "          2.6240e-01,  1.6831e-01,  8.2111e-01,  1.1613e+00, -1.5556e-01,\n",
            "          3.6158e-01, -9.2544e-02, -4.3077e-01, -6.6870e-01,  4.2714e-01,\n",
            "         -7.2189e-02, -1.3300e+00,  6.4484e-01,  4.6052e-01, -3.2078e-01,\n",
            "          2.0638e-02, -1.0028e+00,  3.7909e-01, -3.4182e-01, -8.1267e-01,\n",
            "          5.2879e-01,  3.2810e-01,  3.2112e-01, -3.2522e-02,  6.4134e-01,\n",
            "          1.9382e-01, -2.3635e-01,  3.2635e-01, -6.9831e-02, -8.8821e-02,\n",
            "         -3.6521e-01, -3.6273e-01,  4.2053e-02,  8.1621e-01,  7.5760e-01,\n",
            "          3.8808e-02,  8.8133e-02, -1.9354e-01, -4.6968e-02,  2.7627e-01,\n",
            "         -5.0665e-01, -9.0465e-01,  4.1514e-01,  9.1038e-01,  1.2201e-02,\n",
            "          3.1323e-01, -2.6420e-01,  3.7499e-01, -7.6025e-01, -2.6538e-01,\n",
            "          3.8659e-01, -6.1036e-01,  7.7844e-01,  5.7824e-01, -1.4778e-01,\n",
            "         -1.3283e-01,  1.5448e-01, -4.0686e-01, -6.6625e-01,  8.8176e-01,\n",
            "          1.4166e-01,  8.8334e-01, -9.3192e-02,  3.1086e-01, -3.4804e-01,\n",
            "         -3.3837e-01, -6.4678e-03, -1.0002e-01,  1.0641e+00, -1.8265e-01,\n",
            "         -1.1213e-01, -4.8416e-02,  2.3535e-01, -2.8381e-01,  2.2876e-01,\n",
            "          1.8348e-01,  2.2846e-01, -1.3188e-01, -4.4004e-01,  1.5790e-02,\n",
            "         -1.0737e-01,  1.4659e-01, -1.3718e-01, -4.3367e-01, -1.2695e+00,\n",
            "         -8.8018e-01,  9.4146e-01, -5.0216e-02,  9.5078e-02,  8.5243e-01,\n",
            "         -5.0288e-01, -1.8262e-01, -6.1949e-01,  5.2820e-01, -6.4254e-02,\n",
            "         -1.0649e-01,  2.7173e-01, -9.0438e-01,  5.4450e-01, -1.2803e-01,\n",
            "          1.2240e-01, -4.4521e-01,  5.6568e-01, -8.6433e-02, -1.2760e+00,\n",
            "          7.3186e-01,  5.8342e-01,  9.5274e-03,  3.9081e-01, -3.6625e-01,\n",
            "         -4.6141e-01, -8.5431e-01,  3.4332e-02, -3.0651e-01,  5.0239e-02,\n",
            "         -3.3473e-01,  5.5352e-01, -2.1560e-01, -6.5772e-01,  1.0210e+00,\n",
            "          6.9885e-01,  2.9339e-01, -9.1740e-01,  1.2661e-01,  6.0697e-01,\n",
            "          6.0873e-01, -1.8117e-01,  1.6749e-01, -4.4137e-01, -8.6677e-02,\n",
            "         -4.1258e-01,  8.7817e-01, -6.5408e-01, -7.5443e-01,  1.9365e-01,\n",
            "         -4.1643e-01,  3.5751e-01,  3.8083e-01, -9.5167e-02, -1.4758e-01,\n",
            "         -4.7022e-01, -1.7854e-01, -5.8534e-01, -7.6010e-01,  2.5068e-01,\n",
            "         -8.2764e-01,  1.9907e-01, -5.1194e-02, -2.3130e-01, -1.6735e-01,\n",
            "         -7.0422e-01, -8.8671e-01,  6.3348e-02,  5.9316e-01, -2.2435e-01,\n",
            "         -1.1225e-01,  5.9088e-02, -8.1786e-02, -3.5517e-01,  4.8355e-01,\n",
            "         -6.1975e-01,  1.3534e-01,  2.5004e-01,  5.9763e-01, -2.8607e-01,\n",
            "         -3.9741e-01,  7.3421e-01,  3.4776e-01,  5.8614e-01,  6.9844e-01,\n",
            "         -1.2504e-01, -2.0082e-01,  2.8568e-01, -1.8935e-01, -7.5586e-01,\n",
            "         -4.5528e-01,  2.1297e-01,  5.5294e-01,  2.2505e-01,  1.1979e-01,\n",
            "          4.4388e-01,  1.9454e-01,  7.8703e-01,  3.5629e-01, -9.8211e-02,\n",
            "         -8.2851e-01,  3.1307e-01,  6.3487e-02,  6.8353e-01,  1.1932e-01,\n",
            "          3.3556e-01, -1.4469e-01, -2.1156e-01, -1.2571e-01, -1.2037e-02,\n",
            "         -2.5365e-01, -4.6531e-01, -3.9890e-01, -3.4861e-02, -5.0036e-01,\n",
            "         -5.6318e-02,  4.5078e-01, -5.8738e-02, -4.0350e-01,  5.8401e-01,\n",
            "          5.3261e-02,  2.0684e-01,  2.6775e-02,  1.7530e-01, -8.4064e-01,\n",
            "         -5.9898e-02, -4.7116e-01, -2.7375e-01, -2.7335e-01, -7.8314e-02,\n",
            "          2.6615e-01, -4.6257e-01, -1.7575e-01,  5.9245e-01,  2.4365e-01,\n",
            "         -5.3662e-02, -1.6260e-01, -1.4936e-01,  4.2561e-01,  7.4055e-02,\n",
            "          1.7351e-01,  6.3917e-01,  1.1231e-01,  4.8407e-01,  1.0230e-01,\n",
            "          4.1881e-01, -5.1369e-01, -1.8273e-01,  6.5312e-01, -5.7997e-01,\n",
            "         -2.4771e-01, -1.3723e-01, -6.4875e-02, -1.4613e-01,  9.3664e-03,\n",
            "          1.3509e-01, -7.7290e-01, -3.3026e-01,  3.4079e-01, -3.8656e-01,\n",
            "         -4.6202e-01,  5.2314e-01, -6.7874e-03,  1.0556e+00, -8.9007e-01,\n",
            "          8.9446e-02,  2.3519e-01, -1.2501e-01, -3.1640e-01,  7.2492e-01,\n",
            "          1.9940e-01,  3.2654e-01, -1.4016e-01, -2.0119e-01,  5.8928e-01,\n",
            "         -4.0113e-01,  4.5737e-02,  3.5290e-01, -1.7714e-01,  9.6122e-01,\n",
            "         -8.2836e-01,  1.3820e-01,  7.3026e-01, -5.8683e-02, -1.1158e-01,\n",
            "         -1.6367e-01, -1.8414e-01,  2.7813e-01, -3.3650e-01,  6.7156e-01,\n",
            "          1.1107e+00,  3.9535e-01, -2.9343e-01, -2.9922e-01, -4.5602e-01,\n",
            "         -1.0631e-02, -2.5795e-01,  3.5468e-01,  3.6865e-01, -2.0762e-01,\n",
            "          8.2054e-01, -1.1822e-01, -4.6269e-01,  3.6590e-01,  5.9636e-01,\n",
            "         -8.6935e-02, -3.0846e-01, -1.2448e-02, -2.2436e-01,  5.3856e-01,\n",
            "          9.9827e-03,  2.0880e-01,  7.2058e-01,  4.6582e-02,  2.8972e-02,\n",
            "          8.8615e-01,  6.6082e-01,  5.5637e-01, -1.2002e-01,  1.1771e-01,\n",
            "         -2.3432e-01, -1.6990e-01, -8.8665e-01, -6.5345e-01, -1.9986e-01,\n",
            "          3.4698e-01, -2.6549e-01, -3.6864e-01,  3.0146e-01, -2.8650e-01,\n",
            "          5.4670e-02,  7.0407e-02, -2.8197e-01, -2.2601e-01,  2.0023e-01,\n",
            "         -6.1286e-01,  4.6687e-01, -1.8878e-01,  8.5737e-01,  5.8061e-01,\n",
            "          5.1034e-01, -6.4163e-02, -1.5923e-01,  1.0806e+00,  4.2842e-01,\n",
            "         -1.0807e+00, -4.9134e-01, -7.2540e-01,  4.7119e-01, -5.8812e-02,\n",
            "          2.7414e-01,  7.2449e-01, -1.6577e-01, -2.1085e-01, -1.6071e-01,\n",
            "         -8.3187e-02, -6.9407e-01,  5.0800e-02, -6.1842e-01,  3.6702e-02,\n",
            "          1.2485e-02, -3.1911e-01,  3.8099e-01, -3.4038e-02,  8.1477e-01,\n",
            "         -3.2389e-01, -7.5473e-01, -1.0613e+00, -4.4366e-01, -5.7051e-01,\n",
            "         -2.4230e-01, -5.1570e-01,  2.9485e-01, -3.1633e-01,  3.3485e-01,\n",
            "         -2.2927e-01,  7.2325e-01, -3.7185e-01,  6.5601e-02,  9.5217e-02,\n",
            "          7.5577e-01,  4.3118e-01, -1.3079e+00,  2.8140e-01,  1.3955e-01,\n",
            "          3.5393e-01, -4.3578e-01,  5.6744e-01, -1.3350e+00, -7.7335e-01,\n",
            "          7.2883e-01, -3.4613e-02,  1.6134e-01, -9.7417e-01, -7.5202e-02,\n",
            "          1.7969e-01,  4.0745e-03,  1.3468e+00,  1.4543e-01, -8.9252e-01,\n",
            "         -1.7207e-01,  1.9776e-01, -6.1865e-01, -9.2325e-01,  4.1659e-01,\n",
            "         -3.9110e-01, -1.8438e-01,  1.3150e-01, -5.7202e-01, -5.0146e-01,\n",
            "          1.8540e-01,  4.4876e-02,  3.0103e-01,  5.1958e-01,  8.8415e-02,\n",
            "         -2.2578e-02,  1.0692e-01, -5.4974e-02,  7.4053e-01,  1.3454e-01,\n",
            "          7.0613e-01, -2.9784e-01,  5.7026e-02, -3.8327e-01, -7.3989e-01,\n",
            "          1.4034e+00, -6.4494e-01, -2.1416e-01, -3.5321e-01, -1.7127e-01,\n",
            "          9.3111e-01,  5.6653e-02, -2.9253e-01, -3.1282e-01, -4.6750e-02,\n",
            "          4.1258e-01,  5.9559e-01,  4.7057e-01,  1.3441e+00,  2.0440e-01,\n",
            "          5.8610e-02, -4.8343e-01,  1.6798e-01,  3.8524e-01,  4.6989e-01,\n",
            "         -3.0050e-01, -6.8861e-02,  1.0316e+00,  2.5543e-01, -8.0507e-01,\n",
            "         -7.6556e-02,  1.3059e+00,  1.4805e-01,  7.9728e-02, -7.8661e-01,\n",
            "         -6.5244e-01,  2.1609e-01,  4.2925e-01,  6.1984e-01,  1.2643e-01,\n",
            "          3.1271e-02, -1.8955e-01, -2.5966e-01,  3.0767e-01,  4.3387e-01,\n",
            "         -4.4421e-02,  2.2845e-01,  4.7112e-01,  2.9387e-01, -3.6783e-01,\n",
            "          3.5822e-01,  2.0326e-01, -1.6330e-01, -3.8926e-01,  2.7901e-01,\n",
            "          2.3379e-01, -2.4468e-02, -8.7784e-02, -1.2972e-01, -4.9485e-01,\n",
            "         -8.0667e-01, -3.3797e-02,  2.2615e-01,  1.0926e+00, -5.2936e-01,\n",
            "          2.7048e-01, -2.8808e-01,  4.6551e-01, -2.4401e-01, -1.6610e-01,\n",
            "         -1.3614e+00, -2.3279e-01,  9.7038e-02,  3.2437e-01, -2.3838e-01,\n",
            "         -7.3773e-02,  3.0578e-01,  9.3638e-01,  2.1483e-01, -4.0658e-01,\n",
            "         -5.5195e-01, -5.9653e-01,  4.6334e-01,  1.8203e-01,  5.9207e-02,\n",
            "         -1.4789e-02,  8.7558e-02,  4.7403e-01,  1.1180e-01,  1.4926e-01,\n",
            "         -1.4756e-03, -3.1178e-03,  1.2095e+00, -5.6404e-01, -3.3088e-01,\n",
            "         -1.8919e-02,  3.4407e-02, -6.7331e-01,  2.3701e-01, -8.2962e-01,\n",
            "         -5.3505e-01,  2.4656e-02,  7.9667e-01,  2.6142e-01, -4.4729e-01,\n",
            "         -3.2617e-01,  2.0329e-01,  1.4862e-01,  6.1006e-01,  2.9383e-01,\n",
            "          4.7586e-01,  6.7443e-01, -4.5389e-01,  1.7986e-01, -5.7451e-01,\n",
            "          6.3613e-01, -2.1618e-01,  1.0864e-01,  5.7087e-01, -1.6961e-01,\n",
            "          3.0593e-01, -6.2310e-01,  2.2427e-01,  7.1570e-02,  1.1729e-01,\n",
            "         -1.0337e+00,  1.1331e-02,  8.8466e-01, -3.7566e-01,  9.8954e-02,\n",
            "          1.2308e-01,  2.7942e-01, -1.0155e-01,  2.3073e-01, -6.7383e-02,\n",
            "          9.5772e-01, -3.3283e-01, -5.6719e-02,  2.2820e-01, -4.1393e-01,\n",
            "         -2.3349e-01, -4.2210e-01,  3.6501e-01,  9.9589e-01, -1.4367e-01,\n",
            "         -1.7153e-01, -2.3058e-01,  8.2437e-02,  2.9841e-01, -6.3713e-02,\n",
            "         -2.4345e-01, -1.1457e-01,  4.0894e-01,  9.8938e-01,  5.2090e-02,\n",
            "          4.5258e-01,  7.8978e-01, -1.0574e-02, -6.8047e-02, -3.8313e-01,\n",
            "         -9.5044e-02, -1.2346e-01, -6.8749e-02,  7.8229e-01, -5.8112e-01,\n",
            "         -1.7355e-01, -1.8121e-01,  2.7021e-02,  1.5749e-01, -7.1379e-01,\n",
            "          2.2678e-01,  1.1132e+00,  3.0821e-01,  3.1933e-01, -1.7477e-01,\n",
            "         -3.0333e-01,  7.4054e-02, -5.2311e-01, -2.6320e-01,  2.1487e-02,\n",
            "          3.1863e-01, -2.1998e-01, -4.4395e-03, -7.2138e-01, -6.0070e-01,\n",
            "         -4.4640e-01,  4.6916e-01, -6.8727e-01, -1.3413e-01,  3.7518e-01,\n",
            "         -9.6442e-01,  8.4755e-01,  5.6133e-01,  2.4908e-01,  1.5441e-01,\n",
            "         -4.8652e-01, -6.0353e-01,  3.2674e-01,  5.7850e-01,  2.9115e-01,\n",
            "          4.2015e-02,  2.7713e-02, -7.3127e-01,  4.4343e-01, -2.0914e-01,\n",
            "          3.3381e-01,  4.3869e-02,  5.9354e-01,  4.3706e-01, -8.3653e-02,\n",
            "         -2.1381e-01,  4.7356e-01, -6.7668e-01, -4.9778e-01, -2.8190e-01,\n",
            "         -7.1302e-01,  4.7977e-01, -7.3406e-01, -4.6533e-02, -3.8241e-01,\n",
            "         -6.8297e-02, -9.3585e-02, -5.0076e-02,  1.2627e-01,  9.6241e-02,\n",
            "         -9.7694e-01,  9.2982e-01, -4.1150e-01, -2.0516e-01, -2.3450e-02,\n",
            "         -1.0234e+00, -1.0493e+00, -2.5272e-01, -2.1290e-01,  1.3309e-01,\n",
            "         -8.0075e-01,  8.7926e-01, -9.3456e-01,  3.2007e-01, -6.0594e-01,\n",
            "          9.0279e-02, -1.0325e+00, -9.2683e-01, -7.0981e-02, -3.3099e-01,\n",
            "          1.8931e-01,  9.4097e-01,  9.6532e-02,  8.1532e-02, -1.9758e-01,\n",
            "         -3.5474e-01, -5.3402e-01,  5.2566e-02, -8.4980e-02,  6.3036e-02,\n",
            "          6.0085e-02, -2.3264e-01, -3.9448e-01,  3.0349e-01, -4.3072e-01,\n",
            "         -4.9742e-01, -4.6732e-01, -2.9158e-02, -8.2823e-02,  2.6120e-01,\n",
            "          4.2359e-01,  5.0963e-01,  7.8900e-01,  5.0778e-01, -1.0065e-01,\n",
            "          2.7728e-01, -7.9974e-01, -1.3608e+00, -6.0658e-01, -7.9287e-01,\n",
            "         -1.5459e-01, -2.9128e-01,  1.8357e-01, -1.3437e-01, -1.2009e-01,\n",
            "         -3.3686e-01, -7.0255e-01,  1.9326e-01, -4.6834e-01,  4.7506e-01,\n",
            "         -1.7119e-01, -2.1320e-01,  5.5024e-01,  3.9669e-02, -4.9957e-01,\n",
            "         -2.1464e-01, -9.0781e-01, -4.0770e-01,  3.3485e-01,  8.0494e-01,\n",
            "         -5.5923e-01,  1.0793e-01,  5.5382e-01, -3.8398e-01, -2.1294e-01,\n",
            "         -7.0048e-01,  4.1899e-01, -4.1646e-01,  1.9459e-01, -3.0636e-01,\n",
            "          6.5776e-02, -1.2009e-01,  3.7509e-01,  2.7209e-02,  6.0875e-01,\n",
            "          3.0513e-02, -2.1169e-03,  4.5875e-01,  2.2736e-01, -7.1281e-02,\n",
            "          4.8177e-01, -4.0227e-02, -4.4529e-02,  3.2296e-01, -4.6204e-01,\n",
            "          7.8652e-01,  5.9596e-03,  4.4064e-01,  4.6447e-02, -3.0611e-01,\n",
            "         -9.6375e-01,  7.1633e-02,  8.9036e-01,  2.4717e-01, -8.1758e-01,\n",
            "          1.6463e-01,  6.4066e-01,  3.8774e-01,  3.0308e-03, -3.1381e-01,\n",
            "         -3.0310e-01,  6.4464e-01, -9.9470e-02, -9.4089e-02,  2.7412e-01,\n",
            "         -5.0566e-01,  2.8962e-01,  5.5026e-01,  1.4330e-01,  4.9613e-01,\n",
            "         -3.0057e-01,  8.6906e-01, -4.3218e-01, -4.7848e-01,  3.9884e-01,\n",
            "          1.0541e+00, -2.5967e-01, -6.7256e-01,  4.9121e-01,  6.5776e-03,\n",
            "         -7.3174e-02, -4.9903e-01, -8.5715e-02,  8.8798e-01,  3.8294e-01,\n",
            "          1.5272e-02, -1.1344e-01, -7.0513e-02, -8.7950e-02, -6.6775e-01,\n",
            "         -2.4798e-02, -4.6987e-01, -3.3465e-01,  3.2012e-02, -1.4812e-01,\n",
            "          3.9077e-01, -6.2193e-01, -4.7333e-02, -5.9018e-02,  9.8906e-01,\n",
            "          4.5623e-03, -4.9434e-01, -4.1396e-01, -1.8952e-01,  6.2763e-01,\n",
            "          4.2831e-01,  3.1622e-01, -6.0262e-02,  1.1890e-01,  5.1384e-02,\n",
            "         -1.1467e-01, -1.1038e-01, -6.4407e-02, -2.5978e-01,  7.0077e-01,\n",
            "         -1.8103e-01, -9.4357e-02,  2.7024e-01, -3.7613e-01, -5.2379e-01,\n",
            "          2.9815e-01, -2.4881e-01, -1.8790e-01, -6.7234e-02, -2.8477e-01,\n",
            "          4.4814e-01,  1.4000e-01, -4.4174e-01, -4.7216e-02,  1.2374e-01,\n",
            "         -8.5639e-01, -6.9557e-02, -1.2814e+00, -1.1674e-01,  6.8893e-01,\n",
            "         -5.7538e-01, -3.0099e-01, -6.2540e-01,  1.0731e-02, -2.0780e-01,\n",
            "         -1.0160e-01, -2.8497e-01,  6.9030e-01,  2.2734e-01, -7.2042e-01,\n",
            "          1.3759e-01, -1.8630e-01,  3.2362e-01,  7.0339e-02,  5.2370e-02,\n",
            "         -5.5643e-01,  2.7054e-01,  5.6322e-01,  5.2354e-02, -4.4979e-01,\n",
            "         -4.7779e-01, -7.0877e-02,  7.5537e-01, -3.0523e-01, -7.0052e-01,\n",
            "          4.4397e-01,  6.1000e-02,  7.9205e-01,  3.5431e-01,  3.9736e-02,\n",
            "          5.3561e-01,  1.1157e+00,  8.3831e-01,  2.7935e-01,  4.8554e-01]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor([[-0.5307, -0.3697, -0.4275, -1.4376, -0.6175, -0.0774, -0.4448,  0.4222,\n",
            "          0.3477, -0.7269, -1.0338, -0.8267, -0.2801, -0.9619, -1.1389, -0.6578,\n",
            "         -0.9314, -0.3168, -0.6503, -0.6000, -1.6086, -0.6778, -1.4109,  0.1527,\n",
            "         -0.8626, -1.2284, -0.8950, -1.1568, -0.9758, -0.4503, -0.8602, -0.8639,\n",
            "         -0.5556, -0.5296, -0.4841, -0.5162,  0.5049, -0.7150, -0.5406, -0.0827,\n",
            "         -0.7580, -0.9904, -1.2008, -0.3527, -0.7642, -0.5460, -0.7654, -0.5448,\n",
            "         -1.2100, -1.1438, -0.4479,  0.6995, -0.4546, -0.7810, -0.2692, -1.1747,\n",
            "         -0.4831, -1.5646, -0.4987, -0.5046,  0.6417,  0.1453, -0.0928,  0.1095,\n",
            "         -0.7662, -0.2654, -0.3528, -0.4132, -0.8464, -1.0627, -1.6399,  0.0250,\n",
            "         -1.5561, -0.3453, -1.2319, -1.3892, -0.0857, -0.6409,  0.1460,  0.0805,\n",
            "         -0.8861, -1.5021, -0.1269, -0.7820, -0.5542, -0.1902,  0.0257,  0.3086,\n",
            "         -0.0558, -0.6493, -1.2496, -1.3088, -1.8970, -0.5864,  0.1686, -2.1933,\n",
            "         -0.8045, -0.4014, -1.4708, -0.2660, -1.1983, -0.8395, -0.9648, -0.3555,\n",
            "         -0.2417, -0.4782, -0.4093, -1.1712, -0.8403, -1.4099, -1.0845, -0.5776,\n",
            "          1.1729,  0.2415,  0.2981, -1.0073, -0.7499, -0.2046,  0.6577, -0.3097,\n",
            "         -0.7316,  0.0718,  0.2859,  0.1288,  0.9664, -0.1660,  0.3257, -1.3431,\n",
            "         -1.2607, -1.0731, -1.3529, -1.4469, -0.8281, -1.2955, -0.6096, -1.2886,\n",
            "         -0.9550, -1.2291, -1.3250, -1.5956, -1.6134, -1.6434, -2.1067, -1.6403,\n",
            "         -0.5614, -0.4290, -0.9713, -1.7893, -1.1365, -1.2682,  0.4293,  1.5166,\n",
            "         -1.0536, -0.3935,  0.0884,  0.0895, -0.4023, -0.2376,  0.0965,  0.2097,\n",
            "          0.3951,  0.5334,  0.1743,  0.5109,  0.2195, -0.2600, -0.2474, -0.4366,\n",
            "          0.5243, -0.2251, -0.1573,  0.7795,  0.3478,  0.2499,  0.1229, -0.7667,\n",
            "          0.0313, -0.0564,  0.6492,  0.5311,  0.4435, -0.0223,  0.4495,  0.0725,\n",
            "          0.6127,  0.6700,  0.5245,  0.0645,  0.0708,  0.5028, -0.5679,  0.3428,\n",
            "          0.3127,  0.5158, -0.7642,  0.6033, -0.0277, -0.0203, -0.0260,  0.4879,\n",
            "          0.0050,  0.1047,  0.5156,  0.5190,  0.1090,  0.1115, -0.0820,  0.6097,\n",
            "          1.1764,  0.5319, -0.1629,  0.3718,  0.3917, -0.0431, -0.0697,  0.2735,\n",
            "         -0.0836,  0.2211, -0.4149,  0.5993,  0.1173, -0.2097,  0.1044,  0.5618,\n",
            "          0.1525,  0.4390, -0.0362,  0.6654, -0.4071, -0.1699, -0.0713,  0.3929,\n",
            "          0.3088, -0.1532,  0.7087,  0.7932,  0.4134,  0.3807,  0.6827, -0.1487,\n",
            "          0.4670, -0.1623,  0.3839,  0.3293, -0.3385,  0.2439,  0.3796,  0.1020,\n",
            "          0.6301,  0.2346,  0.4661,  0.6543, -0.9763,  0.4756,  0.7598, -0.5810,\n",
            "          0.3472,  0.3433, -0.0417,  0.3082, -0.1429, -0.5889, -0.3678,  0.2915,\n",
            "          0.6315,  0.6718,  0.1547,  0.4422, -0.2312, -0.3983, -0.7970, -0.9713,\n",
            "         -0.5741,  0.6142, -1.1263, -1.1135, -1.0426, -0.7185, -1.1865, -0.5545,\n",
            "         -0.3662,  0.8982,  0.7787,  0.0821,  0.4243,  0.8732, -0.2304, -0.2787,\n",
            "         -0.6947, -1.5638, -1.0694, -1.2817, -0.3259, -0.9251, -0.9043, -0.8472,\n",
            "         -0.7923, -1.3820, -0.4955, -0.1825, -2.0447, -0.8109, -0.6352, -0.4837,\n",
            "         -1.2381, -1.0647,  0.0349, -0.9195, -1.5294, -0.5695,  0.3145, -0.5154,\n",
            "         -0.5240,  0.1069,  0.4937, -0.5034, -0.9546, -1.1687, -1.2215, -0.8677,\n",
            "         -1.7049, -1.2916, -1.5485, -1.7003, -1.4690, -1.6377, -1.4170,  0.2363,\n",
            "         -0.0947, -0.4604, -0.0759, -0.1755,  0.1383,  0.2174, -0.5036, -0.7991,\n",
            "         -1.5735,  0.1360,  0.7049, -1.0860, -0.4830,  0.5405, -0.4921, -1.5346,\n",
            "         -0.7090,  0.7735, -0.6523, -1.4548, -0.0185, -1.2695, -1.1217, -2.2079,\n",
            "         -1.3895, -0.7574, -0.4475,  0.5355,  0.9807, -0.0449,  0.3580,  0.3286,\n",
            "         -0.1911,  0.4754,  0.0562,  0.1165, -0.4656, -0.4739, -1.1319, -0.5173,\n",
            "         -0.9128, -0.7935, -0.7998, -0.4267, -0.5043, -0.0526, -0.2718, -1.0887,\n",
            "         -1.1666,  0.1363, -0.4609, -0.6032,  0.2109, -0.4063, -0.2013, -0.5896,\n",
            "         -0.8186, -0.3889, -1.0373, -1.1059, -1.0132, -0.1315,  0.6294,  0.2831,\n",
            "         -1.3434, -1.6580, -0.1583,  0.5775, -1.2063, -0.6261,  0.4781,  0.1365,\n",
            "         -0.8201,  0.6650,  0.0716, -1.9735, -1.6443, -0.4733, -0.0928, -0.2639,\n",
            "         -0.2300,  1.0777, -0.0633,  0.3566,  2.0594,  0.6600,  0.4345,  0.8550,\n",
            "         -0.1998,  0.4365,  0.1853,  0.9303,  0.7162,  1.2612, -0.1491,  0.2262,\n",
            "          0.1271, -0.8091,  0.0141,  1.4720,  1.7423,  0.3818, -0.6676,  0.0356,\n",
            "          0.2234,  0.6756,  0.6300,  1.0927, -0.3373, -0.3933,  0.4016,  0.3263,\n",
            "          0.7779,  0.5356,  0.0665, -0.4370, -0.2113,  0.1696,  0.3582,  1.4139,\n",
            "          0.9038, -0.6029, -0.1950,  0.3516,  0.6318, -0.1048, -0.3105,  0.5565,\n",
            "          1.5175,  1.1874, -0.0261,  0.8129, -0.7091,  0.6118,  1.3540,  2.5072,\n",
            "          1.0514, -0.2189, -1.2381, -0.1346, -0.0746,  1.5639,  1.2108,  0.4851,\n",
            "          0.2870,  0.9510, -0.2164, -0.0104,  0.1500,  0.4731,  0.8018,  0.4061,\n",
            "          0.0305,  0.2128,  0.3636, -0.7784, -1.3857, -0.0359, -0.4290,  1.1371,\n",
            "          1.6548,  1.2445,  0.5860,  0.8378,  0.6885, -0.9754,  1.2358, -0.9580,\n",
            "          0.1300, -0.4410, -0.1319,  1.1777, -1.5197,  0.6591,  1.3600,  0.5456,\n",
            "          1.0343,  1.0654,  1.0154,  0.5460,  0.4602,  0.3187, -1.1290, -0.8915,\n",
            "          0.7807,  0.3747,  1.0785,  1.8339,  0.5307,  0.0583,  1.3498,  0.7934,\n",
            "         -0.7310,  0.4101,  0.8419,  1.6921,  0.3790, -0.6503, -0.0355, -0.4490,\n",
            "          0.3313,  0.0725,  0.9990,  0.1781,  0.1452, -0.8589,  0.5013, -0.5827,\n",
            "         -0.4857, -0.5776,  0.1037,  1.3256, -1.1206,  1.6615,  1.0897,  0.7560,\n",
            "          0.4421,  0.8224,  0.6210, -1.9485, -1.2253, -0.0032, -0.5230,  0.0130,\n",
            "          0.8806, -0.1512, -1.3453, -0.5529,  0.4081,  0.4064,  1.2492,  1.0591,\n",
            "          0.1246, -0.1590,  0.9678,  0.0342, -1.2282, -0.6468,  0.2490,  1.1084,\n",
            "          0.5760, -0.5579,  1.0823, -0.0026,  0.9302, -0.7078,  0.3723, -0.2690,\n",
            "         -1.0655,  1.1676,  0.4118,  0.1120,  0.2124, -0.1356,  0.7911,  0.6013,\n",
            "          0.9537,  0.7766, -0.3723,  1.6636,  1.2623,  1.1171, -0.6837,  0.5927,\n",
            "         -0.7040,  0.7508,  0.3502, -0.5444,  1.0180, -0.2087, -0.5778,  0.9273,\n",
            "          2.2240, -0.0766, -0.0721, -0.5313,  0.6026,  0.0765,  1.3557, -0.3542,\n",
            "          0.5134, -0.2829,  1.0219,  0.6685, -0.5240,  0.5639,  0.1296,  0.3205,\n",
            "          1.0748,  0.5533,  1.9018,  0.9977,  1.0193,  0.6379,  0.0469,  0.4962,\n",
            "          0.1138, -1.0827,  0.9362, -0.4931, -1.0255,  0.1633, -0.1165,  1.0017,\n",
            "          0.6442,  0.8993, -0.3565,  0.2155,  1.2968,  0.8490,  0.6779,  0.1798,\n",
            "         -1.6805,  1.2856,  0.0408,  1.4811,  0.8307, -0.8309,  0.7317,  0.4838,\n",
            "         -0.6501, -1.4508,  0.9795,  0.2698,  0.5016,  0.8778, -0.0995,  0.6972,\n",
            "         -0.0064, -0.0062,  0.1631,  0.4996, -0.0644, -1.1597, -0.1163, -0.8297,\n",
            "          0.9289,  0.0861,  1.2490,  0.3444, -0.7654, -0.4086,  0.3356, -0.1482,\n",
            "         -0.2346,  0.5726,  1.3468, -0.6487,  1.6044,  1.0049,  1.0361,  0.1997,\n",
            "          0.5610,  0.4636, -0.4857,  0.5148,  0.8806, -1.4682, -0.0741, -1.1123,\n",
            "         -0.3441, -0.7420, -0.4974,  0.7337,  1.0477,  0.7423, -0.8986,  1.0308,\n",
            "          1.5860,  0.0662, -0.3934,  0.7644,  1.9054, -0.2470, -0.2073,  0.4521,\n",
            "          0.6187, -0.4213, -0.2667,  0.1946,  0.9998,  0.2111,  0.9762,  1.0698,\n",
            "          0.0581, -0.3843,  0.4386, -0.4353,  0.6737, -0.6620, -0.3092,  0.7243,\n",
            "          0.2160,  0.2768,  1.3165,  0.1965, -0.5916,  1.3585, -0.7728, -0.0689,\n",
            "          1.5500, -0.3090,  0.1730,  2.2628, -0.8260,  1.8802, -1.4797,  0.3521,\n",
            "         -0.2711,  0.5757,  0.9892,  0.1419,  0.9646,  0.0147,  0.3866,  0.2777,\n",
            "          0.6275,  0.0697, -0.0500,  0.7556,  0.7672,  1.4469,  0.5439, -0.0663,\n",
            "          0.1635,  0.7332,  0.8044, -0.6024,  1.0400, -0.0193,  1.5017, -0.2590,\n",
            "          0.1824,  0.6172,  0.4665,  0.6561,  1.2081,  0.9717,  0.5197,  0.4778,\n",
            "         -0.1862,  1.3223,  0.4628,  0.3418,  1.4320,  0.7232,  0.9251,  0.5103,\n",
            "          0.4861,  0.5947,  1.5211, -0.7267, -0.9094, -0.7214,  1.0167,  0.8277,\n",
            "          1.5246,  0.2961,  0.4336,  1.3585,  0.2423,  0.0204,  0.7800,  1.2015,\n",
            "          1.7271,  1.0211,  0.4231,  0.1518,  1.0038,  0.7986, -0.8803,  0.4082,\n",
            "         -0.9188,  0.2486, -0.9521, -1.2738,  1.1295,  1.2124,  0.3049,  0.2400,\n",
            "          1.3759,  0.1848, -0.5218,  1.0942, -0.2841,  1.6931, -0.9587, -0.2322,\n",
            "          0.3359, -1.1049,  1.6082,  0.3574, -1.4709, -0.9093,  0.5426,  0.6452,\n",
            "          1.0448, -0.8331,  0.3643,  1.0266,  1.3247, -0.5342,  1.1610,  0.4722,\n",
            "         -0.6006, -0.9487,  0.0450,  0.5302,  1.5436,  1.5989,  1.0190, -0.4903,\n",
            "          1.4611,  0.5651,  0.5098,  0.6327,  0.6118,  1.6770,  0.8280, -0.4132,\n",
            "          0.3935,  1.0298,  1.3463,  1.3969,  1.7349, -0.4150, -0.1865,  0.7231,\n",
            "         -0.5644, -0.0468, -0.3380,  1.0042,  0.1565,  1.3900,  1.0136, -0.0106,\n",
            "         -0.4504,  0.6349,  0.1188, -0.2689,  1.5828, -0.4370,  0.9666, -1.3599,\n",
            "          1.0292, -1.0088, -2.3178,  0.3347,  1.6037, -0.1287, -0.2759,  1.5659,\n",
            "          1.0787, -0.2140,  1.0297,  1.5044,  0.1133,  0.3603, -0.2958, -0.1339,\n",
            "         -0.9330,  0.1025, -0.3293,  0.1733,  0.8966,  0.1376, -0.4921, -0.5267,\n",
            "          1.2628,  0.6830,  1.9619,  1.9560, -0.9340, -0.3869,  1.5049,  0.9340,\n",
            "          0.8910,  0.0505, -0.5556,  1.3219, -0.8009,  1.1271,  1.3794,  1.1108,\n",
            "          0.8721, -0.3988, -1.8154, -0.2626,  0.1173,  0.2692,  0.4006,  0.2649,\n",
            "         -0.1631,  1.1568, -0.6283,  0.6040, -0.2943, -0.8373, -1.0334, -0.5658,\n",
            "         -0.1179,  1.4236, -0.2671,  0.1951,  0.5809, -1.6246,  0.0033, -0.4332,\n",
            "          0.4692,  0.3810,  0.0545, -0.0978, -0.2226, -0.7238, -0.2597,  0.1923,\n",
            "         -0.5790, -0.6148, -1.1691,  0.5778,  0.7935, -0.3247,  0.1056, -0.4826,\n",
            "         -0.6143,  0.2404,  0.9557, -0.3385, -0.2765, -0.4688, -0.0683, -0.8946,\n",
            "          0.2659,  0.2569, -0.3871, -0.7281, -1.1671, -0.2103,  0.4713, -0.5467,\n",
            "          0.8507,  0.0247, -0.0265,  1.0720, -0.2115, -0.3026, -2.0022,  0.9871,\n",
            "         -1.4272,  0.4754,  0.2517, -0.6148, -0.5280,  0.0389,  0.4456, -0.4020,\n",
            "         -0.9215, -1.2209, -2.3850,  1.4265, -0.3757, -0.9735, -0.3108, -1.0819,\n",
            "         -0.8781, -1.9097, -0.8154, -0.3930,  0.3469, -0.5314,  1.3019,  1.0636]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor([[ 9.9711e-03, -3.6899e-03,  6.3092e-03, -1.7100e-02, -1.8859e-02,\n",
            "         -1.3261e-02,  1.1564e-02, -1.8285e-02, -5.4201e-04, -2.0933e-02,\n",
            "         -1.1554e-02, -1.3555e-02,  1.0364e-02, -2.5331e-02, -1.2584e-02,\n",
            "         -1.0028e-02, -6.7645e-03, -1.5688e-02,  5.1348e-03,  1.4650e-02,\n",
            "          2.5145e-04,  1.4313e-02,  2.3668e-03, -1.6060e-02,  1.7875e-04,\n",
            "         -1.3462e-02,  1.4664e-02, -1.2434e-03,  5.6048e-03,  9.9003e-03,\n",
            "         -1.5106e-02,  1.5304e-03,  9.3917e-03, -1.2662e-02,  6.3213e-03,\n",
            "          1.8513e-03,  5.3321e-03, -3.9096e-03,  6.6539e-03,  1.2264e-03,\n",
            "          1.9265e-02, -6.9711e-03,  1.6281e-02, -6.7373e-03,  5.6844e-03,\n",
            "         -2.5929e-03,  1.1761e-02, -2.0438e-02,  9.6021e-03,  6.4706e-03,\n",
            "         -7.3217e-03,  6.0660e-03,  4.5976e-03,  1.3301e-02, -1.7319e-02,\n",
            "          8.0090e-03,  8.5534e-03, -1.0562e-02,  1.6806e-03,  3.5983e-03,\n",
            "         -1.3500e-02, -8.1031e-03, -4.5766e-04,  1.2518e-04,  1.3079e-02,\n",
            "          1.0140e-02, -1.0720e-02,  1.2742e-02, -4.6993e-03, -1.4086e-02,\n",
            "          2.4785e-03, -1.0973e-02,  1.5118e-02,  6.4810e-03, -1.4572e-02,\n",
            "          1.5362e-02, -3.7595e-03,  1.4750e-02,  1.1916e-02, -1.0995e-02,\n",
            "         -8.0154e-03, -4.2553e-03, -1.3107e-02,  4.0837e-03,  8.5378e-03,\n",
            "          6.0986e-03,  1.2490e-02,  2.1034e-02,  8.3162e-03,  8.1626e-03,\n",
            "          2.8109e-03, -1.2911e-02, -7.5348e-03,  1.0207e-02, -8.8148e-03,\n",
            "         -5.0198e-03, -1.7185e-02, -3.4929e-03, -1.9793e-02,  2.3383e-03,\n",
            "         -3.5403e-03, -8.7690e-04,  2.1798e-03, -2.1371e-02,  3.3711e-03,\n",
            "         -1.9177e-02, -5.2753e-03, -8.4092e-03,  6.5390e-03,  1.2302e-02,\n",
            "          1.0996e-03,  1.3175e-02,  1.7951e-02,  7.1403e-03, -9.8654e-03,\n",
            "          8.2013e-03,  6.1426e-03, -1.3320e-02,  1.3562e-02, -6.8305e-03,\n",
            "          2.8356e-03,  1.0580e-02,  1.6830e-03, -2.1388e-03,  1.3440e-02,\n",
            "         -7.7945e-03,  2.7642e-03, -1.5076e-03,  2.9798e-03,  1.1579e-03,\n",
            "         -7.7845e-03, -1.3543e-02,  7.0138e-03,  2.9422e-03,  1.7601e-03,\n",
            "          4.4733e-03, -3.1800e-03, -2.3319e-04,  9.8588e-03,  6.9198e-03,\n",
            "          1.5753e-02,  1.3726e-02, -2.7431e-03, -1.1269e-03, -1.5042e-02,\n",
            "          1.1428e-02, -2.5362e-03, -1.6020e-02, -9.5431e-03,  7.8401e-04,\n",
            "          1.4113e-02,  9.7586e-04,  1.6817e-03,  6.7685e-03,  8.2895e-03,\n",
            "          4.0300e-03, -7.1341e-03, -6.6746e-03, -1.0837e-02,  1.3786e-02,\n",
            "         -7.8712e-03,  9.8538e-03, -9.0394e-05,  8.4438e-03,  1.7036e-02,\n",
            "         -6.2358e-03,  4.1680e-03,  9.4947e-03, -1.5423e-02,  1.0260e-02,\n",
            "          8.2442e-03, -1.3535e-02,  1.3414e-02,  5.8378e-03,  1.5635e-02,\n",
            "          2.6770e-03, -1.3374e-02, -1.4971e-02,  7.8988e-03,  6.7893e-03,\n",
            "         -6.0829e-03, -1.1553e-03, -2.2009e-02, -4.0447e-03,  4.1498e-03,\n",
            "         -2.8598e-03, -1.7609e-02, -1.1687e-02, -1.0476e-02,  9.0169e-03,\n",
            "          9.2777e-03,  1.5524e-02, -8.9410e-03,  1.0338e-02, -5.4648e-03,\n",
            "          1.2384e-02,  1.2747e-02,  1.4480e-02,  4.1344e-03,  2.3525e-02,\n",
            "          3.2262e-03,  1.2405e-02, -1.6795e-02,  1.5675e-02, -5.5726e-03,\n",
            "         -2.1226e-02,  3.3943e-03,  1.9871e-03,  1.2836e-03, -8.9835e-03,\n",
            "          1.5578e-02, -1.1478e-02, -9.9182e-04, -2.7336e-03,  5.1717e-03,\n",
            "         -5.9704e-03, -6.7836e-03,  2.2649e-03,  2.0114e-02,  1.1983e-02,\n",
            "         -1.7748e-03, -4.5218e-03,  9.7152e-03,  1.6342e-02, -1.0774e-02,\n",
            "         -4.9498e-03, -7.2259e-03, -5.0379e-03,  9.5746e-03,  8.1975e-03,\n",
            "         -5.7315e-03, -6.2267e-03,  1.1481e-02, -1.0236e-02,  4.9729e-03,\n",
            "         -1.4196e-02, -1.0576e-02,  6.2791e-03,  1.4440e-02,  2.7579e-03,\n",
            "          1.7379e-02,  8.7031e-03, -1.7460e-02, -3.5399e-03, -1.2479e-02,\n",
            "         -3.3799e-03, -2.5302e-02, -1.7506e-02, -1.1823e-02, -1.5364e-02,\n",
            "          1.9260e-03, -1.6805e-02, -1.9661e-03,  1.0832e-02, -1.0087e-02,\n",
            "         -3.6539e-03, -7.9643e-03, -7.4699e-03, -1.3848e-02,  1.5957e-02,\n",
            "         -7.9018e-03, -5.5242e-03, -6.7170e-03,  1.4420e-02,  1.2361e-02,\n",
            "          1.8219e-02,  1.3813e-02,  7.6574e-04, -1.0998e-02,  1.2870e-02,\n",
            "         -1.8808e-03,  2.5021e-03, -6.5816e-04, -2.5906e-03, -6.0340e-04,\n",
            "         -1.1485e-02,  1.2513e-02,  1.7700e-02, -1.0957e-02,  3.4208e-03,\n",
            "         -1.2212e-02,  1.1940e-02,  1.6730e-02,  2.4491e-02, -1.8901e-03,\n",
            "         -9.7122e-03,  1.3734e-02,  1.2374e-03,  4.0466e-03,  9.7160e-03,\n",
            "          7.7945e-03,  1.0263e-02,  1.3731e-02, -6.6797e-03, -1.2972e-02,\n",
            "          1.4253e-02, -2.3192e-03, -6.7719e-03, -2.2062e-03, -7.0157e-03,\n",
            "         -1.0857e-02, -5.6661e-03,  4.9538e-03,  1.6543e-02,  4.1009e-03,\n",
            "         -1.8594e-02,  7.4893e-03, -9.8906e-03, -6.2970e-03,  5.9425e-04,\n",
            "         -9.6551e-03,  1.2290e-02,  2.0668e-03, -1.6976e-02,  1.0621e-02,\n",
            "          1.3729e-02, -4.6545e-03,  3.1963e-03,  1.3230e-02,  1.7772e-02,\n",
            "          3.9398e-03, -6.8194e-03,  1.4755e-02,  2.3711e-02, -1.8275e-03,\n",
            "         -9.2682e-03,  1.2920e-02,  2.3350e-03, -5.4307e-03,  1.1522e-02,\n",
            "         -1.4163e-02, -1.7851e-02,  1.1943e-03, -1.5681e-02, -1.1491e-02,\n",
            "         -5.2250e-04,  8.1187e-03, -1.0956e-02, -7.5113e-03,  8.0340e-03,\n",
            "          9.9221e-03, -7.9660e-03,  1.3070e-02, -1.1280e-03, -9.2727e-03,\n",
            "         -9.3153e-03,  9.0290e-03, -1.1561e-02,  5.3965e-04,  5.0604e-03,\n",
            "          8.3130e-03, -1.4433e-02, -3.7685e-03,  2.2095e-03,  2.3333e-03,\n",
            "         -1.4411e-02,  1.5149e-02,  9.7043e-03,  8.1062e-03, -7.0478e-03,\n",
            "          1.1902e-02, -3.2759e-03,  1.9826e-02,  5.7540e-03, -3.1181e-03,\n",
            "          4.5270e-03,  4.6938e-03, -1.1879e-03, -3.8004e-03, -1.4604e-02,\n",
            "         -3.7812e-03,  1.8827e-03,  3.6345e-04, -9.0556e-03, -7.0089e-03,\n",
            "          5.7833e-03, -1.2997e-02, -1.6757e-02, -1.1315e-02, -6.3853e-03,\n",
            "          6.9072e-04,  1.3604e-03,  2.6332e-02,  6.2944e-03,  1.8550e-03,\n",
            "         -6.5856e-03, -4.3317e-03,  3.4740e-03, -9.4868e-03,  8.8861e-03,\n",
            "          9.6763e-03, -9.6381e-04,  1.7820e-02,  8.4402e-03,  3.7236e-03,\n",
            "         -1.0138e-02, -1.6463e-03,  8.9607e-03,  4.1300e-03, -9.7330e-03,\n",
            "         -2.3133e-03,  8.0168e-04, -5.1237e-03,  3.1792e-03, -1.1113e-02,\n",
            "          2.8490e-03, -1.6930e-02, -1.0360e-02, -8.2165e-03,  2.1491e-02,\n",
            "         -1.5176e-02,  5.6520e-03, -1.7447e-02,  5.5564e-03, -7.0455e-03,\n",
            "          1.3689e-02,  8.6776e-03, -6.8347e-03, -4.2043e-03, -8.4311e-03,\n",
            "          5.7503e-04, -1.0739e-02,  1.1207e-02,  3.5984e-03, -3.4753e-03,\n",
            "         -9.3438e-03,  1.5867e-02,  7.9790e-03, -4.5365e-03, -2.4396e-02,\n",
            "         -8.9024e-03, -1.2017e-03,  1.1442e-02,  2.6522e-03,  4.8183e-03,\n",
            "         -6.9494e-03,  1.4361e-02,  2.1301e-02, -1.8539e-02, -2.4865e-03,\n",
            "          4.3411e-03,  1.0972e-02, -6.3285e-03, -1.8859e-02,  7.2800e-03,\n",
            "          5.5891e-03,  9.0730e-03, -8.6180e-03, -1.2501e-02,  1.5045e-02,\n",
            "          1.1978e-03,  5.8198e-03, -2.3616e-02, -9.8661e-03, -4.6404e-04,\n",
            "          1.0074e-02,  4.5608e-03, -1.4753e-02,  3.2794e-03,  6.5603e-03,\n",
            "          8.8545e-03,  9.8256e-03,  2.1152e-02, -1.6579e-03, -1.5277e-03,\n",
            "         -8.7102e-03,  2.9950e-03, -8.6673e-03,  1.4599e-02, -1.1495e-02,\n",
            "         -1.4136e-02, -8.1230e-03, -5.0396e-03,  7.3049e-03, -7.7627e-03,\n",
            "         -4.7739e-03, -4.4291e-03, -3.6738e-03, -1.5028e-03,  1.1353e-02,\n",
            "         -1.2961e-02, -9.5884e-03,  1.3807e-02,  4.4124e-03,  4.5008e-03,\n",
            "         -7.4311e-03, -1.0746e-03, -7.4722e-03,  4.8320e-04, -1.4640e-02,\n",
            "          1.3930e-02,  3.0947e-03,  1.7315e-02, -3.6132e-03,  1.3957e-02,\n",
            "          2.4409e-03, -5.0058e-03, -6.4011e-03, -1.6037e-02, -1.7373e-02,\n",
            "          5.8506e-03, -1.2754e-02,  2.8032e-03,  9.5891e-03,  1.4124e-02,\n",
            "         -9.9174e-03, -2.0886e-02, -2.1319e-02, -1.0472e-03, -5.6256e-03,\n",
            "          7.3017e-03,  1.2806e-02, -8.4903e-03,  8.9749e-04,  1.3086e-02,\n",
            "          2.1765e-03,  9.0333e-03,  3.9141e-03, -8.0096e-04, -6.5755e-03,\n",
            "          1.1715e-02, -4.9983e-03, -7.8840e-03, -1.8684e-03, -1.5630e-03,\n",
            "          1.8095e-02,  3.6257e-03,  3.2297e-03, -1.3775e-02, -1.4674e-02,\n",
            "          1.0857e-02,  7.6585e-03, -1.3611e-02,  9.7218e-03,  1.3847e-02,\n",
            "         -4.1098e-03,  7.5536e-03, -9.9852e-04,  9.8200e-04,  1.3078e-02,\n",
            "          7.3393e-03,  4.0939e-03, -4.6718e-03, -3.8621e-04,  2.8509e-03,\n",
            "          2.9312e-03,  2.5712e-03,  1.2325e-02, -2.4973e-03,  4.8261e-03,\n",
            "         -1.4488e-02,  6.5857e-03,  3.0422e-03, -1.1830e-02, -6.1961e-03,\n",
            "         -1.5929e-02,  2.3996e-03,  1.4716e-03, -8.7662e-03,  1.0040e-03,\n",
            "          6.3961e-03, -9.9797e-03,  1.7302e-02, -4.1702e-04,  7.6075e-03,\n",
            "          1.3736e-02,  1.3571e-02,  1.0842e-02,  3.5682e-03, -6.1698e-03,\n",
            "          3.9580e-03, -1.2741e-02,  6.1431e-03, -5.6304e-03,  8.3042e-05,\n",
            "         -1.4402e-03, -1.1079e-02,  1.0546e-02,  1.2354e-02, -1.5948e-02,\n",
            "         -1.3548e-02, -1.4999e-02,  6.8231e-03,  1.3347e-02,  1.9887e-02,\n",
            "         -6.9937e-03, -5.1884e-03, -3.8163e-03,  7.5402e-03, -3.2485e-03,\n",
            "         -1.0628e-02,  9.9788e-04, -1.3732e-02, -1.3966e-02, -1.0280e-02,\n",
            "          6.0068e-03,  1.3008e-03, -1.8850e-02, -6.3604e-03, -5.6196e-03,\n",
            "          1.0030e-02, -2.3386e-03,  7.7485e-03,  3.0934e-03, -1.6572e-02,\n",
            "          8.6838e-03,  6.5963e-03, -1.7551e-02, -1.3489e-02, -6.0346e-03,\n",
            "          4.9317e-03, -1.2051e-02, -7.7904e-03,  2.0809e-03,  1.0586e-02,\n",
            "         -4.0418e-03,  4.5248e-03,  6.0865e-03, -4.1185e-03, -5.2650e-03,\n",
            "          8.2154e-03, -1.3623e-02,  2.1251e-03, -1.7708e-04, -6.1629e-03,\n",
            "         -1.2279e-02, -1.9206e-02, -6.4090e-03,  5.6682e-03, -9.8248e-04,\n",
            "         -1.1310e-02, -1.9257e-03,  1.2240e-02, -2.6077e-03,  1.0122e-02,\n",
            "         -1.2333e-02,  1.7374e-02,  6.7295e-04, -3.8628e-04,  1.1444e-02,\n",
            "         -8.5012e-04, -1.2427e-02,  4.5889e-03,  1.6735e-03,  4.0373e-03,\n",
            "          1.7995e-03,  1.8446e-02, -1.3724e-02,  3.5640e-03,  4.6165e-03,\n",
            "         -2.0671e-02, -1.8567e-02,  9.1076e-03, -2.0173e-03,  9.8522e-03,\n",
            "          1.4262e-02,  8.6206e-03, -3.6883e-03, -2.0991e-02, -1.4022e-02,\n",
            "         -1.4125e-02,  7.1259e-03,  1.4443e-02, -6.2798e-03,  1.4174e-02,\n",
            "         -2.9577e-04, -3.8776e-03,  7.6307e-03,  9.6436e-03,  5.9719e-03,\n",
            "          4.7823e-03,  1.7892e-02, -8.6193e-04,  1.2829e-02, -1.4182e-02,\n",
            "         -1.3578e-02, -1.8822e-03, -3.0838e-04, -1.4406e-03,  6.3499e-03,\n",
            "         -1.4328e-03,  7.3102e-03,  3.1679e-05,  1.9820e-02,  2.1634e-03,\n",
            "          1.0158e-02,  4.3132e-03, -6.2225e-03,  1.8399e-02,  8.5618e-03,\n",
            "         -1.6481e-02, -3.1300e-03, -5.6825e-03, -1.2369e-02,  3.8089e-03,\n",
            "          1.5296e-02,  2.2194e-02, -3.7175e-03, -6.6644e-03, -1.9121e-02,\n",
            "          5.6874e-03,  1.4580e-02, -1.2443e-02, -6.3662e-03, -1.0501e-02,\n",
            "         -1.9743e-03, -7.5306e-04, -1.0522e-02,  8.1545e-03,  1.0366e-02,\n",
            "         -1.6288e-02, -1.0493e-02, -1.8442e-02,  4.5617e-04, -1.0057e-02,\n",
            "          1.6302e-03, -3.9970e-03,  4.7525e-03, -1.6812e-03, -3.5275e-04,\n",
            "          2.2609e-02,  5.0829e-03,  4.5230e-03,  1.5675e-02, -1.3417e-02,\n",
            "          1.8139e-02,  8.4479e-03,  1.3731e-03,  1.5564e-02,  1.2586e-02,\n",
            "         -7.0797e-03, -3.6322e-03, -6.1771e-03, -1.2226e-02, -1.8507e-02,\n",
            "          1.6383e-02,  5.6809e-03,  8.1441e-03, -1.4479e-02,  8.4698e-03,\n",
            "         -6.0974e-03, -4.1242e-03,  1.3778e-03,  1.8307e-03,  1.5593e-03,\n",
            "          9.8530e-03, -2.3234e-02, -2.0310e-03,  2.3665e-02, -8.6323e-03,\n",
            "          1.2503e-02,  1.6113e-02,  1.5999e-02, -4.7316e-03, -1.7818e-02,\n",
            "          1.8441e-02,  8.4885e-03, -1.3715e-02,  4.1064e-03, -1.0398e-02,\n",
            "         -3.9203e-03,  2.7381e-04, -5.4915e-03, -1.5923e-02,  4.7210e-03,\n",
            "          1.3115e-02,  9.0149e-03,  4.5289e-03,  4.9351e-03, -1.3108e-03,\n",
            "         -5.3866e-04, -6.1434e-03, -4.8857e-03,  2.3335e-02, -8.6861e-03,\n",
            "          1.5789e-02, -6.5158e-03, -2.1703e-03, -1.9506e-03,  2.1328e-03,\n",
            "          9.1355e-03,  3.1719e-03, -3.6563e-03,  3.7292e-03,  1.1249e-02,\n",
            "         -1.7586e-02,  1.3871e-03, -1.4443e-02,  7.4069e-03,  1.7401e-02,\n",
            "          1.0637e-02,  1.0965e-02,  7.5489e-03, -3.4167e-03,  4.8652e-03,\n",
            "          1.1922e-02,  2.8230e-03, -8.7609e-05, -1.3195e-02, -1.6343e-03,\n",
            "          1.1080e-02,  1.4201e-02, -5.4474e-03, -6.2381e-03,  6.4098e-03,\n",
            "         -5.1372e-03, -1.0577e-03,  1.5791e-03, -1.5027e-02,  1.0209e-02,\n",
            "          4.0292e-03, -7.6830e-03,  1.7693e-02, -1.7013e-02,  1.1497e-02,\n",
            "          1.3574e-02,  7.2082e-03, -2.7344e-03,  1.2267e-02,  7.7646e-03,\n",
            "         -1.3902e-02, -6.4309e-04,  1.9907e-02, -5.6809e-03,  8.4985e-03,\n",
            "          1.4435e-03, -4.8961e-03,  4.7515e-03, -3.4274e-03, -9.7625e-03,\n",
            "         -1.3618e-02, -1.6775e-02, -1.5218e-03,  7.0955e-03,  4.6977e-03,\n",
            "         -3.9752e-03,  1.6577e-02,  1.2702e-02, -7.6303e-03, -3.8817e-04,\n",
            "          8.4112e-03, -1.6324e-02, -3.8897e-03, -5.5505e-03,  5.5217e-03,\n",
            "          3.6251e-03, -9.3048e-03, -9.0757e-03,  4.6905e-03,  1.2733e-02,\n",
            "          4.0259e-03, -2.7510e-03,  1.4335e-02, -3.2282e-03, -4.2335e-03,\n",
            "         -1.1341e-02,  4.1022e-03, -5.5362e-03, -2.9965e-03,  3.2044e-03,\n",
            "          3.4765e-03, -1.2575e-02,  1.1748e-02, -9.9197e-03,  1.4975e-02,\n",
            "          2.2600e-02,  2.8025e-03,  3.2513e-03,  1.1577e-02, -3.3439e-04,\n",
            "         -2.4508e-03,  1.6202e-02,  4.4539e-03, -3.9435e-03, -3.9524e-03,\n",
            "          7.8773e-03,  8.6506e-03,  1.1045e-03, -1.5639e-03,  8.6582e-03,\n",
            "         -1.2330e-02, -1.0487e-02, -1.6136e-02, -2.2052e-02, -1.3043e-02,\n",
            "         -4.3469e-03,  2.3889e-03, -1.1096e-03,  1.1653e-02,  1.0526e-02,\n",
            "          1.9624e-02,  9.7906e-03,  9.8514e-05, -4.7278e-03,  1.2523e-02,\n",
            "         -7.1006e-03, -1.1852e-02,  9.6423e-04,  2.1538e-03, -8.5808e-03,\n",
            "          7.9006e-03,  1.9765e-02, -1.4418e-02, -7.8477e-03, -4.0206e-04,\n",
            "          1.5027e-02, -2.6558e-03,  7.5271e-03, -7.5929e-03,  4.3250e-03,\n",
            "         -8.9540e-03,  1.0013e-02,  5.5937e-03,  2.6882e-03,  6.3937e-03,\n",
            "         -6.3434e-03,  1.1251e-02,  1.6935e-02, -3.4614e-03,  1.1375e-02,\n",
            "          1.3021e-02,  1.6775e-02, -6.3419e-03, -1.3057e-02, -1.3183e-02,\n",
            "          8.3396e-03, -1.0138e-02,  1.4212e-02,  1.1478e-02,  6.6340e-03,\n",
            "         -4.9858e-03, -4.5090e-03,  9.2761e-03, -1.3627e-02,  1.1513e-02,\n",
            "          1.4734e-04, -1.6452e-03, -2.0488e-02, -1.4912e-02,  1.0099e-02,\n",
            "         -8.8956e-03, -2.7644e-03,  5.0332e-03, -1.5577e-02,  8.9042e-03,\n",
            "          2.1337e-02,  1.6321e-02,  2.8139e-03,  2.0445e-02,  5.9273e-03,\n",
            "          8.6684e-03,  2.5628e-03, -1.7994e-03, -8.4417e-04,  9.0668e-04,\n",
            "          2.3013e-03,  9.2810e-04, -8.1421e-03,  6.1483e-03, -1.1888e-02,\n",
            "          1.5896e-03,  3.4558e-03, -2.0730e-03, -4.5044e-03, -6.2727e-03,\n",
            "         -1.6445e-02,  1.2443e-03,  1.1540e-02, -1.0754e-02, -1.3407e-02,\n",
            "          8.6724e-03,  2.7028e-03, -8.7284e-03,  9.3243e-04,  1.1841e-02,\n",
            "          3.4257e-03,  1.3996e-02, -4.0634e-03,  2.4697e-03, -2.7248e-03,\n",
            "          1.1252e-03,  1.1841e-02, -1.1507e-02, -7.2409e-03,  8.1212e-03,\n",
            "          1.3938e-02, -8.1706e-03, -9.4302e-03,  1.2346e-02, -2.1281e-03,\n",
            "          1.4417e-02,  2.3662e-03,  2.0990e-03,  6.3191e-03,  3.1413e-03,\n",
            "          1.8034e-02,  1.1129e-02,  1.1709e-02, -2.3574e-03,  1.4986e-02]],\n",
            "       grad_fn=<AddmmBackward>)\n",
            "tensor([[-2.0192e+00, -2.8628e-01, -1.7523e-01,  5.3420e-01,  6.0676e-01,\n",
            "         -4.9840e-01,  4.2795e-02, -6.9528e-01, -2.1479e+00, -9.6841e-01,\n",
            "          3.2200e-01,  1.2679e+00,  7.9163e-01,  1.5793e+00,  9.1202e-01,\n",
            "         -1.1930e-01,  1.0090e+00, -2.8410e-01,  9.4523e-01,  1.2158e+00,\n",
            "         -8.2626e-01,  4.3655e-01,  4.9296e-01, -1.7781e-02,  4.4688e-01,\n",
            "         -1.1682e+00, -1.3892e+00, -9.3889e-01, -2.1785e-01,  8.3804e-02,\n",
            "         -1.5549e+00,  9.6942e-01, -1.7662e+00, -7.5809e-02,  7.2081e-01,\n",
            "         -1.3746e+00,  1.7670e-02, -6.2721e-01,  2.8652e-01, -1.8390e+00,\n",
            "          7.5149e-01, -1.3369e+00, -7.3133e-01,  3.0883e-01, -5.0010e-01,\n",
            "         -5.6883e-01,  9.5994e-01, -1.6030e+00, -1.0172e+00, -1.2169e+00,\n",
            "          6.8870e-01, -2.5508e+00, -4.2733e-01, -4.1130e-01, -1.1770e-01,\n",
            "         -5.2743e-01, -1.3257e+00, -1.2100e+00,  1.6646e-01,  2.7027e-01,\n",
            "          3.5563e-01, -1.2873e+00, -1.3984e+00, -3.9958e-01,  1.1538e+00,\n",
            "         -3.3045e-01, -8.8500e-01,  4.8251e-02, -9.9821e-02, -1.1041e-01,\n",
            "          4.7777e-01,  2.6330e-01,  6.1849e-01,  1.2555e+00,  7.1006e-01,\n",
            "          1.7635e+00,  1.1513e+00,  8.2008e-01,  2.0995e+00,  9.7590e-01,\n",
            "         -2.6325e-01,  9.0364e-01, -1.1751e+00,  4.7543e-02, -8.8097e-01,\n",
            "          1.5673e+00, -9.1167e-01,  2.3201e+00,  1.0500e+00, -2.7268e-01,\n",
            "         -4.1004e-01, -1.3407e+00,  7.6934e-01, -1.2660e-01,  1.7024e+00,\n",
            "         -2.2786e+00,  7.7594e-01, -9.6566e-01, -5.1516e-01, -5.2154e-01,\n",
            "          2.4206e-01, -5.4255e-01,  3.7805e-01,  7.3587e-01, -1.0071e+00,\n",
            "         -2.6114e+00, -1.1357e+00,  1.0527e+00, -2.5613e+00, -1.3369e+00,\n",
            "         -2.6113e+00,  3.2761e+00,  1.0534e+00, -3.0880e-01, -3.6488e-01,\n",
            "         -3.0349e+00, -2.6973e+00, -3.4609e-01,  5.5857e-02, -1.3668e+00,\n",
            "          2.7604e-01, -2.1012e+00, -1.2453e+00, -6.9168e-01, -4.1388e-01,\n",
            "          1.0842e+00,  9.7881e-02, -6.6851e-02,  1.3431e-01, -3.8921e-01,\n",
            "         -1.3077e+00,  1.7156e-01,  4.7667e-01,  1.5896e-02, -2.1702e-02,\n",
            "         -1.4960e+00, -8.7707e-01,  1.0557e+00, -9.9788e-01,  8.5865e-01,\n",
            "          4.4371e-01,  4.3620e-02, -2.0276e-01,  1.0944e+00, -4.4904e-01,\n",
            "         -1.3668e-01,  1.5747e-01, -7.6734e-01,  1.1585e+00, -6.7213e-01,\n",
            "         -1.4072e+00,  7.4828e-01, -1.4180e+00,  4.6683e-01,  3.7878e-01,\n",
            "          7.6903e-01, -3.9632e-01, -1.9786e-01, -3.3141e-01, -1.0704e+00,\n",
            "         -8.7563e-01,  1.4385e-01,  1.2509e+00,  1.5434e-01,  5.5041e-01,\n",
            "         -1.0795e+00,  1.8686e-01, -2.5686e+00, -7.4164e-01, -1.4290e+00,\n",
            "         -7.5628e-01,  3.7178e-01,  2.4238e-01, -6.0585e-01,  4.6202e-01,\n",
            "         -2.1997e+00, -1.2910e+00, -1.4365e+00,  4.6152e-01, -9.7866e-01,\n",
            "         -2.9795e-01, -1.5663e+00,  4.5463e-01, -1.2636e+00, -2.3158e-01,\n",
            "         -1.3794e+00, -7.8917e-01,  8.8078e-01, -2.9939e+00,  9.5898e-01,\n",
            "         -4.9781e-01, -3.0455e-01,  1.1328e+00,  5.7178e-01, -3.0826e-01,\n",
            "          1.0976e+00,  1.9721e-01, -1.0086e+00, -1.8552e+00,  5.8053e-01,\n",
            "         -2.9695e-01, -6.0192e-01,  4.7848e-01,  1.5962e+00,  1.0231e+00,\n",
            "         -1.0698e+00, -9.9834e-01, -3.3130e-01, -7.1190e-01, -9.1065e-01,\n",
            "         -1.6255e+00, -9.3331e-01, -3.5354e-01, -1.6284e+00, -1.6051e+00,\n",
            "         -4.5923e-01, -1.4896e+00,  3.2022e-01, -4.6074e-01, -7.3631e-01,\n",
            "         -2.2456e+00, -1.8765e+00, -4.1415e-01,  6.0072e-01, -1.3527e+00,\n",
            "         -4.0823e-01, -1.1089e+00, -6.2538e-01, -9.1634e-01,  1.3643e-01,\n",
            "         -6.3281e-01, -9.5459e-01, -1.9251e-01, -2.3782e+00, -6.7668e-01,\n",
            "         -1.5182e-01,  1.2568e-01,  1.4637e+00,  4.4194e-01, -2.3542e-01,\n",
            "         -5.9862e-01, -1.5858e-01, -4.1145e-01, -1.5515e+00, -2.9354e+00,\n",
            "          6.6555e-01, -1.3754e+00, -9.4916e-01, -5.9552e-01, -1.6788e-01,\n",
            "          9.9328e-02,  9.7726e-01, -2.2401e+00,  3.0131e-01,  2.9078e-01,\n",
            "         -2.1107e+00, -6.7399e-01, -4.6444e-01,  4.0015e-01,  5.0337e-01,\n",
            "         -1.1754e+00, -5.7318e-01, -1.5871e+00,  3.7078e-01, -9.9657e-01,\n",
            "          8.7167e-01, -2.8922e-01, -2.0193e+00, -1.9591e+00, -1.4370e+00,\n",
            "         -1.4309e-01, -1.6768e+00, -4.0911e-01,  1.6911e+00, -1.5173e+00,\n",
            "         -1.2706e+00, -5.7205e-01, -1.0085e+00, -2.1360e-01, -2.8150e-01,\n",
            "         -8.8988e-01,  5.0481e-01,  1.0840e+00, -2.2043e-01,  1.0086e+00,\n",
            "         -1.9305e-02, -1.7078e+00, -9.6460e-01, -3.4400e+00, -2.4148e+00,\n",
            "         -2.1993e+00, -1.2597e+00, -1.2555e+00, -1.9830e-01, -1.6866e+00,\n",
            "          2.9997e-02, -5.3499e-01, -1.5775e+00, -1.9139e+00, -9.4521e-01,\n",
            "          8.6165e-01,  1.2755e+00,  5.0999e-01,  2.0510e-01,  2.1480e-01,\n",
            "          7.8257e-01,  1.9163e+00,  1.3435e-01,  1.0985e+00, -6.6549e-01,\n",
            "          1.9270e+00,  9.3208e-01,  7.5681e-01,  1.9260e+00,  2.3728e+00,\n",
            "          1.8288e+00,  1.1404e+00,  3.6203e-01,  2.6297e+00,  8.5519e-01,\n",
            "          9.2217e-03, -1.2653e+00, -2.4560e+00, -2.1079e+00, -2.5665e-01,\n",
            "         -1.1433e+00, -4.6930e-01,  1.0510e+00,  7.1532e-02, -3.9655e-01,\n",
            "          1.4786e-01,  4.4888e-01, -6.8913e-01, -1.8040e-01, -1.4134e+00,\n",
            "         -1.4945e+00, -1.8608e+00, -7.1153e-01, -1.1410e-01, -2.6646e+00,\n",
            "         -1.5295e+00, -2.7542e+00, -2.3233e+00, -1.7285e+00, -1.1236e+00,\n",
            "         -1.7544e+00, -1.8083e+00, -1.3234e+00, -2.0999e+00, -4.6811e-01,\n",
            "          2.1874e-01, -5.7045e-01, -1.8024e+00, -8.7432e-01, -2.9225e-01,\n",
            "         -3.0907e+00, -4.2412e-02, -1.3164e+00, -8.6320e-01, -8.4693e-02,\n",
            "         -9.3287e-01,  1.0649e+00, -6.3365e-01,  1.4300e-01, -2.6836e+00,\n",
            "         -2.4980e+00, -1.0856e+00, -9.2904e-01, -1.8198e+00, -2.3859e+00,\n",
            "         -4.1026e-01, -2.1216e+00, -1.0290e+00, -1.3809e+00, -1.1624e+00,\n",
            "         -2.0417e+00, -1.6841e+00, -2.3177e+00, -2.5739e+00, -3.2375e+00,\n",
            "         -1.5203e+00, -2.2220e+00, -2.6205e+00, -1.4579e+00, -3.3170e+00,\n",
            "         -1.2156e+00, -1.2983e+00, -1.6699e+00, -8.5985e-01, -1.5641e+00,\n",
            "         -5.3339e-01,  3.9597e-01, -2.3657e+00, -2.6069e+00, -3.0669e-01,\n",
            "         -5.4213e-01, -7.6417e-01, -9.4887e-02, -4.1761e-01,  7.2730e-01,\n",
            "         -1.7719e+00,  2.1189e-01,  8.7404e-01, -7.7859e-01, -1.0070e-01,\n",
            "          4.5921e-01, -1.0825e+00, -1.0545e+00, -2.1760e+00,  1.5862e+00,\n",
            "         -2.3500e+00,  2.5039e+00,  1.7545e+00,  8.8386e-01,  1.8365e+00,\n",
            "         -1.7247e+00,  1.8912e+00,  1.5523e+00,  2.7318e+00,  3.4079e+00,\n",
            "          2.7690e-01,  6.5950e-01, -6.1068e-01,  3.2763e-02, -8.4817e-01,\n",
            "         -1.4758e+00, -5.6919e-02, -6.5984e-01, -1.8647e-01,  1.3999e+00,\n",
            "         -1.8328e+00,  1.5119e+00,  3.2119e-01,  3.2321e-01,  1.2678e+00,\n",
            "          1.4766e+00, -8.8937e-01, -1.5284e-01,  1.2229e+00, -1.0189e+00,\n",
            "          1.0604e+00,  7.9149e-01, -1.1998e+00,  2.5182e+00, -1.1741e+00,\n",
            "         -1.6796e+00,  3.9548e+00,  2.0454e-01,  3.5014e-01, -1.8657e+00,\n",
            "          4.2371e-01,  7.1844e-01,  1.6900e+00,  3.2766e-01, -1.6981e+00,\n",
            "          1.8204e+00, -2.7593e-01,  8.7474e-01,  4.9691e-01,  6.7718e-01,\n",
            "          2.2056e-02, -4.3832e-01,  1.9388e+00,  1.3310e+00,  1.9079e+00,\n",
            "          1.4361e+00, -5.3958e-01, -3.0314e+00, -4.6412e-01, -3.1525e-01,\n",
            "          7.2283e-02, -9.1825e-01, -1.4580e+00,  1.8758e+00,  3.1047e+00,\n",
            "         -3.8865e-01, -3.8927e+00,  6.5368e-01,  2.9625e+00, -4.4407e-01,\n",
            "          2.3237e+00,  2.5180e+00,  1.7550e+00, -1.6200e+00,  1.5102e+00,\n",
            "          2.0459e+00,  8.6667e-02,  2.7858e+00, -3.1609e-01,  9.6396e-02,\n",
            "         -4.4778e-01, -1.6346e-01,  1.9302e+00, -9.2015e-01,  4.8689e-01,\n",
            "         -6.3908e-01,  1.0504e+00, -5.5943e-01, -4.5179e-02,  1.0524e+00,\n",
            "         -1.2735e+00,  5.4517e-01,  1.0086e+00,  7.6448e-01,  1.1804e+00,\n",
            "          2.8239e-01, -1.5603e+00,  1.9261e+00,  1.4770e+00, -3.1617e+00,\n",
            "          2.9498e-01, -1.4345e+00,  8.2647e-01,  1.4658e-02,  6.6903e-02,\n",
            "         -1.6559e-01,  1.1299e+00,  1.0681e+00,  2.6278e-01, -3.7086e-02,\n",
            "          1.4378e+00, -1.0325e-01, -8.7574e-02,  1.0251e+00, -1.2974e+00,\n",
            "         -6.4781e-01,  1.7668e+00,  1.4264e+00,  1.7404e+00,  5.5573e-01,\n",
            "          1.6749e+00,  8.0292e-01,  3.0695e-01,  3.2118e+00,  3.2465e+00,\n",
            "         -1.2985e+00, -4.8888e-01, -1.7664e-01,  5.4815e-01,  2.2078e+00,\n",
            "          4.0588e-01,  5.6065e-02,  1.7420e+00,  1.7903e+00, -1.0510e+00,\n",
            "          2.5948e+00, -5.7209e-01, -3.6156e+00, -5.1029e-01,  4.6594e+00,\n",
            "         -2.2597e+00,  7.9259e-02,  4.6195e-01,  1.5777e+00,  1.2795e-02,\n",
            "         -1.0540e+00,  4.2980e+00, -4.3069e-01,  1.1627e+00,  2.6978e+00,\n",
            "          1.2546e-01, -6.5848e-01, -5.0328e-01,  2.0424e+00,  1.2335e-01,\n",
            "         -2.9357e-01, -5.9324e-01, -2.3614e-01, -5.7356e-01, -2.2464e+00,\n",
            "          5.0021e-01, -4.3863e-02,  1.4947e+00,  2.1316e-01,  1.3493e+00,\n",
            "         -1.4183e+00, -4.1559e-01, -1.4797e+00, -7.8227e-01, -7.4676e-01,\n",
            "         -2.1457e+00, -1.2083e+00, -3.0566e+00,  5.0932e-01,  2.0140e+00,\n",
            "          5.8651e-01, -9.3287e-01,  1.7582e+00,  2.9210e+00,  6.6693e-01,\n",
            "         -6.2255e-01,  2.6034e+00,  2.1118e+00,  1.5372e+00, -1.4313e+00,\n",
            "         -1.9530e+00,  8.1313e-01,  9.1452e-01,  1.8467e+00,  1.0500e+00,\n",
            "          2.7728e+00,  4.1715e-01,  1.0590e+00, -1.1631e+00,  1.8784e+00,\n",
            "          2.9766e+00,  2.7239e+00, -1.0315e+00,  1.2289e+00, -1.2171e+00,\n",
            "          3.0063e+00,  2.1223e+00, -5.2256e-01,  1.7281e+00, -4.9329e-01,\n",
            "          9.6806e-01,  5.3049e-03,  7.0641e-02,  1.7237e+00,  1.5449e+00,\n",
            "          3.4690e+00, -2.2939e-01,  1.8387e+00,  1.1167e+00, -8.5865e-01,\n",
            "         -1.3493e+00,  2.1168e+00,  3.2618e-01,  5.1876e-01,  2.2075e+00,\n",
            "          3.7557e-01,  1.6760e+00,  2.7882e+00,  1.0880e+00, -3.0535e+00,\n",
            "          8.4145e-01,  1.8412e+00,  1.7165e+00,  5.2556e-01,  1.0274e+00,\n",
            "          1.2539e+00, -7.3669e-02, -4.2126e-03,  7.7846e-01,  2.3404e+00,\n",
            "         -2.3412e+00, -3.4380e-03,  2.0223e+00,  8.3366e-01, -2.9713e-01,\n",
            "          2.0206e+00,  2.3654e+00, -7.6537e-02, -7.3839e-01, -1.7224e+00,\n",
            "          9.2039e-01, -1.3620e+00,  1.3895e+00,  6.4870e-01,  1.1775e+00,\n",
            "         -8.8578e-01, -2.5500e+00,  3.9523e+00, -1.3988e+00,  2.8284e+00,\n",
            "          1.9585e-01,  4.9512e-01, -3.4395e-01, -7.1548e-01,  2.7762e+00,\n",
            "         -4.3234e-01, -1.4444e+00, -1.5124e-01,  2.4068e+00,  1.8473e+00,\n",
            "         -1.2998e+00,  7.0568e-01,  2.2554e+00,  3.3445e-01,  5.3089e-01,\n",
            "          2.2176e+00,  3.6494e+00,  2.7311e-01, -4.1407e-01,  1.1101e+00,\n",
            "          3.9293e-01,  1.2157e+00, -1.9813e+00,  8.0827e-01, -9.0830e-02,\n",
            "         -2.8414e+00,  8.1011e-01,  1.7691e+00,  3.3814e-01, -1.6508e+00,\n",
            "          9.4021e-01,  1.1124e+00,  9.6481e-01, -2.3686e+00, -4.1334e-01,\n",
            "          2.8095e+00,  1.0462e+00,  1.6676e+00, -2.2016e+00,  2.1204e+00,\n",
            "         -1.2295e+00, -3.0864e-01,  1.1013e+00, -2.0298e-01,  3.0804e+00,\n",
            "          2.7230e+00,  1.5348e+00,  4.3544e-01,  1.7178e+00,  2.3022e+00,\n",
            "         -1.7902e+00,  6.8146e-01, -2.7648e+00,  6.0601e-01,  1.3430e+00,\n",
            "          7.8596e-01,  2.2803e+00,  3.1669e+00,  1.4578e+00, -1.0865e+00,\n",
            "          1.6216e+00, -3.0828e-02,  1.9145e-01,  1.1150e+00, -8.5983e-01,\n",
            "         -1.9168e+00,  1.8844e+00,  1.7294e+00,  1.0344e+00, -1.0231e+00,\n",
            "          2.2638e+00,  9.1530e-01,  2.0043e+00,  4.8002e-01, -1.3611e+00,\n",
            "          3.0643e-01,  1.3967e+00,  2.8815e+00, -1.7072e-01, -1.1953e-01,\n",
            "          2.4738e+00,  3.1224e-01,  1.4171e+00,  2.7362e+00,  1.8758e+00,\n",
            "          2.0523e+00,  1.3579e-01,  1.3800e+00,  2.9496e+00,  1.5356e+00,\n",
            "         -4.0184e-01,  6.8604e-01, -1.9588e+00,  2.1365e-01,  1.6150e+00,\n",
            "          2.6759e+00,  3.0451e+00, -2.3126e+00,  1.6458e+00,  1.2921e+00,\n",
            "          7.4133e-01, -1.9891e+00,  3.1914e+00,  9.8808e-01,  2.8146e+00,\n",
            "          8.1760e-01,  2.1777e+00,  2.4774e+00,  1.8213e+00,  1.2064e+00,\n",
            "          1.7478e-01,  6.7100e-02,  1.5607e+00,  1.7297e+00, -1.6422e+00,\n",
            "         -5.9140e-01, -3.8390e-01,  3.6788e+00,  3.0195e+00,  2.8804e+00,\n",
            "          1.4293e+00,  9.3833e-01, -6.6961e-01, -8.0420e-01,  7.1620e-01,\n",
            "          1.7577e+00, -1.2739e-01,  1.6635e+00,  8.3802e-01,  2.9069e+00,\n",
            "          5.6641e-01,  1.6763e+00,  1.8023e+00,  1.2448e+00,  2.0955e+00,\n",
            "         -1.0899e+00, -2.1245e-01, -6.0020e-01, -9.0816e-01,  2.2363e+00,\n",
            "          9.8039e-01,  1.1757e+00, -2.8006e-01, -3.7230e-01, -8.1462e-01,\n",
            "          5.4205e-01,  3.0062e+00,  1.4590e+00,  1.9981e+00, -3.8694e-01,\n",
            "         -1.0073e+00,  1.0556e+00, -1.6753e+00,  7.5803e-01, -6.0949e-01,\n",
            "         -3.1086e+00, -1.0591e-01, -1.7571e+00,  7.6516e-01,  6.4611e-01,\n",
            "         -2.4765e+00, -1.5967e-01,  3.2859e-01,  1.9378e+00, -2.1090e+00,\n",
            "          8.7338e-01,  2.1155e+00, -1.3276e+00,  1.7302e+00,  1.9173e+00,\n",
            "         -1.2612e+00, -2.9642e-01,  1.4693e-01,  4.5323e-01,  4.0942e-01,\n",
            "          1.3924e+00,  1.1089e+00,  2.0894e+00, -9.1652e-01,  4.0333e+00,\n",
            "          2.6576e+00,  1.0109e+00, -2.1403e+00,  2.0370e+00,  8.8134e-01,\n",
            "         -5.4605e-01,  2.4844e+00,  2.3644e+00, -1.2937e+00, -9.2822e-02,\n",
            "          1.4165e+00, -3.0663e+00, -1.6304e+00, -4.7139e-01,  2.3929e+00,\n",
            "         -1.7792e+00,  2.4312e+00, -4.8613e-01, -1.3493e+00, -1.2224e+00,\n",
            "         -1.8676e+00, -9.6410e-01, -1.0620e+00,  2.6755e+00,  1.0670e+00,\n",
            "         -4.9472e-02, -2.7073e-01,  1.7069e+00, -2.4171e+00, -2.4395e+00,\n",
            "          8.4427e-01,  1.8568e+00, -3.3393e-01, -1.0088e+00,  4.9052e-01,\n",
            "          1.6485e-01, -1.5486e+00,  2.6333e+00,  1.6328e+00,  8.0100e-02,\n",
            "          2.5880e+00, -2.4305e-01, -7.4340e-01, -1.1761e+00, -8.7806e-01,\n",
            "         -7.9167e-01,  1.2231e-01,  3.7894e+00,  3.9957e+00,  1.7972e+00,\n",
            "          1.6989e-01,  1.5844e+00,  2.3741e+00,  2.3753e+00,  9.8466e-01,\n",
            "          3.1950e-01,  4.5638e-02,  2.1377e+00, -2.0682e-01,  3.0612e+00,\n",
            "          1.3553e+00,  2.0595e-01,  7.0213e-01,  1.1843e+00, -1.0957e-03,\n",
            "          1.5783e+00,  1.2345e+00, -8.8698e-01,  1.2086e+00, -8.9089e-01,\n",
            "         -2.1589e+00, -3.0648e-01, -7.0305e-02,  7.7369e-01,  6.8312e-01,\n",
            "          3.3207e-01,  3.9479e+00,  1.3313e+00, -6.7767e-01, -2.5756e+00,\n",
            "         -3.9708e-01, -3.5840e+00, -1.7997e+00, -1.3608e+00,  6.8440e-01,\n",
            "         -2.6866e+00, -6.0533e-01, -1.8762e-01, -1.6253e+00, -2.0824e+00,\n",
            "         -2.0242e+00, -3.0601e+00, -7.4261e-01, -2.3904e+00,  9.2577e-01,\n",
            "         -3.9063e-01, -1.3786e+00,  1.2586e-01,  7.4255e-01, -2.4181e+00,\n",
            "          1.0531e-01, -3.1193e+00, -2.6690e+00, -4.4924e-01, -7.9893e-01,\n",
            "         -9.3520e-02,  1.3396e-01, -6.1872e-01, -1.1629e+00, -3.3199e-01,\n",
            "         -2.1327e+00, -1.0135e+00, -9.2695e-01, -1.4048e+00, -2.8948e+00,\n",
            "         -1.1022e+00, -8.0229e-01, -1.4796e+00, -1.4841e+00, -2.0789e+00,\n",
            "         -2.1930e+00,  1.1956e+00, -2.4292e-01,  1.0051e+00, -4.8636e-01,\n",
            "         -1.4232e+00,  6.1629e-01, -2.0758e+00, -2.9270e+00, -1.1477e+00,\n",
            "          4.4929e-01, -9.1548e-01,  1.1020e+00,  2.3688e-01, -2.5248e+00,\n",
            "         -3.6948e-01,  8.5381e-01, -1.5050e+00, -1.2825e+00, -5.6968e-01,\n",
            "         -2.0302e+00, -2.5335e+00, -7.0767e-01,  8.4252e-01, -4.1883e-01,\n",
            "         -1.0758e+00, -2.6735e+00, -2.8144e+00, -3.3358e+00, -3.7288e+00,\n",
            "         -2.5903e+00, -2.7828e+00, -2.5995e+00, -2.2204e+00,  3.2923e+00]],\n",
            "       grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KE9t1QHZt20a"
      },
      "source": [
        "We'll talk about this soon (in Transfer Learning)\n",
        "- See this tutorial: [Fine tuning Torchvision Models](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyJ4xu0SYgKB"
      },
      "source": [
        "# **Regularization** for Deep Learning (Part two): \n",
        "\n",
        "- Ensemble Methods: Bagging \n",
        "- Dropout\n",
        "- Adversarial Training: Example Generation and Robustness (Two detailed examples - I'll refer you to them) : [Tutorial1](https://pytorch.org/tutorials/beginner/fgsm_tutorial.html) - [Tutorial2](https://adversarial-ml-tutorial.org/introduction/)  \n",
        "\n",
        "- Physics Informed Neural Network (A bright exmple of regularization technique) : See this [PINNs TF Implementation](https://github.com/pierremtb/PINNs-TF2.0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifJVLnvmY1Hg"
      },
      "source": [
        "**Ensemble Methods**: Bagging as well as other methods\n",
        "\n",
        "* Building a custom (ensemble) model in Pytorch. Take \n",
        " a look at this [dicussion](https://discuss.pytorch.org/t/custom-ensemble-approach/52024)\n",
        "* [Ensemble-Pytorch](https://github.com/AaronX121/Ensemble-Pytorch): Implementation of scikit-learn like ensemble methods in Pytorch\n",
        "* Also, The [sklearn.ensemble](https://scikit-learn.org/stable/modules/ensemble.html) module provides extensive ensembles based on available models in [sklearn](https://scikit-learn.org/stable/index.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYpA85hxceO1"
      },
      "source": [
        "# Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEuDeiaTcpGF"
      },
      "source": [
        "First a bench mark model: \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh5sOsj62kpu"
      },
      "source": [
        "# Modified based on https://github.com/pytorch/examples/tree/master/mnist\n",
        "\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "       # self.dropout1 = nn.Dropout(0.25)\n",
        "       # self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "       # x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "       # x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "\n",
        "def train(args, model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "      loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "            100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P23yJQ9x7l5v"
      },
      "source": [
        "    torch.manual_seed(112)\n",
        "    device =torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKJOmSyb75uF",
        "outputId": "25e2b3ba-084e-4319-9536-a983d64528da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391,
          "referenced_widgets": [
            "e8f50a996ba7449685580972fa95e8ff",
            "2cf9972a7e5543ed8029021601fcb908",
            "f0369dcbb46d463595d69bd2cec3221b",
            "7ca006f569704503902cc1ee1d943ba5",
            "ef1b569fe7b7460da33ceee888e57b63",
            "cb31ab9f8e7b4b24b820148b1753ef02",
            "c0d96acd4168489a906a4e6e48c85733",
            "45630e75a1974e7389715dce3d2543a3",
            "2912881af8024025ade72453a9615328",
            "61b0f97be738468c94ebd8bab4b37fea",
            "913f4c2f1d99435788dc6db9c04eb371",
            "89e6e4490ee3403da9405f428c3b1230",
            "3d4dd4b0185347dea7c0a35067a2a19e",
            "f07204650e654eb1b854db2b548ff0e2",
            "19ddd7894ab34501b0b403b1c9fc7db7",
            "d66b6a97b06c4c3c95a7ad22e88e0d7d",
            "ed123e564e984c81a923b2a757e61bbf",
            "59ae9a25ddbc47f0af89168783ffae3a",
            "0553946a0b484b30888700f3fb54b5e0",
            "61a9e0913e044ab99d01b29903d907f8",
            "c53ec8f2a7914f11bcadceeed9c7231d",
            "8c46cd4ab7aa4fde93248f5d047448a7",
            "da0983de1ef94e67b23d6f66026dbf2c",
            "66154f1de98a4ac69d8f9a24e715b2d8",
            "0124af76a91c499883e6022e0056865f",
            "673ce309375c44a49a056954bfbd9324",
            "52d7d3a112444034b4be9ea5df134541",
            "7aa804904b06450bb32f712bd0c64709",
            "3b59821f57384de19bc1daa29ade79d5",
            "8910fa720078441bbe40d3974a527f27",
            "cd4210e39e514dc89c7c21b944e3c560",
            "d7f0f9c85d534297a4569d758aa35b09"
          ]
        }
      },
      "source": [
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "    dataset_train = datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transform)\n",
        "    dataset_test = datasets.MNIST('../data', train=False,\n",
        "                       transform=transform)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8f50a996ba7449685580972fa95e8ff",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2912881af8024025ade72453a9615328",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed123e564e984c81a923b2a757e61bbf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0124af76a91c499883e6022e0056865f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEPKmYcp8Ag-",
        "outputId": "052b7fdc-6d14-4661-aacb-5007ee68aa4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "dataset_train, dataset_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Dataset MNIST\n",
              "     Number of datapoints: 60000\n",
              "     Root location: ../data\n",
              "     Split: Train\n",
              "     StandardTransform\n",
              " Transform: Compose(\n",
              "                ToTensor()\n",
              "                Normalize(mean=(0.1307,), std=(0.3081,))\n",
              "            ), Dataset MNIST\n",
              "     Number of datapoints: 10000\n",
              "     Root location: ../data\n",
              "     Split: Test\n",
              "     StandardTransform\n",
              " Transform: Compose(\n",
              "                ToTensor()\n",
              "                Normalize(mean=(0.1307,), std=(0.3081,))\n",
              "            ))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIe8ycaY5z6j",
        "outputId": "9545fdcf-80fd-4208-b90b-a1c6621623df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "torch.manual_seed(123)\n",
        "transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transform)\n",
        "dataset2 = datasets.MNIST('../data', train=False,\n",
        "                       transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=256 , shuffle=True,\n",
        "        num_workers=1,pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=256 , shuffle=True,\n",
        "        num_workers=1,pin_memory=True)\n",
        "\n",
        "model = Net().to(device)\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=0.01)\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.1 )\n",
        "for epoch in range(1, 20 + 1):\n",
        "  train(10, model, device, train_loader, optimizer, epoch)\n",
        "  test(model, device, test_loader)\n",
        "  scheduler.step()\n",
        "\n",
        "torch.save(model.state_dict(), \"mnist_1.pt\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.307113\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.246330\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.152776\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.053525\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.883935\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.724492\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.479796\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 1.235827\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 1.034860\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.853716\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.772773\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.717264\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.640164\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.587384\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.539963\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.491984\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.530113\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.548230\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.513601\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.443843\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.422450\n",
            "Train Epoch: 1 [53760/60000 (89%)]\tLoss: 0.472074\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.357606\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.362400\n",
            "\n",
            "Test set: Average loss: 0.3642, Accuracy: 9035/10000 (90%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.381449\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.387114\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.397163\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.414457\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.343093\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.273364\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.326993\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.314593\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.321126\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.387198\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.378030\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.331200\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.342840\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.343132\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.324589\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.333907\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.324017\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.293872\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.334337\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.271919\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.287716\n",
            "Train Epoch: 2 [53760/60000 (89%)]\tLoss: 0.318096\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.299280\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.344123\n",
            "\n",
            "Test set: Average loss: 0.2809, Accuracy: 9207/10000 (92%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.296676\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.289966\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.251180\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.212233\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.346251\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.258924\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.240553\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.212924\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.343948\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.245781\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.285688\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.260310\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.262925\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.284416\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.274247\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.369636\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.255639\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.205742\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.348702\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.281335\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.236348\n",
            "Train Epoch: 3 [53760/60000 (89%)]\tLoss: 0.352732\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.248686\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.212725\n",
            "\n",
            "Test set: Average loss: 0.2476, Accuracy: 9280/10000 (93%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.365744\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.298277\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.275118\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.264347\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.267643\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.212297\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.201517\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.336612\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.206569\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.276591\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.226653\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.232043\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.228995\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.204350\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.241334\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.236407\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.157513\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.246724\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.289585\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.217862\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.190122\n",
            "Train Epoch: 4 [53760/60000 (89%)]\tLoss: 0.219552\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.211791\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.274863\n",
            "\n",
            "Test set: Average loss: 0.2277, Accuracy: 9347/10000 (93%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.275364\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.163495\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.211634\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.221368\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.187051\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.239258\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.234825\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.318659\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.197251\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.191773\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.308626\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.203728\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.177126\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.165913\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.266534\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.222940\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.152253\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.176469\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.196336\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.215812\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.148008\n",
            "Train Epoch: 5 [53760/60000 (89%)]\tLoss: 0.309382\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.240932\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.215547\n",
            "\n",
            "Test set: Average loss: 0.1976, Accuracy: 9422/10000 (94%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.191428\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.185932\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.159369\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.145110\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.135501\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.225197\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.152268\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.287638\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.229681\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.169077\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.144530\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.168833\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.242072\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.194182\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.202316\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.237667\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.201214\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.244648\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.197431\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.248213\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.174230\n",
            "Train Epoch: 6 [53760/60000 (89%)]\tLoss: 0.301559\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.182137\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.264559\n",
            "\n",
            "Test set: Average loss: 0.1856, Accuracy: 9463/10000 (95%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.146918\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.168449\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.155278\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.163332\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.175116\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.181375\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.213896\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.204193\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.190938\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.207276\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.172655\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.165982\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.135855\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.138108\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.171701\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.235605\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.179632\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.225231\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.211486\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.136773\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.201701\n",
            "Train Epoch: 7 [53760/60000 (89%)]\tLoss: 0.305821\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.157176\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.121435\n",
            "\n",
            "Test set: Average loss: 0.1828, Accuracy: 9459/10000 (95%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.141818\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.203620\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.138092\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.145350\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.207396\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.191759\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.171701\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.187539\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.153208\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.180300\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.223382\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.187814\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.161183\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.275724\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.253460\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.138390\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.155385\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.214176\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.185736\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.156547\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.192535\n",
            "Train Epoch: 8 [53760/60000 (89%)]\tLoss: 0.141879\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.140166\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.187109\n",
            "\n",
            "Test set: Average loss: 0.1807, Accuracy: 9482/10000 (95%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.187306\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.093368\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.143789\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.241786\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.203427\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.143240\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.231581\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.163383\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.267532\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.157525\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.200164\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.324732\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.198912\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.243967\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.212492\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.137252\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.097101\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.142338\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.177393\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.209538\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.205792\n",
            "Train Epoch: 9 [53760/60000 (89%)]\tLoss: 0.280951\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.166479\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.187483\n",
            "\n",
            "Test set: Average loss: 0.1794, Accuracy: 9479/10000 (95%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.188699\n",
            "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.135091\n",
            "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.278972\n",
            "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.160426\n",
            "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.179500\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.147554\n",
            "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.152743\n",
            "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.241557\n",
            "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.163514\n",
            "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.172314\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.173245\n",
            "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.130719\n",
            "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.237941\n",
            "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.137331\n",
            "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.163065\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.211750\n",
            "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.138772\n",
            "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.237939\n",
            "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.218149\n",
            "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.208572\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.157576\n",
            "Train Epoch: 10 [53760/60000 (89%)]\tLoss: 0.160953\n",
            "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.122246\n",
            "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.149422\n",
            "\n",
            "Test set: Average loss: 0.1774, Accuracy: 9495/10000 (95%)\n",
            "\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.157060\n",
            "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 0.122874\n",
            "Train Epoch: 11 [5120/60000 (9%)]\tLoss: 0.226685\n",
            "Train Epoch: 11 [7680/60000 (13%)]\tLoss: 0.109502\n",
            "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 0.176309\n",
            "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.294165\n",
            "Train Epoch: 11 [15360/60000 (26%)]\tLoss: 0.158714\n",
            "Train Epoch: 11 [17920/60000 (30%)]\tLoss: 0.150464\n",
            "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.152675\n",
            "Train Epoch: 11 [23040/60000 (38%)]\tLoss: 0.202501\n",
            "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.154528\n",
            "Train Epoch: 11 [28160/60000 (47%)]\tLoss: 0.147456\n",
            "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.171763\n",
            "Train Epoch: 11 [33280/60000 (55%)]\tLoss: 0.145884\n",
            "Train Epoch: 11 [35840/60000 (60%)]\tLoss: 0.227826\n",
            "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.155230\n",
            "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 0.179676\n",
            "Train Epoch: 11 [43520/60000 (72%)]\tLoss: 0.173603\n",
            "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 0.209978\n",
            "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 0.186613\n",
            "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.158794\n",
            "Train Epoch: 11 [53760/60000 (89%)]\tLoss: 0.191946\n",
            "Train Epoch: 11 [56320/60000 (94%)]\tLoss: 0.225307\n",
            "Train Epoch: 11 [58880/60000 (98%)]\tLoss: 0.221299\n",
            "\n",
            "Test set: Average loss: 0.1771, Accuracy: 9494/10000 (95%)\n",
            "\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.171638\n",
            "Train Epoch: 12 [2560/60000 (4%)]\tLoss: 0.200342\n",
            "Train Epoch: 12 [5120/60000 (9%)]\tLoss: 0.235149\n",
            "Train Epoch: 12 [7680/60000 (13%)]\tLoss: 0.139043\n",
            "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 0.226337\n",
            "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.208545\n",
            "Train Epoch: 12 [15360/60000 (26%)]\tLoss: 0.187146\n",
            "Train Epoch: 12 [17920/60000 (30%)]\tLoss: 0.181952\n",
            "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 0.179988\n",
            "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 0.232278\n",
            "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.137126\n",
            "Train Epoch: 12 [28160/60000 (47%)]\tLoss: 0.173634\n",
            "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 0.184903\n",
            "Train Epoch: 12 [33280/60000 (55%)]\tLoss: 0.142646\n",
            "Train Epoch: 12 [35840/60000 (60%)]\tLoss: 0.239662\n",
            "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.157015\n",
            "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.218006\n",
            "Train Epoch: 12 [43520/60000 (72%)]\tLoss: 0.168534\n",
            "Train Epoch: 12 [46080/60000 (77%)]\tLoss: 0.155817\n",
            "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 0.151685\n",
            "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.271424\n",
            "Train Epoch: 12 [53760/60000 (89%)]\tLoss: 0.218201\n",
            "Train Epoch: 12 [56320/60000 (94%)]\tLoss: 0.146335\n",
            "Train Epoch: 12 [58880/60000 (98%)]\tLoss: 0.154123\n",
            "\n",
            "Test set: Average loss: 0.1768, Accuracy: 9490/10000 (95%)\n",
            "\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.196438\n",
            "Train Epoch: 13 [2560/60000 (4%)]\tLoss: 0.178500\n",
            "Train Epoch: 13 [5120/60000 (9%)]\tLoss: 0.247617\n",
            "Train Epoch: 13 [7680/60000 (13%)]\tLoss: 0.148203\n",
            "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 0.254861\n",
            "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.206857\n",
            "Train Epoch: 13 [15360/60000 (26%)]\tLoss: 0.162599\n",
            "Train Epoch: 13 [17920/60000 (30%)]\tLoss: 0.219827\n",
            "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.182422\n",
            "Train Epoch: 13 [23040/60000 (38%)]\tLoss: 0.187031\n",
            "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.182751\n",
            "Train Epoch: 13 [28160/60000 (47%)]\tLoss: 0.204018\n",
            "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 0.203286\n",
            "Train Epoch: 13 [33280/60000 (55%)]\tLoss: 0.191987\n",
            "Train Epoch: 13 [35840/60000 (60%)]\tLoss: 0.176741\n",
            "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.190892\n",
            "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.192314\n",
            "Train Epoch: 13 [43520/60000 (72%)]\tLoss: 0.124297\n",
            "Train Epoch: 13 [46080/60000 (77%)]\tLoss: 0.146513\n",
            "Train Epoch: 13 [48640/60000 (81%)]\tLoss: 0.221885\n",
            "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.171415\n",
            "Train Epoch: 13 [53760/60000 (89%)]\tLoss: 0.135726\n",
            "Train Epoch: 13 [56320/60000 (94%)]\tLoss: 0.193544\n",
            "Train Epoch: 13 [58880/60000 (98%)]\tLoss: 0.280918\n",
            "\n",
            "Test set: Average loss: 0.1766, Accuracy: 9491/10000 (95%)\n",
            "\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.170310\n",
            "Train Epoch: 14 [2560/60000 (4%)]\tLoss: 0.148429\n",
            "Train Epoch: 14 [5120/60000 (9%)]\tLoss: 0.275483\n",
            "Train Epoch: 14 [7680/60000 (13%)]\tLoss: 0.206962\n",
            "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 0.136207\n",
            "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.228239\n",
            "Train Epoch: 14 [15360/60000 (26%)]\tLoss: 0.138180\n",
            "Train Epoch: 14 [17920/60000 (30%)]\tLoss: 0.123770\n",
            "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 0.159469\n",
            "Train Epoch: 14 [23040/60000 (38%)]\tLoss: 0.159852\n",
            "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.186142\n",
            "Train Epoch: 14 [28160/60000 (47%)]\tLoss: 0.248508\n",
            "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 0.218626\n",
            "Train Epoch: 14 [33280/60000 (55%)]\tLoss: 0.155976\n",
            "Train Epoch: 14 [35840/60000 (60%)]\tLoss: 0.163481\n",
            "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.163534\n",
            "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 0.222581\n",
            "Train Epoch: 14 [43520/60000 (72%)]\tLoss: 0.258513\n",
            "Train Epoch: 14 [46080/60000 (77%)]\tLoss: 0.177320\n",
            "Train Epoch: 14 [48640/60000 (81%)]\tLoss: 0.210302\n",
            "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.162361\n",
            "Train Epoch: 14 [53760/60000 (89%)]\tLoss: 0.198126\n",
            "Train Epoch: 14 [56320/60000 (94%)]\tLoss: 0.142096\n",
            "Train Epoch: 14 [58880/60000 (98%)]\tLoss: 0.150204\n",
            "\n",
            "Test set: Average loss: 0.1764, Accuracy: 9489/10000 (95%)\n",
            "\n",
            "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.142897\n",
            "Train Epoch: 15 [2560/60000 (4%)]\tLoss: 0.226468\n",
            "Train Epoch: 15 [5120/60000 (9%)]\tLoss: 0.176312\n",
            "Train Epoch: 15 [7680/60000 (13%)]\tLoss: 0.143165\n",
            "Train Epoch: 15 [10240/60000 (17%)]\tLoss: 0.201193\n",
            "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.201765\n",
            "Train Epoch: 15 [15360/60000 (26%)]\tLoss: 0.162782\n",
            "Train Epoch: 15 [17920/60000 (30%)]\tLoss: 0.196929\n",
            "Train Epoch: 15 [20480/60000 (34%)]\tLoss: 0.086969\n",
            "Train Epoch: 15 [23040/60000 (38%)]\tLoss: 0.291206\n",
            "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.155715\n",
            "Train Epoch: 15 [28160/60000 (47%)]\tLoss: 0.127642\n",
            "Train Epoch: 15 [30720/60000 (51%)]\tLoss: 0.192388\n",
            "Train Epoch: 15 [33280/60000 (55%)]\tLoss: 0.171720\n",
            "Train Epoch: 15 [35840/60000 (60%)]\tLoss: 0.226487\n",
            "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.188624\n",
            "Train Epoch: 15 [40960/60000 (68%)]\tLoss: 0.196191\n",
            "Train Epoch: 15 [43520/60000 (72%)]\tLoss: 0.215683\n",
            "Train Epoch: 15 [46080/60000 (77%)]\tLoss: 0.152513\n",
            "Train Epoch: 15 [48640/60000 (81%)]\tLoss: 0.171465\n",
            "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.245127\n",
            "Train Epoch: 15 [53760/60000 (89%)]\tLoss: 0.164409\n",
            "Train Epoch: 15 [56320/60000 (94%)]\tLoss: 0.181886\n",
            "Train Epoch: 15 [58880/60000 (98%)]\tLoss: 0.320708\n",
            "\n",
            "Test set: Average loss: 0.1762, Accuracy: 9490/10000 (95%)\n",
            "\n",
            "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.286059\n",
            "Train Epoch: 16 [2560/60000 (4%)]\tLoss: 0.244766\n",
            "Train Epoch: 16 [5120/60000 (9%)]\tLoss: 0.176173\n",
            "Train Epoch: 16 [7680/60000 (13%)]\tLoss: 0.144383\n",
            "Train Epoch: 16 [10240/60000 (17%)]\tLoss: 0.215431\n",
            "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 0.162181\n",
            "Train Epoch: 16 [15360/60000 (26%)]\tLoss: 0.168657\n",
            "Train Epoch: 16 [17920/60000 (30%)]\tLoss: 0.212809\n",
            "Train Epoch: 16 [20480/60000 (34%)]\tLoss: 0.159811\n",
            "Train Epoch: 16 [23040/60000 (38%)]\tLoss: 0.259408\n",
            "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.220739\n",
            "Train Epoch: 16 [28160/60000 (47%)]\tLoss: 0.170455\n",
            "Train Epoch: 16 [30720/60000 (51%)]\tLoss: 0.223283\n",
            "Train Epoch: 16 [33280/60000 (55%)]\tLoss: 0.237474\n",
            "Train Epoch: 16 [35840/60000 (60%)]\tLoss: 0.184958\n",
            "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 0.170403\n",
            "Train Epoch: 16 [40960/60000 (68%)]\tLoss: 0.230805\n",
            "Train Epoch: 16 [43520/60000 (72%)]\tLoss: 0.183524\n",
            "Train Epoch: 16 [46080/60000 (77%)]\tLoss: 0.201270\n",
            "Train Epoch: 16 [48640/60000 (81%)]\tLoss: 0.192238\n",
            "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.147130\n",
            "Train Epoch: 16 [53760/60000 (89%)]\tLoss: 0.161835\n",
            "Train Epoch: 16 [56320/60000 (94%)]\tLoss: 0.190943\n",
            "Train Epoch: 16 [58880/60000 (98%)]\tLoss: 0.208388\n",
            "\n",
            "Test set: Average loss: 0.1762, Accuracy: 9490/10000 (95%)\n",
            "\n",
            "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.138966\n",
            "Train Epoch: 17 [2560/60000 (4%)]\tLoss: 0.101348\n",
            "Train Epoch: 17 [5120/60000 (9%)]\tLoss: 0.160668\n",
            "Train Epoch: 17 [7680/60000 (13%)]\tLoss: 0.140514\n",
            "Train Epoch: 17 [10240/60000 (17%)]\tLoss: 0.104047\n",
            "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 0.167567\n",
            "Train Epoch: 17 [15360/60000 (26%)]\tLoss: 0.293577\n",
            "Train Epoch: 17 [17920/60000 (30%)]\tLoss: 0.118544\n",
            "Train Epoch: 17 [20480/60000 (34%)]\tLoss: 0.169215\n",
            "Train Epoch: 17 [23040/60000 (38%)]\tLoss: 0.229251\n",
            "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.133027\n",
            "Train Epoch: 17 [28160/60000 (47%)]\tLoss: 0.242444\n",
            "Train Epoch: 17 [30720/60000 (51%)]\tLoss: 0.204847\n",
            "Train Epoch: 17 [33280/60000 (55%)]\tLoss: 0.191603\n",
            "Train Epoch: 17 [35840/60000 (60%)]\tLoss: 0.168487\n",
            "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 0.177132\n",
            "Train Epoch: 17 [40960/60000 (68%)]\tLoss: 0.128301\n",
            "Train Epoch: 17 [43520/60000 (72%)]\tLoss: 0.159495\n",
            "Train Epoch: 17 [46080/60000 (77%)]\tLoss: 0.170750\n",
            "Train Epoch: 17 [48640/60000 (81%)]\tLoss: 0.167845\n",
            "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.161665\n",
            "Train Epoch: 17 [53760/60000 (89%)]\tLoss: 0.196799\n",
            "Train Epoch: 17 [56320/60000 (94%)]\tLoss: 0.193543\n",
            "Train Epoch: 17 [58880/60000 (98%)]\tLoss: 0.196501\n",
            "\n",
            "Test set: Average loss: 0.1762, Accuracy: 9490/10000 (95%)\n",
            "\n",
            "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.178977\n",
            "Train Epoch: 18 [2560/60000 (4%)]\tLoss: 0.120702\n",
            "Train Epoch: 18 [5120/60000 (9%)]\tLoss: 0.207902\n",
            "Train Epoch: 18 [7680/60000 (13%)]\tLoss: 0.183116\n",
            "Train Epoch: 18 [10240/60000 (17%)]\tLoss: 0.139938\n",
            "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 0.208946\n",
            "Train Epoch: 18 [15360/60000 (26%)]\tLoss: 0.194912\n",
            "Train Epoch: 18 [17920/60000 (30%)]\tLoss: 0.114899\n",
            "Train Epoch: 18 [20480/60000 (34%)]\tLoss: 0.253203\n",
            "Train Epoch: 18 [23040/60000 (38%)]\tLoss: 0.170223\n",
            "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.147134\n",
            "Train Epoch: 18 [28160/60000 (47%)]\tLoss: 0.220090\n",
            "Train Epoch: 18 [30720/60000 (51%)]\tLoss: 0.222176\n",
            "Train Epoch: 18 [33280/60000 (55%)]\tLoss: 0.214211\n",
            "Train Epoch: 18 [35840/60000 (60%)]\tLoss: 0.189911\n",
            "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 0.167564\n",
            "Train Epoch: 18 [40960/60000 (68%)]\tLoss: 0.183680\n",
            "Train Epoch: 18 [43520/60000 (72%)]\tLoss: 0.236313\n",
            "Train Epoch: 18 [46080/60000 (77%)]\tLoss: 0.231399\n",
            "Train Epoch: 18 [48640/60000 (81%)]\tLoss: 0.144449\n",
            "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.214758\n",
            "Train Epoch: 18 [53760/60000 (89%)]\tLoss: 0.212750\n",
            "Train Epoch: 18 [56320/60000 (94%)]\tLoss: 0.147158\n",
            "Train Epoch: 18 [58880/60000 (98%)]\tLoss: 0.192718\n",
            "\n",
            "Test set: Average loss: 0.1762, Accuracy: 9490/10000 (95%)\n",
            "\n",
            "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.263781\n",
            "Train Epoch: 19 [2560/60000 (4%)]\tLoss: 0.183720\n",
            "Train Epoch: 19 [5120/60000 (9%)]\tLoss: 0.176208\n",
            "Train Epoch: 19 [7680/60000 (13%)]\tLoss: 0.165672\n",
            "Train Epoch: 19 [10240/60000 (17%)]\tLoss: 0.133228\n",
            "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 0.263556\n",
            "Train Epoch: 19 [15360/60000 (26%)]\tLoss: 0.195063\n",
            "Train Epoch: 19 [17920/60000 (30%)]\tLoss: 0.162448\n",
            "Train Epoch: 19 [20480/60000 (34%)]\tLoss: 0.174943\n",
            "Train Epoch: 19 [23040/60000 (38%)]\tLoss: 0.253810\n",
            "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.267342\n",
            "Train Epoch: 19 [28160/60000 (47%)]\tLoss: 0.197658\n",
            "Train Epoch: 19 [30720/60000 (51%)]\tLoss: 0.153706\n",
            "Train Epoch: 19 [33280/60000 (55%)]\tLoss: 0.179995\n",
            "Train Epoch: 19 [35840/60000 (60%)]\tLoss: 0.229466\n",
            "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 0.167240\n",
            "Train Epoch: 19 [40960/60000 (68%)]\tLoss: 0.165942\n",
            "Train Epoch: 19 [43520/60000 (72%)]\tLoss: 0.116418\n",
            "Train Epoch: 19 [46080/60000 (77%)]\tLoss: 0.169974\n",
            "Train Epoch: 19 [48640/60000 (81%)]\tLoss: 0.171786\n",
            "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.139511\n",
            "Train Epoch: 19 [53760/60000 (89%)]\tLoss: 0.199231\n",
            "Train Epoch: 19 [56320/60000 (94%)]\tLoss: 0.191598\n",
            "Train Epoch: 19 [58880/60000 (98%)]\tLoss: 0.294211\n",
            "\n",
            "Test set: Average loss: 0.1762, Accuracy: 9490/10000 (95%)\n",
            "\n",
            "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.155055\n",
            "Train Epoch: 20 [2560/60000 (4%)]\tLoss: 0.178612\n",
            "Train Epoch: 20 [5120/60000 (9%)]\tLoss: 0.142860\n",
            "Train Epoch: 20 [7680/60000 (13%)]\tLoss: 0.211385\n",
            "Train Epoch: 20 [10240/60000 (17%)]\tLoss: 0.220983\n",
            "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 0.120255\n",
            "Train Epoch: 20 [15360/60000 (26%)]\tLoss: 0.164219\n",
            "Train Epoch: 20 [17920/60000 (30%)]\tLoss: 0.172308\n",
            "Train Epoch: 20 [20480/60000 (34%)]\tLoss: 0.181552\n",
            "Train Epoch: 20 [23040/60000 (38%)]\tLoss: 0.150278\n",
            "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.171441\n",
            "Train Epoch: 20 [28160/60000 (47%)]\tLoss: 0.249869\n",
            "Train Epoch: 20 [30720/60000 (51%)]\tLoss: 0.227506\n",
            "Train Epoch: 20 [33280/60000 (55%)]\tLoss: 0.154207\n",
            "Train Epoch: 20 [35840/60000 (60%)]\tLoss: 0.205372\n",
            "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 0.186253\n",
            "Train Epoch: 20 [40960/60000 (68%)]\tLoss: 0.225461\n",
            "Train Epoch: 20 [43520/60000 (72%)]\tLoss: 0.205352\n",
            "Train Epoch: 20 [46080/60000 (77%)]\tLoss: 0.189648\n",
            "Train Epoch: 20 [48640/60000 (81%)]\tLoss: 0.197712\n",
            "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.125356\n",
            "Train Epoch: 20 [53760/60000 (89%)]\tLoss: 0.201266\n",
            "Train Epoch: 20 [56320/60000 (94%)]\tLoss: 0.160205\n",
            "Train Epoch: 20 [58880/60000 (98%)]\tLoss: 0.157071\n",
            "\n",
            "Test set: Average loss: 0.1762, Accuracy: 9490/10000 (95%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY2XY7LVKSYI"
      },
      "source": [
        "Now let's start:\n",
        "\n",
        " **Torch.dropout**: https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK6w8tJeHlir",
        "outputId": "6406df78-1edc-4e7d-e75c-25c416548080",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dropout_layer = nn.Dropout(1)\n",
        "input = torch.randn(2, 20)\n",
        "output = dropout_layer(input)\n",
        "print(input)\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.2393, -0.8378,  0.8559, -1.3261,  0.0330,  1.2033,  0.4886, -0.2248,\n",
            "          2.1273,  0.3787, -0.8657,  1.5655, -0.9437, -1.9373,  0.1816, -0.5516,\n",
            "         -1.7298,  1.1813, -0.1465, -1.0134],\n",
            "        [-0.0535,  1.7472, -0.8287, -0.6501, -0.1857,  1.2836,  0.6029, -0.2008,\n",
            "         -2.5054,  1.4367,  0.9614, -0.8520, -1.1531, -2.1133, -0.2215,  1.1383,\n",
            "          0.1116, -1.4435, -0.9796, -0.3777]])\n",
            "tensor([[-0., -0., 0., -0., 0., 0., 0., -0., 0., 0., -0., 0., -0., -0., 0., -0., -0., 0., -0., -0.],\n",
            "        [-0., 0., -0., -0., -0., 0., 0., -0., -0., 0., 0., -0., -0., -0., -0., 0., 0., -0., -0., -0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIySiX5WHli-",
        "outputId": "934a6002-da8b-4b41-f5c4-8ca2c081614e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dropout_layer = nn.Dropout(0.25)\n",
        "input = torch.randn(2, 20)\n",
        "output = dropout_layer(input)\n",
        "print(input)\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 2.0883, -0.3782, -0.1384,  0.4667,  0.7491,  1.3325, -0.4197,  1.2035,\n",
            "         -0.1668,  1.5479, -0.8813,  0.7535,  0.3926, -0.2745, -0.6635,  1.1863,\n",
            "          1.7974, -0.2479, -0.3005,  1.4417],\n",
            "        [-1.0637, -0.1633,  1.2450, -0.8118,  0.0041, -0.8165,  0.3768, -1.0938,\n",
            "         -1.5452, -1.2712, -1.1258, -1.6945, -0.5335, -0.1772,  0.3597, -1.6240,\n",
            "          0.1669,  0.0416, -1.1723, -0.5961]])\n",
            "tensor([[ 2.7844, -0.5042, -0.0000,  0.6223,  0.9988,  1.7767, -0.5596,  0.0000,\n",
            "         -0.0000,  0.0000, -1.1750,  1.0047,  0.5235, -0.3660, -0.8846,  0.0000,\n",
            "          2.3965, -0.0000, -0.4007,  1.9222],\n",
            "        [-1.4182, -0.2178,  0.0000, -0.0000,  0.0055, -1.0887,  0.5024, -1.4584,\n",
            "         -2.0603, -1.6949, -1.5011, -2.2593, -0.7113, -0.2363,  0.4796, -2.1653,\n",
            "          0.2226,  0.0555, -1.5631, -0.7948]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zyh5OLnkHrut"
      },
      "source": [
        "**First Experiment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0WiMrtvdOiC"
      },
      "source": [
        "# Modified based on https://github.com/pytorch/examples/tree/master/mnist\n",
        "\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "\n",
        "def train(args, model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "            100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2cKMBg9dOiM"
      },
      "source": [
        "    torch.manual_seed(112)\n",
        "    device =torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6RUS_OzdOiW",
        "outputId": "94e3f5a0-fff9-4406-a5e0-43d045e7df1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391,
          "referenced_widgets": [
            "b0fe813f38f24ddb81eaf8817a5b7394",
            "5de78faa6b3f4fae99ee78702bdf3725",
            "6bbc23c1fc54437d957b19610cafee6b",
            "bc2a50250d3e47bc81c78e6e1043cbdf",
            "4c3d760863e1453b9571beb261f3bd21",
            "476a22380f594e569c2118bbf570f31e",
            "02d85d400a2b4df8bb6642304c9d40d1",
            "cf556641dae141058659be6b6e38e41b",
            "64496b196a854f31afa52f2b03ce82fd",
            "713cc95745d042a19d06fb71bef3165b",
            "9adcfc100bb74646ad076673ae1568f0",
            "943e7b82f0fe489798aaa4050e997958",
            "db0ae758229a4166a7fd16a75be8af89",
            "11795d8b40c94cc2b836848ebb8d1b04",
            "84aba354b41b4dd687b4773697ef758b",
            "044cadd98fb34c03bd5884684e381d0c",
            "da230d20932e4c4d9889de6d14e7858c",
            "89ebcaee786a4d27883c018db18a9c55",
            "3b466702501e4a0b924df67f5ad75ea3",
            "a358c2bb5a974d68bc35bc20408face3",
            "7312e747524a4927921ac6ca2bb266dd",
            "0323b213a55840bcbfe94e2e18dd543c",
            "3913ff8aa1394b8babe87184d5adf248",
            "c632ebdc777740fba794f21e0cd98fb5",
            "78211633889d4b58acf3bb7097a28343",
            "139dfa1b893c43dba1a9b7a205ca4a87",
            "38f930f64136423197938bb71a72e20f",
            "15f45c1e33694f8792a784852241acd1",
            "87ef93a5633e4a5cbd8d02f8d8b7ecc1",
            "cc53cf304b8044689e805859d2dd8df0",
            "1f06806c520f4dbd8f566c7d57d34ffb",
            "1b2698227a32439c95d746fbf36e55bc"
          ]
        }
      },
      "source": [
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "    dataset_train = datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transform)\n",
        "    dataset_test = datasets.MNIST('../data', train=False,\n",
        "                       transform=transform)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b0fe813f38f24ddb81eaf8817a5b7394",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "64496b196a854f31afa52f2b03ce82fd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da230d20932e4c4d9889de6d14e7858c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78211633889d4b58acf3bb7097a28343",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBCzsiIzdOii",
        "outputId": "4bbafe5c-3256-4d5e-8cf9-ba92a82b6a96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataset_train, dataset_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Dataset MNIST\n",
              "     Number of datapoints: 60000\n",
              "     Root location: ../data\n",
              "     Split: Train\n",
              "     StandardTransform\n",
              " Transform: Compose(\n",
              "                ToTensor()\n",
              "                Normalize(mean=(0.1307,), std=(0.3081,))\n",
              "            ), Dataset MNIST\n",
              "     Number of datapoints: 10000\n",
              "     Root location: ../data\n",
              "     Split: Test\n",
              "     StandardTransform\n",
              " Transform: Compose(\n",
              "                ToTensor()\n",
              "                Normalize(mean=(0.1307,), std=(0.3081,))\n",
              "            ))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xufrCaD1dOis",
        "outputId": "2dfabbe8-d007-4562-82f8-26fd9db06e7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "torch.manual_seed(123)\n",
        "transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transform)\n",
        "dataset2 = datasets.MNIST('../data', train=False,\n",
        "                       transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=256 , shuffle=True,\n",
        "        num_workers=1,pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=256 , shuffle=True,\n",
        "        num_workers=1,pin_memory=True)\n",
        "\n",
        "model = Net().to(device)\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=0.01)\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.1 )\n",
        "for epoch in range(1, 20 + 1):\n",
        "  train(10, model, device, train_loader, optimizer, epoch)\n",
        "  test(model, device, test_loader)\n",
        "  scheduler.step()\n",
        "\n",
        "torch.save(model.state_dict(), \"mnist_51.pt\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.290775\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.263582\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.178796\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.112189\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.012206\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.875768\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.732704\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 1.570873\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 1.368344\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 1.252594\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.100150\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 1.010099\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.966332\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.941042\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.841713\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.837612\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.686441\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.746758\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.710856\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.590436\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.720797\n",
            "Train Epoch: 1 [53760/60000 (89%)]\tLoss: 0.592270\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.631259\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.642372\n",
            "\n",
            "Test set: Average loss: 0.4319, Accuracy: 8948/10000 (89%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.528654\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.586515\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.515576\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.609790\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.540426\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.573524\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.605953\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.512915\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.457066\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.573658\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.551757\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.490347\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.578040\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.528624\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.527982\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.405295\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.517632\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.479923\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.462523\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.498634\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.474977\n",
            "Train Epoch: 2 [53760/60000 (89%)]\tLoss: 0.331938\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.463074\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.353085\n",
            "\n",
            "Test set: Average loss: 0.3034, Accuracy: 9184/10000 (92%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.520644\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.408751\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.509896\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.374411\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.412221\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.509424\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.438134\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.393450\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.482891\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.425608\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.445590\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.296856\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.438663\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.380485\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.427759\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.380887\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.334172\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.386153\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.320582\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.453327\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.433130\n",
            "Train Epoch: 3 [53760/60000 (89%)]\tLoss: 0.355488\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.375118\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.391808\n",
            "\n",
            "Test set: Average loss: 0.2536, Accuracy: 9270/10000 (93%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.345058\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.342453\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.415047\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.378100\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.345773\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.383259\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.313650\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.298275\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.447042\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.306154\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.323054\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.365468\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.299869\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.388690\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.359797\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.405433\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.223466\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.294452\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.301911\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.361998\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.404356\n",
            "Train Epoch: 4 [53760/60000 (89%)]\tLoss: 0.342228\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.362425\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.321455\n",
            "\n",
            "Test set: Average loss: 0.2239, Accuracy: 9347/10000 (93%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.319255\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.276295\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.331594\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.319815\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.360743\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.299651\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.310805\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.356642\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.264708\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.305791\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.243690\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.317649\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.338733\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.288981\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.412603\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.288177\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.332188\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.311667\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.325526\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.254781\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.313067\n",
            "Train Epoch: 5 [53760/60000 (89%)]\tLoss: 0.321613\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.366367\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.322866\n",
            "\n",
            "Test set: Average loss: 0.1999, Accuracy: 9401/10000 (94%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.290010\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.305916\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.319146\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.324158\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.346237\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.264835\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.253696\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.293083\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.285250\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.316286\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.224631\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.374705\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.295645\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.328472\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.317615\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.358284\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.378203\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.360612\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.245104\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.383652\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.199030\n",
            "Train Epoch: 6 [53760/60000 (89%)]\tLoss: 0.343761\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.295123\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.413396\n",
            "\n",
            "Test set: Average loss: 0.1972, Accuracy: 9412/10000 (94%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.361477\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.315155\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.238756\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.340135\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.306622\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.381325\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.275678\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.299546\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.299127\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.368355\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.292875\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.251842\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.321139\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.236401\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.287873\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.400755\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.291035\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.270492\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.247363\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.220127\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.310938\n",
            "Train Epoch: 7 [53760/60000 (89%)]\tLoss: 0.315064\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.284549\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.290378\n",
            "\n",
            "Test set: Average loss: 0.1952, Accuracy: 9423/10000 (94%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.254599\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.247653\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.389998\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.348583\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.210679\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.254010\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.284899\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.268815\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.329986\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.339923\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.372233\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.250928\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.332815\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.312336\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.363734\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.251571\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.307163\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.279995\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.328919\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.281599\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.244932\n",
            "Train Epoch: 8 [53760/60000 (89%)]\tLoss: 0.343259\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.216041\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.247291\n",
            "\n",
            "Test set: Average loss: 0.1933, Accuracy: 9429/10000 (94%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.306724\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.383016\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.276896\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.333368\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.240672\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.260778\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.414311\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.324826\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.274061\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.310779\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.305880\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.253489\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.449248\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.307251\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.244678\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.263197\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.214318\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.253809\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.328739\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.278277\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.232723\n",
            "Train Epoch: 9 [53760/60000 (89%)]\tLoss: 0.194822\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.355110\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.291911\n",
            "\n",
            "Test set: Average loss: 0.1912, Accuracy: 9434/10000 (94%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.321302\n",
            "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.286387\n",
            "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.330160\n",
            "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.308588\n",
            "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.399664\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.293721\n",
            "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.275535\n",
            "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.285028\n",
            "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.252560\n",
            "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.280674\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.257435\n",
            "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.217980\n",
            "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.306591\n",
            "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.350655\n",
            "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.245636\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.273023\n",
            "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.277739\n",
            "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.218864\n",
            "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.281480\n",
            "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.266646\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.229814\n",
            "Train Epoch: 10 [53760/60000 (89%)]\tLoss: 0.260560\n",
            "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.311530\n",
            "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.291214\n",
            "\n",
            "Test set: Average loss: 0.1893, Accuracy: 9437/10000 (94%)\n",
            "\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.216612\n",
            "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 0.334356\n",
            "Train Epoch: 11 [5120/60000 (9%)]\tLoss: 0.259423\n",
            "Train Epoch: 11 [7680/60000 (13%)]\tLoss: 0.245516\n",
            "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 0.241565\n",
            "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.280076\n",
            "Train Epoch: 11 [15360/60000 (26%)]\tLoss: 0.349877\n",
            "Train Epoch: 11 [17920/60000 (30%)]\tLoss: 0.400005\n",
            "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.289035\n",
            "Train Epoch: 11 [23040/60000 (38%)]\tLoss: 0.332846\n",
            "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.210882\n",
            "Train Epoch: 11 [28160/60000 (47%)]\tLoss: 0.254324\n",
            "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.228780\n",
            "Train Epoch: 11 [33280/60000 (55%)]\tLoss: 0.316197\n",
            "Train Epoch: 11 [35840/60000 (60%)]\tLoss: 0.359940\n",
            "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.299286\n",
            "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 0.256288\n",
            "Train Epoch: 11 [43520/60000 (72%)]\tLoss: 0.331329\n",
            "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 0.296556\n",
            "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 0.299547\n",
            "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.329078\n",
            "Train Epoch: 11 [53760/60000 (89%)]\tLoss: 0.279830\n",
            "Train Epoch: 11 [56320/60000 (94%)]\tLoss: 0.301510\n",
            "Train Epoch: 11 [58880/60000 (98%)]\tLoss: 0.258482\n",
            "\n",
            "Test set: Average loss: 0.1891, Accuracy: 9436/10000 (94%)\n",
            "\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.366287\n",
            "Train Epoch: 12 [2560/60000 (4%)]\tLoss: 0.293073\n",
            "Train Epoch: 12 [5120/60000 (9%)]\tLoss: 0.309203\n",
            "Train Epoch: 12 [7680/60000 (13%)]\tLoss: 0.308151\n",
            "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 0.305318\n",
            "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.176157\n",
            "Train Epoch: 12 [15360/60000 (26%)]\tLoss: 0.312133\n",
            "Train Epoch: 12 [17920/60000 (30%)]\tLoss: 0.271106\n",
            "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 0.330410\n",
            "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 0.276607\n",
            "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.320195\n",
            "Train Epoch: 12 [28160/60000 (47%)]\tLoss: 0.243619\n",
            "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 0.228889\n",
            "Train Epoch: 12 [33280/60000 (55%)]\tLoss: 0.217276\n",
            "Train Epoch: 12 [35840/60000 (60%)]\tLoss: 0.363306\n",
            "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.272722\n",
            "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.336097\n",
            "Train Epoch: 12 [43520/60000 (72%)]\tLoss: 0.297192\n",
            "Train Epoch: 12 [46080/60000 (77%)]\tLoss: 0.295889\n",
            "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 0.225930\n",
            "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.309346\n",
            "Train Epoch: 12 [53760/60000 (89%)]\tLoss: 0.371225\n",
            "Train Epoch: 12 [56320/60000 (94%)]\tLoss: 0.288020\n",
            "Train Epoch: 12 [58880/60000 (98%)]\tLoss: 0.395131\n",
            "\n",
            "Test set: Average loss: 0.1889, Accuracy: 9437/10000 (94%)\n",
            "\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.411791\n",
            "Train Epoch: 13 [2560/60000 (4%)]\tLoss: 0.341648\n",
            "Train Epoch: 13 [5120/60000 (9%)]\tLoss: 0.228991\n",
            "Train Epoch: 13 [7680/60000 (13%)]\tLoss: 0.341469\n",
            "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 0.251528\n",
            "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.309416\n",
            "Train Epoch: 13 [15360/60000 (26%)]\tLoss: 0.317967\n",
            "Train Epoch: 13 [17920/60000 (30%)]\tLoss: 0.259803\n",
            "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.264934\n",
            "Train Epoch: 13 [23040/60000 (38%)]\tLoss: 0.245141\n",
            "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.253377\n",
            "Train Epoch: 13 [28160/60000 (47%)]\tLoss: 0.242825\n",
            "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 0.280502\n",
            "Train Epoch: 13 [33280/60000 (55%)]\tLoss: 0.416161\n",
            "Train Epoch: 13 [35840/60000 (60%)]\tLoss: 0.431999\n",
            "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.325408\n",
            "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.347108\n",
            "Train Epoch: 13 [43520/60000 (72%)]\tLoss: 0.282015\n",
            "Train Epoch: 13 [46080/60000 (77%)]\tLoss: 0.292762\n",
            "Train Epoch: 13 [48640/60000 (81%)]\tLoss: 0.322954\n",
            "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.348890\n",
            "Train Epoch: 13 [53760/60000 (89%)]\tLoss: 0.351431\n",
            "Train Epoch: 13 [56320/60000 (94%)]\tLoss: 0.270850\n",
            "Train Epoch: 13 [58880/60000 (98%)]\tLoss: 0.181420\n",
            "\n",
            "Test set: Average loss: 0.1887, Accuracy: 9438/10000 (94%)\n",
            "\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.186829\n",
            "Train Epoch: 14 [2560/60000 (4%)]\tLoss: 0.318571\n",
            "Train Epoch: 14 [5120/60000 (9%)]\tLoss: 0.337072\n",
            "Train Epoch: 14 [7680/60000 (13%)]\tLoss: 0.280692\n",
            "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 0.335473\n",
            "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.248792\n",
            "Train Epoch: 14 [15360/60000 (26%)]\tLoss: 0.178658\n",
            "Train Epoch: 14 [17920/60000 (30%)]\tLoss: 0.286037\n",
            "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 0.230717\n",
            "Train Epoch: 14 [23040/60000 (38%)]\tLoss: 0.258683\n",
            "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.260742\n",
            "Train Epoch: 14 [28160/60000 (47%)]\tLoss: 0.337158\n",
            "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 0.203625\n",
            "Train Epoch: 14 [33280/60000 (55%)]\tLoss: 0.329452\n",
            "Train Epoch: 14 [35840/60000 (60%)]\tLoss: 0.259194\n",
            "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.232066\n",
            "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 0.276598\n",
            "Train Epoch: 14 [43520/60000 (72%)]\tLoss: 0.269285\n",
            "Train Epoch: 14 [46080/60000 (77%)]\tLoss: 0.286128\n",
            "Train Epoch: 14 [48640/60000 (81%)]\tLoss: 0.332750\n",
            "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.321241\n",
            "Train Epoch: 14 [53760/60000 (89%)]\tLoss: 0.323447\n",
            "Train Epoch: 14 [56320/60000 (94%)]\tLoss: 0.229573\n",
            "Train Epoch: 14 [58880/60000 (98%)]\tLoss: 0.182487\n",
            "\n",
            "Test set: Average loss: 0.1885, Accuracy: 9439/10000 (94%)\n",
            "\n",
            "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.244034\n",
            "Train Epoch: 15 [2560/60000 (4%)]\tLoss: 0.238281\n",
            "Train Epoch: 15 [5120/60000 (9%)]\tLoss: 0.265255\n",
            "Train Epoch: 15 [7680/60000 (13%)]\tLoss: 0.299425\n",
            "Train Epoch: 15 [10240/60000 (17%)]\tLoss: 0.297380\n",
            "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.227463\n",
            "Train Epoch: 15 [15360/60000 (26%)]\tLoss: 0.273965\n",
            "Train Epoch: 15 [17920/60000 (30%)]\tLoss: 0.385453\n",
            "Train Epoch: 15 [20480/60000 (34%)]\tLoss: 0.335887\n",
            "Train Epoch: 15 [23040/60000 (38%)]\tLoss: 0.307720\n",
            "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.340687\n",
            "Train Epoch: 15 [28160/60000 (47%)]\tLoss: 0.236805\n",
            "Train Epoch: 15 [30720/60000 (51%)]\tLoss: 0.341533\n",
            "Train Epoch: 15 [33280/60000 (55%)]\tLoss: 0.355242\n",
            "Train Epoch: 15 [35840/60000 (60%)]\tLoss: 0.210365\n",
            "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.366293\n",
            "Train Epoch: 15 [40960/60000 (68%)]\tLoss: 0.270187\n",
            "Train Epoch: 15 [43520/60000 (72%)]\tLoss: 0.231427\n",
            "Train Epoch: 15 [46080/60000 (77%)]\tLoss: 0.263105\n",
            "Train Epoch: 15 [48640/60000 (81%)]\tLoss: 0.330426\n",
            "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.365315\n",
            "Train Epoch: 15 [53760/60000 (89%)]\tLoss: 0.306414\n",
            "Train Epoch: 15 [56320/60000 (94%)]\tLoss: 0.228197\n",
            "Train Epoch: 15 [58880/60000 (98%)]\tLoss: 0.236322\n",
            "\n",
            "Test set: Average loss: 0.1883, Accuracy: 9441/10000 (94%)\n",
            "\n",
            "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.263996\n",
            "Train Epoch: 16 [2560/60000 (4%)]\tLoss: 0.299135\n",
            "Train Epoch: 16 [5120/60000 (9%)]\tLoss: 0.336073\n",
            "Train Epoch: 16 [7680/60000 (13%)]\tLoss: 0.346784\n",
            "Train Epoch: 16 [10240/60000 (17%)]\tLoss: 0.237898\n",
            "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 0.253673\n",
            "Train Epoch: 16 [15360/60000 (26%)]\tLoss: 0.238604\n",
            "Train Epoch: 16 [17920/60000 (30%)]\tLoss: 0.226131\n",
            "Train Epoch: 16 [20480/60000 (34%)]\tLoss: 0.372709\n",
            "Train Epoch: 16 [23040/60000 (38%)]\tLoss: 0.270555\n",
            "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.310879\n",
            "Train Epoch: 16 [28160/60000 (47%)]\tLoss: 0.301116\n",
            "Train Epoch: 16 [30720/60000 (51%)]\tLoss: 0.281181\n",
            "Train Epoch: 16 [33280/60000 (55%)]\tLoss: 0.311356\n",
            "Train Epoch: 16 [35840/60000 (60%)]\tLoss: 0.285620\n",
            "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 0.254338\n",
            "Train Epoch: 16 [40960/60000 (68%)]\tLoss: 0.267415\n",
            "Train Epoch: 16 [43520/60000 (72%)]\tLoss: 0.275452\n",
            "Train Epoch: 16 [46080/60000 (77%)]\tLoss: 0.329917\n",
            "Train Epoch: 16 [48640/60000 (81%)]\tLoss: 0.283451\n",
            "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.274922\n",
            "Train Epoch: 16 [53760/60000 (89%)]\tLoss: 0.242978\n",
            "Train Epoch: 16 [56320/60000 (94%)]\tLoss: 0.235712\n",
            "Train Epoch: 16 [58880/60000 (98%)]\tLoss: 0.265701\n",
            "\n",
            "Test set: Average loss: 0.1883, Accuracy: 9441/10000 (94%)\n",
            "\n",
            "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.213020\n",
            "Train Epoch: 17 [2560/60000 (4%)]\tLoss: 0.345246\n",
            "Train Epoch: 17 [5120/60000 (9%)]\tLoss: 0.243377\n",
            "Train Epoch: 17 [7680/60000 (13%)]\tLoss: 0.289064\n",
            "Train Epoch: 17 [10240/60000 (17%)]\tLoss: 0.315623\n",
            "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 0.321787\n",
            "Train Epoch: 17 [15360/60000 (26%)]\tLoss: 0.289928\n",
            "Train Epoch: 17 [17920/60000 (30%)]\tLoss: 0.218327\n",
            "Train Epoch: 17 [20480/60000 (34%)]\tLoss: 0.233682\n",
            "Train Epoch: 17 [23040/60000 (38%)]\tLoss: 0.399409\n",
            "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.271729\n",
            "Train Epoch: 17 [28160/60000 (47%)]\tLoss: 0.242690\n",
            "Train Epoch: 17 [30720/60000 (51%)]\tLoss: 0.237945\n",
            "Train Epoch: 17 [33280/60000 (55%)]\tLoss: 0.293489\n",
            "Train Epoch: 17 [35840/60000 (60%)]\tLoss: 0.346221\n",
            "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 0.261556\n",
            "Train Epoch: 17 [40960/60000 (68%)]\tLoss: 0.314552\n",
            "Train Epoch: 17 [43520/60000 (72%)]\tLoss: 0.340028\n",
            "Train Epoch: 17 [46080/60000 (77%)]\tLoss: 0.249546\n",
            "Train Epoch: 17 [48640/60000 (81%)]\tLoss: 0.268928\n",
            "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.279903\n",
            "Train Epoch: 17 [53760/60000 (89%)]\tLoss: 0.284758\n",
            "Train Epoch: 17 [56320/60000 (94%)]\tLoss: 0.249912\n",
            "Train Epoch: 17 [58880/60000 (98%)]\tLoss: 0.284977\n",
            "\n",
            "Test set: Average loss: 0.1882, Accuracy: 9441/10000 (94%)\n",
            "\n",
            "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.246599\n",
            "Train Epoch: 18 [2560/60000 (4%)]\tLoss: 0.250553\n",
            "Train Epoch: 18 [5120/60000 (9%)]\tLoss: 0.306657\n",
            "Train Epoch: 18 [7680/60000 (13%)]\tLoss: 0.356226\n",
            "Train Epoch: 18 [10240/60000 (17%)]\tLoss: 0.344017\n",
            "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 0.235500\n",
            "Train Epoch: 18 [15360/60000 (26%)]\tLoss: 0.343824\n",
            "Train Epoch: 18 [17920/60000 (30%)]\tLoss: 0.248543\n",
            "Train Epoch: 18 [20480/60000 (34%)]\tLoss: 0.320411\n",
            "Train Epoch: 18 [23040/60000 (38%)]\tLoss: 0.252884\n",
            "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.313912\n",
            "Train Epoch: 18 [28160/60000 (47%)]\tLoss: 0.239403\n",
            "Train Epoch: 18 [30720/60000 (51%)]\tLoss: 0.296275\n",
            "Train Epoch: 18 [33280/60000 (55%)]\tLoss: 0.303802\n",
            "Train Epoch: 18 [35840/60000 (60%)]\tLoss: 0.233791\n",
            "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 0.323088\n",
            "Train Epoch: 18 [40960/60000 (68%)]\tLoss: 0.248376\n",
            "Train Epoch: 18 [43520/60000 (72%)]\tLoss: 0.283197\n",
            "Train Epoch: 18 [46080/60000 (77%)]\tLoss: 0.315583\n",
            "Train Epoch: 18 [48640/60000 (81%)]\tLoss: 0.302810\n",
            "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.281242\n",
            "Train Epoch: 18 [53760/60000 (89%)]\tLoss: 0.292625\n",
            "Train Epoch: 18 [56320/60000 (94%)]\tLoss: 0.264414\n",
            "Train Epoch: 18 [58880/60000 (98%)]\tLoss: 0.296412\n",
            "\n",
            "Test set: Average loss: 0.1882, Accuracy: 9441/10000 (94%)\n",
            "\n",
            "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.330292\n",
            "Train Epoch: 19 [2560/60000 (4%)]\tLoss: 0.332629\n",
            "Train Epoch: 19 [5120/60000 (9%)]\tLoss: 0.257777\n",
            "Train Epoch: 19 [7680/60000 (13%)]\tLoss: 0.284062\n",
            "Train Epoch: 19 [10240/60000 (17%)]\tLoss: 0.323911\n",
            "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 0.375065\n",
            "Train Epoch: 19 [15360/60000 (26%)]\tLoss: 0.277724\n",
            "Train Epoch: 19 [17920/60000 (30%)]\tLoss: 0.353949\n",
            "Train Epoch: 19 [20480/60000 (34%)]\tLoss: 0.297054\n",
            "Train Epoch: 19 [23040/60000 (38%)]\tLoss: 0.396812\n",
            "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.233767\n",
            "Train Epoch: 19 [28160/60000 (47%)]\tLoss: 0.281159\n",
            "Train Epoch: 19 [30720/60000 (51%)]\tLoss: 0.315926\n",
            "Train Epoch: 19 [33280/60000 (55%)]\tLoss: 0.285878\n",
            "Train Epoch: 19 [35840/60000 (60%)]\tLoss: 0.232989\n",
            "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 0.283244\n",
            "Train Epoch: 19 [40960/60000 (68%)]\tLoss: 0.242261\n",
            "Train Epoch: 19 [43520/60000 (72%)]\tLoss: 0.327468\n",
            "Train Epoch: 19 [46080/60000 (77%)]\tLoss: 0.214596\n",
            "Train Epoch: 19 [48640/60000 (81%)]\tLoss: 0.224554\n",
            "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.219732\n",
            "Train Epoch: 19 [53760/60000 (89%)]\tLoss: 0.309047\n",
            "Train Epoch: 19 [56320/60000 (94%)]\tLoss: 0.217412\n",
            "Train Epoch: 19 [58880/60000 (98%)]\tLoss: 0.342037\n",
            "\n",
            "Test set: Average loss: 0.1882, Accuracy: 9441/10000 (94%)\n",
            "\n",
            "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.270467\n",
            "Train Epoch: 20 [2560/60000 (4%)]\tLoss: 0.359959\n",
            "Train Epoch: 20 [5120/60000 (9%)]\tLoss: 0.265224\n",
            "Train Epoch: 20 [7680/60000 (13%)]\tLoss: 0.260993\n",
            "Train Epoch: 20 [10240/60000 (17%)]\tLoss: 0.275330\n",
            "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 0.255231\n",
            "Train Epoch: 20 [15360/60000 (26%)]\tLoss: 0.240890\n",
            "Train Epoch: 20 [17920/60000 (30%)]\tLoss: 0.259857\n",
            "Train Epoch: 20 [20480/60000 (34%)]\tLoss: 0.292630\n",
            "Train Epoch: 20 [23040/60000 (38%)]\tLoss: 0.331081\n",
            "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.235743\n",
            "Train Epoch: 20 [28160/60000 (47%)]\tLoss: 0.224654\n",
            "Train Epoch: 20 [30720/60000 (51%)]\tLoss: 0.278274\n",
            "Train Epoch: 20 [33280/60000 (55%)]\tLoss: 0.280666\n",
            "Train Epoch: 20 [35840/60000 (60%)]\tLoss: 0.247440\n",
            "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 0.261415\n",
            "Train Epoch: 20 [40960/60000 (68%)]\tLoss: 0.316250\n",
            "Train Epoch: 20 [43520/60000 (72%)]\tLoss: 0.278397\n",
            "Train Epoch: 20 [46080/60000 (77%)]\tLoss: 0.225515\n",
            "Train Epoch: 20 [48640/60000 (81%)]\tLoss: 0.268272\n",
            "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.275153\n",
            "Train Epoch: 20 [53760/60000 (89%)]\tLoss: 0.250426\n",
            "Train Epoch: 20 [56320/60000 (94%)]\tLoss: 0.253100\n",
            "Train Epoch: 20 [58880/60000 (98%)]\tLoss: 0.265559\n",
            "\n",
            "Test set: Average loss: 0.1882, Accuracy: 9441/10000 (94%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWmFM4o8H188"
      },
      "source": [
        "**Second Experiment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cyzl5-5WIIBB"
      },
      "source": [
        "# Modified based on https://github.com/pytorch/examples/tree/master/mnist\n",
        "\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "\n",
        "def train(args, model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "            100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "908NCd9bIIBZ"
      },
      "source": [
        "    torch.manual_seed(112)\n",
        "    device =torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA8xtwLOIIBm"
      },
      "source": [
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "    dataset_train = datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transform)\n",
        "    dataset_test = datasets.MNIST('../data', train=False,\n",
        "                       transform=transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY5XmK-EIIBz",
        "outputId": "e6fe144b-37fa-4e7e-e73e-527a0233961f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "torch.manual_seed(123)\n",
        "transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transform)\n",
        "dataset2 = datasets.MNIST('../data', train=False,\n",
        "                       transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=256 , shuffle=True,\n",
        "        num_workers=1,pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=256 , shuffle=True,\n",
        "        num_workers=1,pin_memory=True)\n",
        "\n",
        "model = Net().to(device)\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=0.01)\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.1 )\n",
        "for epoch in range(1, 20 + 1):\n",
        "  train(10, model, device, train_loader, optimizer, epoch)\n",
        "  test(model, device, test_loader)\n",
        "  scheduler.step()\n",
        "\n",
        "torch.save(model.state_dict(), \"mnist_51.pt\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.290485\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.242163\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.153081\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.053765\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.915162\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.728943\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 1.546496\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 1.316280\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 1.071331\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.965430\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.813209\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.739535\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.623678\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.664210\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.566425\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.585027\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.465509\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.512838\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.518450\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.403659\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.484594\n",
            "Train Epoch: 1 [53760/60000 (89%)]\tLoss: 0.397729\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.497946\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.544772\n",
            "\n",
            "Test set: Average loss: 0.3766, Accuracy: 9010/10000 (90%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.370053\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.422271\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.360810\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.473989\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.400162\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.407159\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.414749\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.345485\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.323020\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.410314\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.416556\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.316707\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.425170\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.389282\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.402002\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.305858\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.385940\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.348867\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.370200\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.362612\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.340391\n",
            "Train Epoch: 2 [53760/60000 (89%)]\tLoss: 0.282769\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.341546\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.300808\n",
            "\n",
            "Test set: Average loss: 0.2840, Accuracy: 9211/10000 (92%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.383282\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.299324\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.356385\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.250768\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.327231\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.365074\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.310898\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.358356\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.337802\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.312070\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.308074\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.221705\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.348706\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.243850\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.309071\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.309450\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.261671\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.317117\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.248228\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.346748\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.308819\n",
            "Train Epoch: 3 [53760/60000 (89%)]\tLoss: 0.245429\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.308777\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.277543\n",
            "\n",
            "Test set: Average loss: 0.2362, Accuracy: 9317/10000 (93%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.255639\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.253955\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.330666\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.275864\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.204652\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.325234\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.230472\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.206061\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.340924\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.239960\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.229938\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.322380\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.229916\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.265146\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.280988\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.322247\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.152087\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.168397\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.209348\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.308352\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.309275\n",
            "Train Epoch: 4 [53760/60000 (89%)]\tLoss: 0.240029\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.272816\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.260051\n",
            "\n",
            "Test set: Average loss: 0.2082, Accuracy: 9392/10000 (94%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.259285\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.179785\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.220880\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.263121\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.264171\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.205989\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.231626\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.288694\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.191811\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.177076\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.171547\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.221136\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.269757\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.206320\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.323532\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.218721\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.252022\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.215824\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.217982\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.192576\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.199876\n",
            "Train Epoch: 5 [53760/60000 (89%)]\tLoss: 0.231714\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.248589\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.212370\n",
            "\n",
            "Test set: Average loss: 0.1846, Accuracy: 9451/10000 (95%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.190085\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.228852\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.219224\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.202521\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.255955\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.191954\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.151182\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.215740\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.187294\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.196598\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.160972\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.230471\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.183441\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.266949\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.218392\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.218459\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.268145\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.232919\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.155121\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.279369\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.147267\n",
            "Train Epoch: 6 [53760/60000 (89%)]\tLoss: 0.254761\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.176048\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.268799\n",
            "\n",
            "Test set: Average loss: 0.1796, Accuracy: 9464/10000 (95%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.220173\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.217866\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.182659\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.248923\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.185644\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.270838\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.227081\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.209960\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.212170\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.228031\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.212664\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.171158\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.244357\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.170633\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.203188\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.290048\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.181116\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.176084\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.186903\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.168512\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.240475\n",
            "Train Epoch: 7 [53760/60000 (89%)]\tLoss: 0.216726\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.219314\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.209887\n",
            "\n",
            "Test set: Average loss: 0.1775, Accuracy: 9470/10000 (95%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.190266\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.182287\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.288072\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.258931\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.117449\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.180106\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.184263\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.194335\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.196487\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.219786\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.241898\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.174239\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.252544\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.235735\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.268309\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.185109\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.218824\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.203297\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.231621\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.204724\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.172398\n",
            "Train Epoch: 8 [53760/60000 (89%)]\tLoss: 0.220327\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.155267\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.205414\n",
            "\n",
            "Test set: Average loss: 0.1759, Accuracy: 9479/10000 (95%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.253853\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.292265\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.203233\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.266357\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.142616\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.155281\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.292757\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.232574\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.171703\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.199910\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.230789\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.185707\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.325421\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.182962\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.144920\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.180278\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.149179\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.181419\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.251735\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.168217\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.143265\n",
            "Train Epoch: 9 [53760/60000 (89%)]\tLoss: 0.135785\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.265226\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.181362\n",
            "\n",
            "Test set: Average loss: 0.1733, Accuracy: 9493/10000 (95%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.248189\n",
            "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.190870\n",
            "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.205895\n",
            "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.206133\n",
            "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.255272\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.223024\n",
            "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.190741\n",
            "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.199738\n",
            "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.180090\n",
            "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.174315\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.179226\n",
            "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.172390\n",
            "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.254455\n",
            "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.228121\n",
            "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.205756\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.193740\n",
            "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.216640\n",
            "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.143129\n",
            "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.173103\n",
            "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.191092\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.158832\n",
            "Train Epoch: 10 [53760/60000 (89%)]\tLoss: 0.171025\n",
            "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.224777\n",
            "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.193348\n",
            "\n",
            "Test set: Average loss: 0.1716, Accuracy: 9492/10000 (95%)\n",
            "\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.127323\n",
            "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 0.230889\n",
            "Train Epoch: 11 [5120/60000 (9%)]\tLoss: 0.208646\n",
            "Train Epoch: 11 [7680/60000 (13%)]\tLoss: 0.139061\n",
            "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 0.166880\n",
            "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.182047\n",
            "Train Epoch: 11 [15360/60000 (26%)]\tLoss: 0.241822\n",
            "Train Epoch: 11 [17920/60000 (30%)]\tLoss: 0.290513\n",
            "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.228070\n",
            "Train Epoch: 11 [23040/60000 (38%)]\tLoss: 0.194895\n",
            "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.190063\n",
            "Train Epoch: 11 [28160/60000 (47%)]\tLoss: 0.143734\n",
            "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.158714\n",
            "Train Epoch: 11 [33280/60000 (55%)]\tLoss: 0.202739\n",
            "Train Epoch: 11 [35840/60000 (60%)]\tLoss: 0.227609\n",
            "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.202518\n",
            "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 0.158869\n",
            "Train Epoch: 11 [43520/60000 (72%)]\tLoss: 0.289929\n",
            "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 0.203266\n",
            "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 0.206278\n",
            "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.242490\n",
            "Train Epoch: 11 [53760/60000 (89%)]\tLoss: 0.199855\n",
            "Train Epoch: 11 [56320/60000 (94%)]\tLoss: 0.211187\n",
            "Train Epoch: 11 [58880/60000 (98%)]\tLoss: 0.167275\n",
            "\n",
            "Test set: Average loss: 0.1712, Accuracy: 9493/10000 (95%)\n",
            "\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.245030\n",
            "Train Epoch: 12 [2560/60000 (4%)]\tLoss: 0.168876\n",
            "Train Epoch: 12 [5120/60000 (9%)]\tLoss: 0.196036\n",
            "Train Epoch: 12 [7680/60000 (13%)]\tLoss: 0.203097\n",
            "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 0.222792\n",
            "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.130831\n",
            "Train Epoch: 12 [15360/60000 (26%)]\tLoss: 0.215271\n",
            "Train Epoch: 12 [17920/60000 (30%)]\tLoss: 0.170096\n",
            "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 0.256537\n",
            "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 0.168387\n",
            "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.207977\n",
            "Train Epoch: 12 [28160/60000 (47%)]\tLoss: 0.158945\n",
            "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 0.155776\n",
            "Train Epoch: 12 [33280/60000 (55%)]\tLoss: 0.174918\n",
            "Train Epoch: 12 [35840/60000 (60%)]\tLoss: 0.236177\n",
            "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.191153\n",
            "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.196767\n",
            "Train Epoch: 12 [43520/60000 (72%)]\tLoss: 0.164721\n",
            "Train Epoch: 12 [46080/60000 (77%)]\tLoss: 0.234591\n",
            "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 0.140472\n",
            "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.238407\n",
            "Train Epoch: 12 [53760/60000 (89%)]\tLoss: 0.224661\n",
            "Train Epoch: 12 [56320/60000 (94%)]\tLoss: 0.232388\n",
            "Train Epoch: 12 [58880/60000 (98%)]\tLoss: 0.239044\n",
            "\n",
            "Test set: Average loss: 0.1710, Accuracy: 9495/10000 (95%)\n",
            "\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.282430\n",
            "Train Epoch: 13 [2560/60000 (4%)]\tLoss: 0.267372\n",
            "Train Epoch: 13 [5120/60000 (9%)]\tLoss: 0.178722\n",
            "Train Epoch: 13 [7680/60000 (13%)]\tLoss: 0.249608\n",
            "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 0.176324\n",
            "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.197796\n",
            "Train Epoch: 13 [15360/60000 (26%)]\tLoss: 0.221470\n",
            "Train Epoch: 13 [17920/60000 (30%)]\tLoss: 0.162713\n",
            "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.186531\n",
            "Train Epoch: 13 [23040/60000 (38%)]\tLoss: 0.150460\n",
            "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.171264\n",
            "Train Epoch: 13 [28160/60000 (47%)]\tLoss: 0.190242\n",
            "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 0.184667\n",
            "Train Epoch: 13 [33280/60000 (55%)]\tLoss: 0.284964\n",
            "Train Epoch: 13 [35840/60000 (60%)]\tLoss: 0.286718\n",
            "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.224510\n",
            "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.230902\n",
            "Train Epoch: 13 [43520/60000 (72%)]\tLoss: 0.182041\n",
            "Train Epoch: 13 [46080/60000 (77%)]\tLoss: 0.181037\n",
            "Train Epoch: 13 [48640/60000 (81%)]\tLoss: 0.205737\n",
            "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.248979\n",
            "Train Epoch: 13 [53760/60000 (89%)]\tLoss: 0.278822\n",
            "Train Epoch: 13 [56320/60000 (94%)]\tLoss: 0.191402\n",
            "Train Epoch: 13 [58880/60000 (98%)]\tLoss: 0.129717\n",
            "\n",
            "Test set: Average loss: 0.1708, Accuracy: 9496/10000 (95%)\n",
            "\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.131972\n",
            "Train Epoch: 14 [2560/60000 (4%)]\tLoss: 0.220238\n",
            "Train Epoch: 14 [5120/60000 (9%)]\tLoss: 0.203313\n",
            "Train Epoch: 14 [7680/60000 (13%)]\tLoss: 0.199147\n",
            "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 0.248869\n",
            "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.160007\n",
            "Train Epoch: 14 [15360/60000 (26%)]\tLoss: 0.143941\n",
            "Train Epoch: 14 [17920/60000 (30%)]\tLoss: 0.170852\n",
            "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 0.161213\n",
            "Train Epoch: 14 [23040/60000 (38%)]\tLoss: 0.176696\n",
            "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.162365\n",
            "Train Epoch: 14 [28160/60000 (47%)]\tLoss: 0.226155\n",
            "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 0.155863\n",
            "Train Epoch: 14 [33280/60000 (55%)]\tLoss: 0.212025\n",
            "Train Epoch: 14 [35840/60000 (60%)]\tLoss: 0.157091\n",
            "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.148919\n",
            "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 0.204203\n",
            "Train Epoch: 14 [43520/60000 (72%)]\tLoss: 0.190871\n",
            "Train Epoch: 14 [46080/60000 (77%)]\tLoss: 0.222537\n",
            "Train Epoch: 14 [48640/60000 (81%)]\tLoss: 0.261375\n",
            "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.230309\n",
            "Train Epoch: 14 [53760/60000 (89%)]\tLoss: 0.223776\n",
            "Train Epoch: 14 [56320/60000 (94%)]\tLoss: 0.161351\n",
            "Train Epoch: 14 [58880/60000 (98%)]\tLoss: 0.110895\n",
            "\n",
            "Test set: Average loss: 0.1706, Accuracy: 9499/10000 (95%)\n",
            "\n",
            "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.167708\n",
            "Train Epoch: 15 [2560/60000 (4%)]\tLoss: 0.152441\n",
            "Train Epoch: 15 [5120/60000 (9%)]\tLoss: 0.198481\n",
            "Train Epoch: 15 [7680/60000 (13%)]\tLoss: 0.204310\n",
            "Train Epoch: 15 [10240/60000 (17%)]\tLoss: 0.241337\n",
            "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.156469\n",
            "Train Epoch: 15 [15360/60000 (26%)]\tLoss: 0.221976\n",
            "Train Epoch: 15 [17920/60000 (30%)]\tLoss: 0.287180\n",
            "Train Epoch: 15 [20480/60000 (34%)]\tLoss: 0.206039\n",
            "Train Epoch: 15 [23040/60000 (38%)]\tLoss: 0.211764\n",
            "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.226951\n",
            "Train Epoch: 15 [28160/60000 (47%)]\tLoss: 0.152663\n",
            "Train Epoch: 15 [30720/60000 (51%)]\tLoss: 0.256885\n",
            "Train Epoch: 15 [33280/60000 (55%)]\tLoss: 0.293499\n",
            "Train Epoch: 15 [35840/60000 (60%)]\tLoss: 0.126581\n",
            "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.218493\n",
            "Train Epoch: 15 [40960/60000 (68%)]\tLoss: 0.202311\n",
            "Train Epoch: 15 [43520/60000 (72%)]\tLoss: 0.161159\n",
            "Train Epoch: 15 [46080/60000 (77%)]\tLoss: 0.186004\n",
            "Train Epoch: 15 [48640/60000 (81%)]\tLoss: 0.211981\n",
            "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.318030\n",
            "Train Epoch: 15 [53760/60000 (89%)]\tLoss: 0.208868\n",
            "Train Epoch: 15 [56320/60000 (94%)]\tLoss: 0.167673\n",
            "Train Epoch: 15 [58880/60000 (98%)]\tLoss: 0.169761\n",
            "\n",
            "Test set: Average loss: 0.1704, Accuracy: 9498/10000 (95%)\n",
            "\n",
            "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.171699\n",
            "Train Epoch: 16 [2560/60000 (4%)]\tLoss: 0.220636\n",
            "Train Epoch: 16 [5120/60000 (9%)]\tLoss: 0.219607\n",
            "Train Epoch: 16 [7680/60000 (13%)]\tLoss: 0.246163\n",
            "Train Epoch: 16 [10240/60000 (17%)]\tLoss: 0.195096\n",
            "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 0.162506\n",
            "Train Epoch: 16 [15360/60000 (26%)]\tLoss: 0.145520\n",
            "Train Epoch: 16 [17920/60000 (30%)]\tLoss: 0.168305\n",
            "Train Epoch: 16 [20480/60000 (34%)]\tLoss: 0.261940\n",
            "Train Epoch: 16 [23040/60000 (38%)]\tLoss: 0.199340\n",
            "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.208053\n",
            "Train Epoch: 16 [28160/60000 (47%)]\tLoss: 0.189881\n",
            "Train Epoch: 16 [30720/60000 (51%)]\tLoss: 0.198056\n",
            "Train Epoch: 16 [33280/60000 (55%)]\tLoss: 0.202619\n",
            "Train Epoch: 16 [35840/60000 (60%)]\tLoss: 0.196396\n",
            "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 0.159596\n",
            "Train Epoch: 16 [40960/60000 (68%)]\tLoss: 0.151493\n",
            "Train Epoch: 16 [43520/60000 (72%)]\tLoss: 0.206300\n",
            "Train Epoch: 16 [46080/60000 (77%)]\tLoss: 0.242146\n",
            "Train Epoch: 16 [48640/60000 (81%)]\tLoss: 0.179711\n",
            "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.200995\n",
            "Train Epoch: 16 [53760/60000 (89%)]\tLoss: 0.201166\n",
            "Train Epoch: 16 [56320/60000 (94%)]\tLoss: 0.126084\n",
            "Train Epoch: 16 [58880/60000 (98%)]\tLoss: 0.157812\n",
            "\n",
            "Test set: Average loss: 0.1703, Accuracy: 9497/10000 (95%)\n",
            "\n",
            "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.163030\n",
            "Train Epoch: 17 [2560/60000 (4%)]\tLoss: 0.255201\n",
            "Train Epoch: 17 [5120/60000 (9%)]\tLoss: 0.165622\n",
            "Train Epoch: 17 [7680/60000 (13%)]\tLoss: 0.187006\n",
            "Train Epoch: 17 [10240/60000 (17%)]\tLoss: 0.265493\n",
            "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 0.217730\n",
            "Train Epoch: 17 [15360/60000 (26%)]\tLoss: 0.211948\n",
            "Train Epoch: 17 [17920/60000 (30%)]\tLoss: 0.141162\n",
            "Train Epoch: 17 [20480/60000 (34%)]\tLoss: 0.170269\n",
            "Train Epoch: 17 [23040/60000 (38%)]\tLoss: 0.290709\n",
            "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.176013\n",
            "Train Epoch: 17 [28160/60000 (47%)]\tLoss: 0.139149\n",
            "Train Epoch: 17 [30720/60000 (51%)]\tLoss: 0.160110\n",
            "Train Epoch: 17 [33280/60000 (55%)]\tLoss: 0.173122\n",
            "Train Epoch: 17 [35840/60000 (60%)]\tLoss: 0.261264\n",
            "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 0.184760\n",
            "Train Epoch: 17 [40960/60000 (68%)]\tLoss: 0.175491\n",
            "Train Epoch: 17 [43520/60000 (72%)]\tLoss: 0.218733\n",
            "Train Epoch: 17 [46080/60000 (77%)]\tLoss: 0.167985\n",
            "Train Epoch: 17 [48640/60000 (81%)]\tLoss: 0.179287\n",
            "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.162817\n",
            "Train Epoch: 17 [53760/60000 (89%)]\tLoss: 0.228646\n",
            "Train Epoch: 17 [56320/60000 (94%)]\tLoss: 0.182698\n",
            "Train Epoch: 17 [58880/60000 (98%)]\tLoss: 0.204860\n",
            "\n",
            "Test set: Average loss: 0.1703, Accuracy: 9498/10000 (95%)\n",
            "\n",
            "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.150234\n",
            "Train Epoch: 18 [2560/60000 (4%)]\tLoss: 0.186356\n",
            "Train Epoch: 18 [5120/60000 (9%)]\tLoss: 0.267923\n",
            "Train Epoch: 18 [7680/60000 (13%)]\tLoss: 0.238130\n",
            "Train Epoch: 18 [10240/60000 (17%)]\tLoss: 0.256710\n",
            "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 0.148625\n",
            "Train Epoch: 18 [15360/60000 (26%)]\tLoss: 0.234256\n",
            "Train Epoch: 18 [17920/60000 (30%)]\tLoss: 0.141014\n",
            "Train Epoch: 18 [20480/60000 (34%)]\tLoss: 0.227512\n",
            "Train Epoch: 18 [23040/60000 (38%)]\tLoss: 0.156437\n",
            "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.243746\n",
            "Train Epoch: 18 [28160/60000 (47%)]\tLoss: 0.166035\n",
            "Train Epoch: 18 [30720/60000 (51%)]\tLoss: 0.201304\n",
            "Train Epoch: 18 [33280/60000 (55%)]\tLoss: 0.230861\n",
            "Train Epoch: 18 [35840/60000 (60%)]\tLoss: 0.176383\n",
            "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 0.228260\n",
            "Train Epoch: 18 [40960/60000 (68%)]\tLoss: 0.188324\n",
            "Train Epoch: 18 [43520/60000 (72%)]\tLoss: 0.202499\n",
            "Train Epoch: 18 [46080/60000 (77%)]\tLoss: 0.218960\n",
            "Train Epoch: 18 [48640/60000 (81%)]\tLoss: 0.205154\n",
            "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.169421\n",
            "Train Epoch: 18 [53760/60000 (89%)]\tLoss: 0.183577\n",
            "Train Epoch: 18 [56320/60000 (94%)]\tLoss: 0.188913\n",
            "Train Epoch: 18 [58880/60000 (98%)]\tLoss: 0.186332\n",
            "\n",
            "Test set: Average loss: 0.1703, Accuracy: 9498/10000 (95%)\n",
            "\n",
            "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.231313\n",
            "Train Epoch: 19 [2560/60000 (4%)]\tLoss: 0.232657\n",
            "Train Epoch: 19 [5120/60000 (9%)]\tLoss: 0.200249\n",
            "Train Epoch: 19 [7680/60000 (13%)]\tLoss: 0.185552\n",
            "Train Epoch: 19 [10240/60000 (17%)]\tLoss: 0.243254\n",
            "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 0.238450\n",
            "Train Epoch: 19 [15360/60000 (26%)]\tLoss: 0.182715\n",
            "Train Epoch: 19 [17920/60000 (30%)]\tLoss: 0.262485\n",
            "Train Epoch: 19 [20480/60000 (34%)]\tLoss: 0.203238\n",
            "Train Epoch: 19 [23040/60000 (38%)]\tLoss: 0.247288\n",
            "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.191268\n",
            "Train Epoch: 19 [28160/60000 (47%)]\tLoss: 0.182317\n",
            "Train Epoch: 19 [30720/60000 (51%)]\tLoss: 0.212180\n",
            "Train Epoch: 19 [33280/60000 (55%)]\tLoss: 0.196203\n",
            "Train Epoch: 19 [35840/60000 (60%)]\tLoss: 0.151628\n",
            "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 0.205274\n",
            "Train Epoch: 19 [40960/60000 (68%)]\tLoss: 0.191250\n",
            "Train Epoch: 19 [43520/60000 (72%)]\tLoss: 0.235852\n",
            "Train Epoch: 19 [46080/60000 (77%)]\tLoss: 0.149407\n",
            "Train Epoch: 19 [48640/60000 (81%)]\tLoss: 0.127741\n",
            "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.176740\n",
            "Train Epoch: 19 [53760/60000 (89%)]\tLoss: 0.228266\n",
            "Train Epoch: 19 [56320/60000 (94%)]\tLoss: 0.153748\n",
            "Train Epoch: 19 [58880/60000 (98%)]\tLoss: 0.284766\n",
            "\n",
            "Test set: Average loss: 0.1703, Accuracy: 9499/10000 (95%)\n",
            "\n",
            "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.192357\n",
            "Train Epoch: 20 [2560/60000 (4%)]\tLoss: 0.219785\n",
            "Train Epoch: 20 [5120/60000 (9%)]\tLoss: 0.180813\n",
            "Train Epoch: 20 [7680/60000 (13%)]\tLoss: 0.185382\n",
            "Train Epoch: 20 [10240/60000 (17%)]\tLoss: 0.193041\n",
            "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 0.170055\n",
            "Train Epoch: 20 [15360/60000 (26%)]\tLoss: 0.190472\n",
            "Train Epoch: 20 [17920/60000 (30%)]\tLoss: 0.198428\n",
            "Train Epoch: 20 [20480/60000 (34%)]\tLoss: 0.226780\n",
            "Train Epoch: 20 [23040/60000 (38%)]\tLoss: 0.225939\n",
            "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.169355\n",
            "Train Epoch: 20 [28160/60000 (47%)]\tLoss: 0.158662\n",
            "Train Epoch: 20 [30720/60000 (51%)]\tLoss: 0.191039\n",
            "Train Epoch: 20 [33280/60000 (55%)]\tLoss: 0.200545\n",
            "Train Epoch: 20 [35840/60000 (60%)]\tLoss: 0.211619\n",
            "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 0.149816\n",
            "Train Epoch: 20 [40960/60000 (68%)]\tLoss: 0.208430\n",
            "Train Epoch: 20 [43520/60000 (72%)]\tLoss: 0.217863\n",
            "Train Epoch: 20 [46080/60000 (77%)]\tLoss: 0.177365\n",
            "Train Epoch: 20 [48640/60000 (81%)]\tLoss: 0.195099\n",
            "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.166046\n",
            "Train Epoch: 20 [53760/60000 (89%)]\tLoss: 0.178288\n",
            "Train Epoch: 20 [56320/60000 (94%)]\tLoss: 0.161035\n",
            "Train Epoch: 20 [58880/60000 (98%)]\tLoss: 0.174586\n",
            "\n",
            "Test set: Average loss: 0.1703, Accuracy: 9499/10000 (95%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8E0QJPhKhdss"
      },
      "source": [
        "# **Optimization** for Training Deep Models: Different Optimizers and Batch Normalization\n",
        "\n",
        "* https://pytorch.org/docs/stable/optim.html\n",
        "* https://github.com/jettify/pytorch-optimizer\n",
        "* https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/optimizers/\n",
        "* See this: https://github.com/ritchieng/deep-learning-wizard/tree/master/docs/deep_learning/boosting_models_pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5Rz4Mk3BNMD"
      },
      "source": [
        "# Different Optimizers \n",
        "* Browed from: https://github.com/ritchieng/deep-learning-wizard/tree/master/docs/deep_learning/boosting_models_pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGWiJJi856Ec"
      },
      "source": [
        "### Model: 1 Hidden Layer Feedforward Neural Network (ReLU Activation)\n",
        "<img src=\"./images/nn1.png\" alt=\"deeplearningwizard\" style=\"width: 900px;\"/>\n",
        "\n",
        "### Steps\n",
        "- Step 1: Load Dataset\n",
        "- Step 2: Make Dataset Iterable\n",
        "- Step 3: Create Model Class\n",
        "- Step 4: Instantiate Model Class\n",
        "- Step 5: Instantiate Loss Class\n",
        "- **Step 6: Instantiate Optimizer Class**\n",
        "- Step 7: Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wgrr195_56Eg",
        "outputId": "f91a62c4-df71-4ac6-d296-af8653429a65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512,
          "referenced_widgets": [
            "375c0fa26fe84659b5ea58ca1b6d5885",
            "2946d0de64214c4987914ef391797b46",
            "347a139a8276413e9ce5c740eb7de677",
            "5c6c44e0e21c47df95ffce75279907ba",
            "e178374ec2d04034bbae0cf17863dd92",
            "57e3dbc622d84100a25fd09367e671e7",
            "090df969e6d046be88d95825b6c7852a",
            "ed2f4edccb12402eac59ac0af28508cd",
            "30f0ad6de7ed491d870c8634bdec27d1",
            "cc0318f1c3a745269f225df7e4451ae7",
            "343b6427c2fe4d549f23183325faa253",
            "4671251a5a154c3bb7ee2600f89ffa2b",
            "543020197bf6442f86e75fed1f59bc6a",
            "00ef9272df8d40c3b243a78405d9cd70",
            "eae2ec4dab0c4c7680ff851308a00467",
            "91b9c9b4c71b4d0cb1da6ddc74a5183a",
            "1dbd5846a293491491638b0a8e7dea53",
            "dd0d2879ed2b41b990ebb6a2f0f34a22",
            "8e26d99813db4896b84e78b99669cae3",
            "908e4f7575ba4e8fafc61edbdab20c3f",
            "d1497bc424374cd78f7438300a629dfe",
            "981951708e3b42ff806265d9918f6011",
            "6a7dabcef3a842b3b1bd6bbbef33bd84",
            "7214ead952cd4302ad50248e4f458037",
            "466f9b0076174a1ab9e1264a91b16060",
            "74560ec4f6c34232a027f8cfc6cbf286",
            "fd9205f792e64fffa4447a975dfd9753",
            "d3bad3560c124032bbb6f26dea2ae78e",
            "bdddbf380d5148e79e61b75884c69f70",
            "4ded011e1ff3424fadcd7588979a26cc",
            "7fb340b492244825a29cfd3536719199",
            "766b729d2ece492c95865e3f19587168"
          ]
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "# Set seed\n",
        "torch.manual_seed(0)\n",
        "\n",
        "\n",
        "'''\n",
        "STEP 1: LOADING DATASET\n",
        "'''\n",
        "\n",
        "train_dataset = dsets.MNIST(root='./data', \n",
        "                            train=True, \n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)\n",
        "\n",
        "test_dataset = dsets.MNIST(root='./data', \n",
        "                           train=False, \n",
        "                           transform=transforms.ToTensor())\n",
        "\n",
        "'''\n",
        "STEP 2: MAKING DATASET ITERABLE\n",
        "'''\n",
        "\n",
        "batch_size = 100\n",
        "n_iters = 3000\n",
        "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "'''\n",
        "STEP 3: CREATE MODEL CLASS\n",
        "'''\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        # Linear function\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
        "        # Non-linearity\n",
        "        self.relu = nn.ReLU()\n",
        "        # Linear function (readout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Linear function\n",
        "        out = self.fc1(x)\n",
        "        # Non-linearity\n",
        "        out = self.relu(out)\n",
        "        # Linear function (readout)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "'''\n",
        "STEP 4: INSTANTIATE MODEL CLASS\n",
        "'''\n",
        "input_dim = 28*28\n",
        "hidden_dim = 100\n",
        "output_dim = 10\n",
        "\n",
        "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "'''\n",
        "STEP 5: INSTANTIATE LOSS CLASS\n",
        "'''\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "'''\n",
        "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
        "'''\n",
        "learning_rate = 0.1\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "'''\n",
        "STEP 7: TRAIN THE MODEL\n",
        "'''\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Load images as Variable\n",
        "        images = images.view(-1, 28*28).requires_grad_()\n",
        "        \n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "        \n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        iter += 1\n",
        "        \n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                # Load images to a Torch Variable\n",
        "                images = images.view(-1, 28*28)\n",
        "                \n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "                \n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                \n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "                \n",
        "                # Total correct predictions\n",
        "                correct += (predicted == labels).sum()\n",
        "            \n",
        "            accuracy = 100 * correct / total\n",
        "            \n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "375c0fa26fe84659b5ea58ca1b6d5885",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "30f0ad6de7ed491d870c8634bdec27d1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1dbd5846a293491491638b0a8e7dea53",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "466f9b0076174a1ab9e1264a91b16060",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration: 500. Loss: 0.26971518993377686. Accuracy: 91.51000213623047\n",
            "Iteration: 1000. Loss: 0.1802702695131302. Accuracy: 92.87999725341797\n",
            "Iteration: 1500. Loss: 0.1705324351787567. Accuracy: 94.13999938964844\n",
            "Iteration: 2000. Loss: 0.2071806788444519. Accuracy: 94.68000030517578\n",
            "Iteration: 2500. Loss: 0.15649180114269257. Accuracy: 95.16000366210938\n",
            "Iteration: 3000. Loss: 0.13750530779361725. Accuracy: 95.83999633789062\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXnqJXOd56FQ"
      },
      "source": [
        "### Non-Technical Process \n",
        "1. Convert inputs/labels to variables\n",
        "2. Clear gradient buffers\n",
        "3. Get output given inputs \n",
        "4. Get loss by comparing with labels\n",
        "5. Get gradients w.r.t. parameters\n",
        "6. **Update parameters using gradients**\n",
        "    - `parameters = parameters - learning_rate * parameters_gradients`\n",
        "7. REPEAT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbvZYlbd56FT"
      },
      "source": [
        "### Why is it called Gradient Descent? \n",
        "- Use gradients (error signals) $\\rightarrow$ update parameters to minimize our loss (descent) $\\rightarrow$ better predictive accuracy\n",
        "\n",
        "\n",
        "### Mathematical Interpretation of Gradient Descent\n",
        "- Model's parameters: $\\theta \\in ℝ^d$\n",
        "- Loss function: $J(\\theta)$\n",
        "- Gradient w.r.t. parameters: $ \\nabla J(\\theta)$\n",
        "- Learning rate: $\\eta$\n",
        "- Batch Gradient descent: $\\theta = \\theta - \\eta \\cdot  \\nabla J(\\theta)$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKKSZr2Z56FX"
      },
      "source": [
        "## Optimization Algorithm 1: Batch Gradient Descent\n",
        "- What we've covered so far: batch gradient descent\n",
        "    - $\\theta = \\theta - \\eta \\cdot  \\nabla J(\\theta)$\n",
        "- Characteristics\n",
        "    - Compute the gradient of the lost function w.r.t. parameters for the entire training data, $\\nabla J(\\theta)$ \n",
        "    - Use this to update our parameters at every iteration\n",
        "- Problems\n",
        "    - Unable to fit whole datasets in memory \n",
        "    - Computationally slow as we attempt to compute a large Jacobian matrix $\\rightarrow$ first order derivative, $\\nabla J(\\theta)$\n",
        "- Conceptually easy to understand $\\rightarrow$ rarely used"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgZTyz7856Fa"
      },
      "source": [
        "## Optimization Algorithm 2: Stochastic Gradient Descent \n",
        "- Modification of batch gradient descent\n",
        "    - $\\theta = \\theta - \\eta \\cdot  \\nabla J(\\theta, x^{i}, y^{i})$\n",
        "- Characteristics\n",
        "    - Compute the gradient of the lost function w.r.t. parameters for the **one set of training sample (1 input and 1 label)**, $\\nabla J(\\theta, x^{i}, y^{i})$\n",
        "    - Use this to update our parameters at every iteration\n",
        "- Benefits\n",
        "    - Able to fit large datasets\n",
        "    - Computationally faster $\\rightarrow$ instead gradients w.r.t to the whole training data, we get the gradients w.r.t. training sample\n",
        "- Problems\n",
        "    - Updating very frequently $\\rightarrow$ huge variance in parameter updates $\\rightarrow$ may overshoot local minima \n",
        "        - Can be solved by carefully decaying your learning rate $\\rightarrow$ take smaller steps in incorporating gradients to improve the parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD9xqsKR56Fc"
      },
      "source": [
        "## Optimization Algorithm 3: Mini-batch Gradient Descent\n",
        "- Combination of batch gradient descent & stochastic gradient descent\n",
        "    - $\\theta = \\theta - \\eta \\cdot  \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n})$\n",
        "- Characteristics\n",
        "    - Compute the gradient of the lost function w.r.t. parameters for **n sets of training sample (n input and n label)**, $\\nabla J(\\theta, x^{i: i+n}, y^{i:i+n})$\n",
        "    - Use this to update our parameters at every iteration\n",
        "- Benefits\n",
        "    - Able to fit large datasets\n",
        "    - Computationally faster $\\rightarrow$ instead gradients w.r.t to the whole training data, we get the gradients w.r.t. training sample\n",
        "    - Lower variance of parameter updates\n",
        "- This is often called SGD in deep learning frameworks .__.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uwywzB056Ff",
        "outputId": "c7095d13-f1da-48f6-f1a1-de3e87a75c30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "# Set seed\n",
        "torch.manual_seed(0)\n",
        "\n",
        "'''\n",
        "STEP 1: LOADING DATASET\n",
        "'''\n",
        "\n",
        "train_dataset = dsets.MNIST(root='./data', \n",
        "                            train=True, \n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)\n",
        "\n",
        "test_dataset = dsets.MNIST(root='./data', \n",
        "                           train=False, \n",
        "                           transform=transforms.ToTensor())\n",
        "\n",
        "'''\n",
        "STEP 2: MAKING DATASET ITERABLE\n",
        "'''\n",
        "\n",
        "batch_size = 100\n",
        "n_iters = 3000\n",
        "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "'''\n",
        "STEP 3: CREATE MODEL CLASS\n",
        "'''\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        # Linear function\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        # Non-linearity\n",
        "        self.relu = nn.ReLU()\n",
        "        # Linear function (readout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Linear function\n",
        "        out = self.fc1(x)\n",
        "        # Non-linearity\n",
        "        out = self.relu(out)\n",
        "        # Linear function (readout)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "'''\n",
        "STEP 4: INSTANTIATE MODEL CLASS\n",
        "'''\n",
        "input_dim = 28*28\n",
        "hidden_dim = 100\n",
        "output_dim = 10\n",
        "\n",
        "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "'''\n",
        "STEP 5: INSTANTIATE LOSS CLASS\n",
        "'''\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "'''\n",
        "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
        "'''\n",
        "learning_rate = 0.1\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "'''\n",
        "STEP 7: TRAIN THE MODEL\n",
        "'''\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Load images as Variable\n",
        "        images = images.view(-1, 28*28).requires_grad_()\n",
        "        \n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "        \n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        iter += 1\n",
        "        \n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                # Load images to a Torch Variable\n",
        "                images = images.view(-1, 28*28).requires_grad_()\n",
        "                \n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "                \n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                \n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "                \n",
        "                # Total correct predictions\n",
        "                correct += (predicted == labels).sum()\n",
        "            \n",
        "            accuracy = 100 * correct / total\n",
        "            \n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 0.26971518993377686. Accuracy: 91.51000213623047\n",
            "Iteration: 1000. Loss: 0.1802702695131302. Accuracy: 92.87999725341797\n",
            "Iteration: 1500. Loss: 0.1705324351787567. Accuracy: 94.13999938964844\n",
            "Iteration: 2000. Loss: 0.2071806788444519. Accuracy: 94.68000030517578\n",
            "Iteration: 2500. Loss: 0.15649180114269257. Accuracy: 95.16000366210938\n",
            "Iteration: 3000. Loss: 0.13750530779361725. Accuracy: 95.83999633789062\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OeIA0GW56Fu"
      },
      "source": [
        "## Optimization Algorithm 4: SGD Momentum\n",
        "- Modification of SGD\n",
        "    - $v_t = \\gamma v_{t-1} + \\eta \\cdot  \\nabla J(\\theta, x^{i: i+n}, y^{i:i+n})$\n",
        "    - $\\theta = \\theta - v_t$\n",
        "- Characteristics\n",
        "    - Compute the gradient of the lost function w.r.t. parameters for **n sets of training sample (n input and n label)**, $\\nabla J(\\theta, x^{i: i+n}, y^{i:i+n})$\n",
        "    - Use this to add to the previous update vector $v_{t-1}$\n",
        "    - Momentum, usually set to $\\gamma = 0.9$\n",
        "    - Parameters updated with update vector, $v_t$ that incorporates previous update vector\n",
        "        - $\\gamma v_{t}$ increases if gradient same sign/direction as $v_{t-1}$ \n",
        "            - Gives SGD the push when it is going in the right direction (minimizing loss)\n",
        "            - Accelerated convergence\n",
        "        - $\\gamma v_{t}$ decreases if gradient different sign/direction as $v_{t-1}$\n",
        "            - Dampens SGD when it is going in a different direction\n",
        "            - Lower variation in loss minimization\n",
        "- Problems\n",
        "    - It might go the wrong direction (higher loss) $\\rightarrow$ continue to be accelerated to the wrong direction (higher loss) \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X6BGz2056Fx",
        "outputId": "c81b5b9c-7de2-4de4-974f-5411fef59636",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "# Set seed\n",
        "torch.manual_seed(0)\n",
        "\n",
        "'''\n",
        "STEP 1: LOADING DATASET\n",
        "'''\n",
        "\n",
        "train_dataset = dsets.MNIST(root='./data', \n",
        "                            train=True, \n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)\n",
        "\n",
        "test_dataset = dsets.MNIST(root='./data', \n",
        "                           train=False, \n",
        "                           transform=transforms.ToTensor())\n",
        "\n",
        "'''\n",
        "STEP 2: MAKING DATASET ITERABLE\n",
        "'''\n",
        "\n",
        "batch_size = 100\n",
        "n_iters = 3000\n",
        "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "'''\n",
        "STEP 3: CREATE MODEL CLASS\n",
        "'''\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        # Linear function\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
        "        # Non-linearity\n",
        "        self.relu = nn.ReLU()\n",
        "        # Linear function (readout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Linear function\n",
        "        out = self.fc1(x)\n",
        "        # Non-linearity\n",
        "        out = self.relu(out)\n",
        "        # Linear function (readout)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "'''\n",
        "STEP 4: INSTANTIATE MODEL CLASS\n",
        "'''\n",
        "input_dim = 28*28\n",
        "hidden_dim = 100\n",
        "output_dim = 10\n",
        "\n",
        "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "'''\n",
        "STEP 5: INSTANTIATE LOSS CLASS\n",
        "'''\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "'''\n",
        "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
        "'''\n",
        "learning_rate = 0.1\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "\n",
        "'''\n",
        "STEP 7: TRAIN THE MODEL\n",
        "'''\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Load images as Variable\n",
        "        images = images.view(-1, 28*28).requires_grad_()\n",
        "        \n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "        \n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        iter += 1\n",
        "        \n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                # Load images to a Torch Variable\n",
        "                images = images.view(-1, 28*28)\n",
        "                \n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "                \n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                \n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "                \n",
        "                # Total correct predictions\n",
        "                correct += (predicted == labels).sum()\n",
        "            \n",
        "            accuracy = 100 * correct / total\n",
        "            \n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 0.11467370390892029. Accuracy: 96.19999694824219\n",
            "Iteration: 1000. Loss: 0.056595753878355026. Accuracy: 96.55000305175781\n",
            "Iteration: 1500. Loss: 0.15522664785385132. Accuracy: 97.16999816894531\n",
            "Iteration: 2000. Loss: 0.046876028180122375. Accuracy: 97.30999755859375\n",
            "Iteration: 2500. Loss: 0.058556076139211655. Accuracy: 97.16999816894531\n",
            "Iteration: 3000. Loss: 0.08808634430170059. Accuracy: 97.52999877929688\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu32XZmi56F8"
      },
      "source": [
        "## Optimization Algorithm 4: SGD Nesterov\n",
        "- Modification of SGD Momentum \n",
        "    - $v_t = \\gamma v_{t-1} + \\eta \\cdot  \\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n})$\n",
        "    - $\\theta = \\theta - v_t$\n",
        "- Characteristics\n",
        "    - Compute the gradient of the lost function w.r.t. **future approximate parameters** for **n sets of training sample (n input and n label)**, $\\nabla J(\\theta - \\gamma v_{t-1}, x^{i: i+n}, y^{i:i+n})$\n",
        "        - Use this to add to the previous update vector $v_{t-1}$\n",
        "        - Momentum, usually set to $\\gamma = 0.9$\n",
        "    - Gradients w.r.t. future approximate parameters $\\rightarrow$ sense of where we will be $\\rightarrow$ anticipate if we are going in the wrong direction in the next step $\\rightarrow$ slow down accordingly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_miSbo4R56F-",
        "outputId": "1b696489-5810-4576-cf64-7f0e6f69242f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "# Set seed\n",
        "torch.manual_seed(0)\n",
        "\n",
        "'''\n",
        "STEP 1: LOADING DATASET\n",
        "'''\n",
        "\n",
        "train_dataset = dsets.MNIST(root='./data', \n",
        "                            train=True, \n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)\n",
        "\n",
        "test_dataset = dsets.MNIST(root='./data', \n",
        "                           train=False, \n",
        "                           transform=transforms.ToTensor())\n",
        "\n",
        "'''\n",
        "STEP 2: MAKING DATASET ITERABLE\n",
        "'''\n",
        "\n",
        "batch_size = 100\n",
        "n_iters = 3000\n",
        "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "'''\n",
        "STEP 3: CREATE MODEL CLASS\n",
        "'''\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        # Linear function\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
        "        # Non-linearity\n",
        "        self.relu = nn.ReLU()\n",
        "        # Linear function (readout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Linear function\n",
        "        out = self.fc1(x)\n",
        "        # Non-linearity\n",
        "        out = self.relu(out)\n",
        "        # Linear function (readout)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "'''\n",
        "STEP 4: INSTANTIATE MODEL CLASS\n",
        "'''\n",
        "input_dim = 28*28\n",
        "hidden_dim = 100\n",
        "output_dim = 10\n",
        "\n",
        "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "'''\n",
        "STEP 5: INSTANTIATE LOSS CLASS\n",
        "'''\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "'''\n",
        "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
        "'''\n",
        "learning_rate = 0.1\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)\n",
        "\n",
        "'''\n",
        "STEP 7: TRAIN THE MODEL\n",
        "'''\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Load images as Variable\n",
        "        images = images.view(-1, 28*28).requires_grad_()\n",
        "        \n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "        \n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        iter += 1\n",
        "        \n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                # Load images to a Torch Variable\n",
        "                images = images.view(-1, 28*28)\n",
        "                \n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "                \n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                \n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "                \n",
        "                # Total correct predictions\n",
        "                correct += (predicted == labels).sum()\n",
        "            \n",
        "            accuracy = 100 * correct / total\n",
        "            \n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 0.15253432095050812. Accuracy: 96.38999938964844\n",
            "Iteration: 1000. Loss: 0.055656854063272476. Accuracy: 96.44000244140625\n",
            "Iteration: 1500. Loss: 0.13686475157737732. Accuracy: 97.18000030517578\n",
            "Iteration: 2000. Loss: 0.07397746294736862. Accuracy: 97.45999908447266\n",
            "Iteration: 2500. Loss: 0.07314040511846542. Accuracy: 97.37999725341797\n",
            "Iteration: 3000. Loss: 0.06474008411169052. Accuracy: 97.44999694824219\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpNKB26a56GJ"
      },
      "source": [
        "## Optimization Algorithm 4: Adam\n",
        "- Adaptive Learning Rates\n",
        "    - $m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)g_t$\n",
        "        - Keeping track of decaying gradient\n",
        "        - Estimate of the mean of gradients\n",
        "    - $v_t = \\beta_2 v_{t-1} + (1 - \\beta_2)g_t^2$\n",
        "        - Keeping track of decaying squared gradient \n",
        "        - Estimate of the variance of gradients\n",
        "    - When $m_t, v_t$ initializes as 0, $m_t, v_t \\rightarrow 0$ initially when decay rates small, $\\beta_1, \\beta_2 \\rightarrow 1$  \n",
        "        - Need to correct this with:\n",
        "        - $ \\hat m_t = \\frac{m_t}{1- \\beta_1}$\n",
        "        - $ \\hat v_t = \\frac{v_t}{1- \\beta_2}$\n",
        "    - $\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat v_t} + \\epsilon}\\hat m_t$\n",
        "    - Default recommended values\n",
        "        - $\\beta_1 = 0.9$\n",
        "        - $\\beta_2 = 0.999$\n",
        "        - $\\epsilon = 10^{-8}$\n",
        "- Instead of learning rate $\\rightarrow$ equations account for estimates of mean/variance of gradients to determine the next learning rate\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SVxNtNl56GM",
        "outputId": "208752ee-d325-4981-f257-91e8c2861845",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "# Set seed\n",
        "torch.manual_seed(0)\n",
        "\n",
        "'''\n",
        "STEP 1: LOADING DATASET\n",
        "'''\n",
        "\n",
        "train_dataset = dsets.MNIST(root='./data', \n",
        "                            train=True, \n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)\n",
        "\n",
        "test_dataset = dsets.MNIST(root='./data', \n",
        "                           train=False, \n",
        "                           transform=transforms.ToTensor())\n",
        "\n",
        "'''\n",
        "STEP 2: MAKING DATASET ITERABLE\n",
        "'''\n",
        "\n",
        "batch_size = 100\n",
        "n_iters = 3000\n",
        "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "'''\n",
        "STEP 3: CREATE MODEL CLASS\n",
        "'''\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        # Linear function\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
        "        # Non-linearity\n",
        "        self.relu = nn.ReLU()\n",
        "        # Linear function (readout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Linear function\n",
        "        out = self.fc1(x)\n",
        "        # Non-linearity\n",
        "        out = self.relu(out)\n",
        "        # Linear function (readout)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "'''\n",
        "STEP 4: INSTANTIATE MODEL CLASS\n",
        "'''\n",
        "input_dim = 28*28\n",
        "hidden_dim = 100\n",
        "output_dim = 10\n",
        "\n",
        "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "'''\n",
        "STEP 5: INSTANTIATE LOSS CLASS\n",
        "'''\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "'''\n",
        "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
        "'''\n",
        "# learning_rate = 0.001\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "'''\n",
        "STEP 7: TRAIN THE MODEL\n",
        "'''\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Load images as Variable\n",
        "        images = images.view(-1, 28*28).requires_grad_()\n",
        "        \n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "        \n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        iter += 1\n",
        "        \n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                # Load images to a Torch Variable\n",
        "                images = images.view(-1, 28*28)\n",
        "                \n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "                \n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                \n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "                \n",
        "                # Total correct predictions\n",
        "                correct += (predicted == labels).sum()\n",
        "            \n",
        "            accuracy = 100 * correct / total\n",
        "            \n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 0.23362819850444794. Accuracy: 93.3499984741211\n",
            "Iteration: 1000. Loss: 0.11055783182382584. Accuracy: 94.8499984741211\n",
            "Iteration: 1500. Loss: 0.14103564620018005. Accuracy: 95.95999908447266\n",
            "Iteration: 2000. Loss: 0.107691690325737. Accuracy: 96.55000305175781\n",
            "Iteration: 2500. Loss: 0.0956580713391304. Accuracy: 96.56999969482422\n",
            "Iteration: 3000. Loss: 0.07698246836662292. Accuracy: 97.16999816894531\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l6X7dF456GX"
      },
      "source": [
        "## Other Adaptive Algorithms\n",
        "- Other adaptive algorithms (like Adam, adapting learning rates)\n",
        "    - Adagrad\n",
        "    - Adadelta\n",
        "    - Adamax\n",
        "    - RMSProp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl3hvbvg56GY"
      },
      "source": [
        "## Optimization Algorithm 5: Adagrad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX1fdYEB56Ga",
        "outputId": "9fb6bc9d-50da-4084-de55-ea911e43bee3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "# Set seed\n",
        "torch.manual_seed(0)\n",
        "\n",
        "'''\n",
        "STEP 1: LOADING DATASET\n",
        "'''\n",
        "\n",
        "train_dataset = dsets.MNIST(root='./data', \n",
        "                            train=True, \n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)\n",
        "\n",
        "test_dataset = dsets.MNIST(root='./data', \n",
        "                           train=False, \n",
        "                           transform=transforms.ToTensor())\n",
        "\n",
        "'''\n",
        "STEP 2: MAKING DATASET ITERABLE\n",
        "'''\n",
        "\n",
        "batch_size = 100\n",
        "n_iters = 3000\n",
        "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "'''\n",
        "STEP 3: CREATE MODEL CLASS\n",
        "'''\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        # Linear function\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
        "        # Non-linearity\n",
        "        self.relu = nn.ReLU()\n",
        "        # Linear function (readout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Linear function\n",
        "        out = self.fc1(x)\n",
        "        # Non-linearity\n",
        "        out = self.relu(out)\n",
        "        # Linear function (readout)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "'''\n",
        "STEP 4: INSTANTIATE MODEL CLASS\n",
        "'''\n",
        "input_dim = 28*28\n",
        "hidden_dim = 100\n",
        "output_dim = 10\n",
        "\n",
        "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "'''\n",
        "STEP 5: INSTANTIATE LOSS CLASS\n",
        "'''\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "'''\n",
        "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
        "'''\n",
        "# learning_rate = 0.001\n",
        "\n",
        "optimizer = torch.optim.Adagrad(model.parameters())\n",
        "\n",
        "'''\n",
        "STEP 7: TRAIN THE MODEL\n",
        "'''\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Load images as Variable\n",
        "        images = images.view(-1, 28*28).requires_grad_()\n",
        "        \n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "        \n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        iter += 1\n",
        "        \n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                # Load images to a Torch Variable\n",
        "                images = images.view(-1, 28*28)\n",
        "                \n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "                \n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                \n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "                \n",
        "                # Total correct predictions\n",
        "                correct += (predicted == labels).sum()\n",
        "            \n",
        "            accuracy = 100 * correct / total\n",
        "            \n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 0.24082742631435394. Accuracy: 93.0\n",
            "Iteration: 1000. Loss: 0.15385352075099945. Accuracy: 93.9800033569336\n",
            "Iteration: 1500. Loss: 0.1544286012649536. Accuracy: 94.80999755859375\n",
            "Iteration: 2000. Loss: 0.17311468720436096. Accuracy: 95.12000274658203\n",
            "Iteration: 2500. Loss: 0.19428884983062744. Accuracy: 95.37999725341797\n",
            "Iteration: 3000. Loss: 0.14685769379138947. Accuracy: 95.66999816894531\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__bKTzgn56Gi"
      },
      "source": [
        "## Optimization Algorithm 6: Adadelta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKGyvTeB56Gk",
        "outputId": "9cb53e62-f290-4689-d59f-e1c6d4940e10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# Set seed\n",
        "torch.manual_seed(0)\n",
        "\n",
        "'''\n",
        "STEP 1: LOADING DATASET\n",
        "'''\n",
        "\n",
        "train_dataset = dsets.MNIST(root='./data', \n",
        "                            train=True, \n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)\n",
        "\n",
        "test_dataset = dsets.MNIST(root='./data', \n",
        "                           train=False, \n",
        "                           transform=transforms.ToTensor())\n",
        "\n",
        "'''\n",
        "STEP 2: MAKING DATASET ITERABLE\n",
        "'''\n",
        "\n",
        "batch_size = 100\n",
        "n_iters = 3000\n",
        "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "'''\n",
        "STEP 3: CREATE MODEL CLASS\n",
        "'''\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        # Linear function\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
        "        # Non-linearity\n",
        "        self.relu = nn.ReLU()\n",
        "        # Linear function (readout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Linear function\n",
        "        out = self.fc1(x)\n",
        "        # Non-linearity\n",
        "        out = self.relu(out)\n",
        "        # Linear function (readout)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "'''\n",
        "STEP 4: INSTANTIATE MODEL CLASS\n",
        "'''\n",
        "input_dim = 28*28\n",
        "hidden_dim = 100\n",
        "output_dim = 10\n",
        "\n",
        "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "'''\n",
        "STEP 5: INSTANTIATE LOSS CLASS\n",
        "'''\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "'''\n",
        "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
        "'''\n",
        "# learning_rate = 0.001\n",
        "\n",
        "optimizer = torch.optim.Adadelta(model.parameters())\n",
        "\n",
        "'''\n",
        "STEP 7: TRAIN THE MODEL\n",
        "'''\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Load images as Variable\n",
        "        images = images.view(-1, 28*28).requires_grad_()\n",
        "        \n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "        \n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        iter += 1\n",
        "        \n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                # Load images to a Torch Variablee\n",
        "                images = images.view(-1, 28*28)\n",
        "                \n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "                \n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                \n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "                \n",
        "                # Total correct predictions\n",
        "                correct += (predicted == labels).sum()\n",
        "            \n",
        "            accuracy = 100 * correct / total\n",
        "            \n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 0.16266268491744995. Accuracy: 94.79000091552734\n",
            "Iteration: 1000. Loss: 0.06723383069038391. Accuracy: 95.98999786376953\n",
            "Iteration: 1500. Loss: 0.11248779296875. Accuracy: 96.94999694824219\n",
            "Iteration: 2000. Loss: 0.06128310784697533. Accuracy: 97.18000030517578\n",
            "Iteration: 2500. Loss: 0.05549431964755058. Accuracy: 97.25\n",
            "Iteration: 3000. Loss: 0.04269982874393463. Accuracy: 97.37999725341797\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbvwCp3556Gu"
      },
      "source": [
        "## Optimization Algorithm 6: Adamax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74A175Y256Gx",
        "outputId": "1bf7f15f-5823-4e40-a70e-155dcdf2babd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "# Set seed\n",
        "torch.manual_seed(0)\n",
        "\n",
        "'''\n",
        "STEP 1: LOADING DATASET\n",
        "'''\n",
        "\n",
        "train_dataset = dsets.MNIST(root='./data', \n",
        "                            train=True, \n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)\n",
        "\n",
        "test_dataset = dsets.MNIST(root='./data', \n",
        "                           train=False, \n",
        "                           transform=transforms.ToTensor())\n",
        "\n",
        "'''\n",
        "STEP 2: MAKING DATASET ITERABLE\n",
        "'''\n",
        "\n",
        "batch_size = 100\n",
        "n_iters = 3000\n",
        "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "'''\n",
        "STEP 3: CREATE MODEL CLASS\n",
        "'''\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        # Linear function\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
        "        # Non-linearity\n",
        "        self.relu = nn.ReLU()\n",
        "        # Linear function (readout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Linear function\n",
        "        out = self.fc1(x)\n",
        "        # Non-linearity\n",
        "        out = self.relu(out)\n",
        "        # Linear function (readout)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "'''\n",
        "STEP 4: INSTANTIATE MODEL CLASS\n",
        "'''\n",
        "input_dim = 28*28\n",
        "hidden_dim = 100\n",
        "output_dim = 10\n",
        "\n",
        "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "'''\n",
        "STEP 5: INSTANTIATE LOSS CLASS\n",
        "'''\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "'''\n",
        "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
        "'''\n",
        "# learning_rate = 0.001\n",
        "\n",
        "optimizer = torch.optim.Adamax(model.parameters())\n",
        "\n",
        "'''\n",
        "STEP 7: TRAIN THE MODEL\n",
        "'''\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Load images as Variable\n",
        "        images = images.view(-1, 28*28).requires_grad_()\n",
        "        \n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "        \n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        iter += 1\n",
        "        \n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                # Load images to a Torch Variable\n",
        "                images = images.view(-1, 28*28)\n",
        "                \n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "                \n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                \n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "                \n",
        "                # Total correct predictions\n",
        "                correct += (predicted == labels).sum()\n",
        "            \n",
        "            accuracy = 100 * correct / total\n",
        "            \n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 0.24909545481204987. Accuracy: 92.19000244140625\n",
            "Iteration: 1000. Loss: 0.1435246467590332. Accuracy: 93.61000061035156\n",
            "Iteration: 1500. Loss: 0.15786641836166382. Accuracy: 94.9800033569336\n",
            "Iteration: 2000. Loss: 0.14738872647285461. Accuracy: 95.48999786376953\n",
            "Iteration: 2500. Loss: 0.14099086821079254. Accuracy: 95.81999969482422\n",
            "Iteration: 3000. Loss: 0.11718937754631042. Accuracy: 96.55000305175781\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcwsmLCV56G6"
      },
      "source": [
        "## Optimization Algorithm 7: RMSProp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaA-tnoC56G8",
        "outputId": "eed572cf-3424-4a6f-ebde-f95e3ad0d9f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "\n",
        "# Set seed\n",
        "torch.manual_seed(0)\n",
        "\n",
        "'''\n",
        "STEP 1: LOADING DATASET\n",
        "'''\n",
        "\n",
        "train_dataset = dsets.MNIST(root='./data', \n",
        "                            train=True, \n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)\n",
        "\n",
        "test_dataset = dsets.MNIST(root='./data', \n",
        "                           train=False, \n",
        "                           transform=transforms.ToTensor())\n",
        "\n",
        "'''\n",
        "STEP 2: MAKING DATASET ITERABLE\n",
        "'''\n",
        "\n",
        "batch_size = 100\n",
        "n_iters = 3000\n",
        "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)\n",
        "\n",
        "'''\n",
        "STEP 3: CREATE MODEL CLASS\n",
        "'''\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        # Linear function\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
        "        # Non-linearity\n",
        "        self.relu = nn.ReLU()\n",
        "        # Linear function (readout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Linear function\n",
        "        out = self.fc1(x)\n",
        "        # Non-linearity\n",
        "        out = self.relu(out)\n",
        "        # Linear function (readout)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "'''\n",
        "STEP 4: INSTANTIATE MODEL CLASS\n",
        "'''\n",
        "input_dim = 28*28\n",
        "hidden_dim = 100\n",
        "output_dim = 10\n",
        "\n",
        "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "'''\n",
        "STEP 5: INSTANTIATE LOSS CLASS\n",
        "'''\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "'''\n",
        "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
        "'''\n",
        "# learning_rate = 0.001\n",
        "\n",
        "optimizer = torch.optim.RMSprop(model.parameters())\n",
        "\n",
        "'''\n",
        "STEP 7: TRAIN THE MODEL\n",
        "'''\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Load images as Variable\n",
        "        images = images.view(-1, 28*28).requires_grad_()\n",
        "        \n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "        \n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "        \n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        iter += 1\n",
        "        \n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                # Load images to a Torch Variable\n",
        "                images = images.view(-1, 28*28).requires_grad_()\n",
        "                \n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "                \n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                \n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "                \n",
        "                # Total correct predictions\n",
        "                correct += (predicted == labels).sum()\n",
        "            \n",
        "            accuracy = 100 * correct / total\n",
        "            \n",
        "            # Print Loss\n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 0.1688031703233719. Accuracy: 95.13999938964844\n",
            "Iteration: 1000. Loss: 0.035940684378147125. Accuracy: 95.55999755859375\n",
            "Iteration: 1500. Loss: 0.13070625066757202. Accuracy: 96.19000244140625\n",
            "Iteration: 2000. Loss: 0.12187916785478592. Accuracy: 95.56999969482422\n",
            "Iteration: 2500. Loss: 0.22075626254081726. Accuracy: 96.55999755859375\n",
            "Iteration: 3000. Loss: 0.06314142048358917. Accuracy: 96.76000213623047\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYQn5k-D56HG"
      },
      "source": [
        "## Summary of Optimization Algorithms Performance\n",
        "- SGD: 95.78%\n",
        "- **SGD Momentum: 97.69%**\n",
        "- **SGD Nesterov: 97.58%**\n",
        "- **Adam: 97.20%**\n",
        "- Adagrad: 95.51%\n",
        "- **Adadelta: 97.45%**\n",
        "- Adamax: 96.58%\n",
        "- **RMSProp: 97.1%**\n",
        "\n",
        "## Simple Suggestions\n",
        "- Momentum/Nesterov\n",
        "    - Powerful if we control the learning rate schedule\n",
        "- Adam\n",
        "    - Lazy to control the learning rate schedule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR3JdESG56HI"
      },
      "source": [
        "## Summary\n",
        "- **Recap of 7 step process**\n",
        "    - Step 1: Load Dataset\n",
        "    - Step 2: Make Dataset Iterable\n",
        "    - Step 3: Create Model Class\n",
        "    - Step 4: Instantiate Model Class\n",
        "    - Step 5: Instantiate Loss Class\n",
        "    - **Step 6: Instantiate Optimizer Class**\n",
        "    - Step 7: Train Model\n",
        "- **Step 6**\n",
        "    - Update parameters using gradients\n",
        "    - `parameters = parameters - learning_rate * parameters_gradients`\n",
        "- **Gradient descent**\n",
        "    - Using gradients (error signals from loss class) to update parameters\n",
        "- **Mathematical** interpretation: $\\theta = \\theta - \\eta \\cdot  \\nabla J(\\theta)$\n",
        "- **Optimisation Algorithms**\n",
        "    - Batch gradient descent\n",
        "    - Stochastic gradient descent\n",
        "    - Mini-batch gradient descent (SGD)\n",
        "    - SGD + Momentum\n",
        "    - SGD + Nesterov\n",
        "    - Adam\n",
        "    - Other adaptive algorithms: adagrad, adamax, adadelta, RMSProp\n",
        "- **Recommendations**\n",
        "    - SGD+M\n",
        "    - SGD+N\n",
        "    - Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1GgsvzhCUsX"
      },
      "source": [
        "See Tune for hyperparameter optimization: https://docs.ray.io/en/master/tune/\n",
        "\n",
        "And: https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP1RrLa3flWE"
      },
      "source": [
        "This is a must (super informative): https://playground.tensorflow.org/ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNBME0iQfumq"
      },
      "source": [
        "# Now let's work on our benchmark model: This time with Adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Oz5U6kvgFnH"
      },
      "source": [
        "# Modified based on https://github.com/pytorch/examples/tree/master/mnist\n",
        "\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "\n",
        "def train(args, model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "            100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3jfdOXDgFne"
      },
      "source": [
        "    torch.manual_seed(112)\n",
        "    device =torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t0jD_EQgFns",
        "outputId": "a95985fb-c160-49c3-8d1f-58683ca0a683",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391,
          "referenced_widgets": [
            "ad333496c2e8439eb818e7890674f2a6",
            "e29ab94fedb041c29dbb93ef9b5a448a",
            "9dbde49b93d74aa7a1613bd053c2d63a",
            "5f1a0f64a0ca44b68c98ecdcc601a453",
            "aded5ce8dcaa4352984753b3acb6fa6c",
            "47b7df285c184448b2c390e79aa90542",
            "91369670e30b46b7a53995247f564097",
            "86187e99d3a946b69f89b61cac4f31ee",
            "d8d055366c82440492defe9c6863dd3c",
            "89c2c827fd8943bcbd68b58b195e369b",
            "37861fcb323e438098233ca09f7827ad",
            "fb7291e727db4509a6a7a67d1dda812c",
            "f3583ae523584a1baa4f1fb4c3d86cce",
            "8f3f9a844ee444ad873f353d30944f14",
            "4697bfdabfec4ad2b0ade20cf7b90ae4",
            "8504e22fb09c453f991fdae53f0e3a87",
            "3042072dfdda4d1db834f890dd662e19",
            "10f0b56358e444c98bab5dcbae6fc6cf",
            "b3dc332146684f0194ebd5b26031c5fa",
            "42b508582fce444d8523ce4b9d03911d",
            "c60eb968a6454cb2857e69b574a8ee5d",
            "7fd1956f137740ee846a0a882e77c5a0",
            "074914326b6f48009d6200fa3582c802",
            "6c982346032d430b98feb8a49073810a",
            "922510af19674f1cb630f6603eae6eb4",
            "deef16bc03034c53a6dfc3f7e0208853",
            "659d56e8c1fc4196b72363d5202182dc",
            "09fc0d5f071e467c9fc20435cb1baa73",
            "d089e7c745f740359777ea0fd4e68204",
            "8b56b6a7cc524115b027963e8fdca5ed",
            "c03f136ccae34e8ca32f6fd3357c7023",
            "1245a5d32fe14b7da71ed2d6b9c4a833"
          ]
        }
      },
      "source": [
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "    dataset_train = datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transform)\n",
        "    dataset_test = datasets.MNIST('../data', train=False,\n",
        "                       transform=transform)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad333496c2e8439eb818e7890674f2a6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8d055366c82440492defe9c6863dd3c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3042072dfdda4d1db834f890dd662e19",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "922510af19674f1cb630f6603eae6eb4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYL25Uu9gFn4",
        "outputId": "da5df0f3-49c3-4f1a-c3ee-44c059392cec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataset_train, dataset_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Dataset MNIST\n",
              "     Number of datapoints: 60000\n",
              "     Root location: ../data\n",
              "     Split: Train\n",
              "     StandardTransform\n",
              " Transform: Compose(\n",
              "                ToTensor()\n",
              "                Normalize(mean=(0.1307,), std=(0.3081,))\n",
              "            ), Dataset MNIST\n",
              "     Number of datapoints: 10000\n",
              "     Root location: ../data\n",
              "     Split: Test\n",
              "     StandardTransform\n",
              " Transform: Compose(\n",
              "                ToTensor()\n",
              "                Normalize(mean=(0.1307,), std=(0.3081,))\n",
              "            ))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bXJbwfCgFoE",
        "outputId": "572540be-8f5d-45cf-9ab9-af37e1b97d4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "torch.manual_seed(123)\n",
        "transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transform)\n",
        "dataset2 = datasets.MNIST('../data', train=False,\n",
        "                       transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=256 , shuffle=True,\n",
        "        num_workers=1,pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=256 , shuffle=True,\n",
        "        num_workers=1,pin_memory=True)\n",
        "\n",
        "model = Net().to(device)\n",
        "#optimizer = optim.Adadelta(model.parameters(), lr=0.01)\n",
        "optimizer=optim.Adam(model.parameters())\n",
        "# see the Adam optimizer precisely: https://pytorch.org/docs/stable/_modules/torch/optim/adam.html#Adam\n",
        "\n",
        "#scheduler = StepLR(optimizer, step_size=5, gamma=0.1 )\n",
        "for epoch in range(1, 20 + 1):\n",
        "  train(10, model, device, train_loader, optimizer, epoch)\n",
        "  test(model, device, test_loader)\n",
        " # scheduler.step()\n",
        "\n",
        "torch.save(model.state_dict(), \"mnist_51.pt\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.290485\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.499287\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.340078\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.307996\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.230379\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.192829\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.120983\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.170480\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.116984\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.088619\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.084167\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.113863\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.074733\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.117818\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.105022\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.100068\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.082639\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.073992\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.050055\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.045510\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.095484\n",
            "Train Epoch: 1 [53760/60000 (89%)]\tLoss: 0.076591\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.124876\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.075977\n",
            "\n",
            "Test set: Average loss: 0.0530, Accuracy: 9834/10000 (98%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.038767\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.045978\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.036462\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.070774\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.034060\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.023839\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.076024\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.055012\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.057485\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.067817\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.058894\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.049877\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.062807\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.050658\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.049850\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.019758\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.033105\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.034204\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.058150\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.017465\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.028050\n",
            "Train Epoch: 2 [53760/60000 (89%)]\tLoss: 0.030081\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.053855\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.037435\n",
            "\n",
            "Test set: Average loss: 0.0437, Accuracy: 9854/10000 (99%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.040258\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.059152\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.023651\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.018681\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.032521\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.042692\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.066232\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.059033\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.052681\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.013628\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.024234\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.012549\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.020801\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.023712\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.042082\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.041305\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.015947\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.039903\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.040085\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.061792\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.031285\n",
            "Train Epoch: 3 [53760/60000 (89%)]\tLoss: 0.021848\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.043242\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.008386\n",
            "\n",
            "Test set: Average loss: 0.0356, Accuracy: 9891/10000 (99%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.011945\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.025798\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.018047\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.037771\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.010642\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.026682\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.014052\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.009249\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.027113\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.032362\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.017960\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.025508\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.020879\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.033773\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.021012\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.012738\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.011860\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.022414\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.016765\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.038518\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.047810\n",
            "Train Epoch: 4 [53760/60000 (89%)]\tLoss: 0.023138\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.007809\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.019850\n",
            "\n",
            "Test set: Average loss: 0.0270, Accuracy: 9911/10000 (99%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.027281\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.019141\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.002880\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.009248\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.009749\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.007365\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.003978\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.072228\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.014402\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.007567\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.010981\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.022093\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.011975\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.017739\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.043542\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.005273\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.018469\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.028206\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.022521\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.025207\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.014452\n",
            "Train Epoch: 5 [53760/60000 (89%)]\tLoss: 0.018824\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.029144\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.026721\n",
            "\n",
            "Test set: Average loss: 0.0326, Accuracy: 9900/10000 (99%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.010272\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.008770\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.010693\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.004123\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.006782\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.004671\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.006275\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.009044\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.010839\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.008757\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.006829\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.029546\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.011522\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.025430\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.016296\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.037090\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.010655\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.010034\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.009099\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.022206\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.015329\n",
            "Train Epoch: 6 [53760/60000 (89%)]\tLoss: 0.018335\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.006499\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.096980\n",
            "\n",
            "Test set: Average loss: 0.0307, Accuracy: 9901/10000 (99%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.005224\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.019179\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.007202\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.044704\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.006920\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.007233\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.020319\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.002918\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.020222\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.019313\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.015101\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.035006\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.030175\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.021197\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.017991\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.031067\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.027156\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.014518\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.004211\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.005764\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.005011\n",
            "Train Epoch: 7 [53760/60000 (89%)]\tLoss: 0.015329\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.011475\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.028486\n",
            "\n",
            "Test set: Average loss: 0.0359, Accuracy: 9897/10000 (99%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.011315\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.001278\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.014843\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.016224\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.019483\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.005331\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.003258\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.004119\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.018758\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.007893\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.016246\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.003988\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.004930\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.005849\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.003323\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.009174\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.004014\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.020384\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.021848\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.002136\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.003024\n",
            "Train Epoch: 8 [53760/60000 (89%)]\tLoss: 0.016807\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.017651\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.003845\n",
            "\n",
            "Test set: Average loss: 0.0346, Accuracy: 9897/10000 (99%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.018072\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.005565\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.008063\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.003216\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.002269\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.000788\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.004188\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.001246\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.005541\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.012201\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.009077\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.004575\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.027594\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.011827\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.002363\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.000616\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.005561\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.011779\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.010796\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.001861\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.002458\n",
            "Train Epoch: 9 [53760/60000 (89%)]\tLoss: 0.020087\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.003859\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.008497\n",
            "\n",
            "Test set: Average loss: 0.0365, Accuracy: 9902/10000 (99%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.023877\n",
            "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.002504\n",
            "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.007028\n",
            "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.034663\n",
            "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.003928\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.003128\n",
            "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.001976\n",
            "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.006431\n",
            "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.001959\n",
            "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.007540\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.006671\n",
            "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.009442\n",
            "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.002096\n",
            "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.003846\n",
            "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.010267\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.003425\n",
            "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.008400\n",
            "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.000293\n",
            "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.009811\n",
            "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.010759\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.012644\n",
            "Train Epoch: 10 [53760/60000 (89%)]\tLoss: 0.003199\n",
            "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.018973\n",
            "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.010061\n",
            "\n",
            "Test set: Average loss: 0.0316, Accuracy: 9914/10000 (99%)\n",
            "\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.002601\n",
            "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 0.010395\n",
            "Train Epoch: 11 [5120/60000 (9%)]\tLoss: 0.003161\n",
            "Train Epoch: 11 [7680/60000 (13%)]\tLoss: 0.001116\n",
            "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 0.007894\n",
            "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.001201\n",
            "Train Epoch: 11 [15360/60000 (26%)]\tLoss: 0.000956\n",
            "Train Epoch: 11 [17920/60000 (30%)]\tLoss: 0.000616\n",
            "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.016990\n",
            "Train Epoch: 11 [23040/60000 (38%)]\tLoss: 0.007838\n",
            "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.007925\n",
            "Train Epoch: 11 [28160/60000 (47%)]\tLoss: 0.011945\n",
            "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.032239\n",
            "Train Epoch: 11 [33280/60000 (55%)]\tLoss: 0.009228\n",
            "Train Epoch: 11 [35840/60000 (60%)]\tLoss: 0.008614\n",
            "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.002370\n",
            "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 0.015199\n",
            "Train Epoch: 11 [43520/60000 (72%)]\tLoss: 0.003611\n",
            "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 0.000442\n",
            "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 0.003657\n",
            "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.001140\n",
            "Train Epoch: 11 [53760/60000 (89%)]\tLoss: 0.003774\n",
            "Train Epoch: 11 [56320/60000 (94%)]\tLoss: 0.009778\n",
            "Train Epoch: 11 [58880/60000 (98%)]\tLoss: 0.001579\n",
            "\n",
            "Test set: Average loss: 0.0344, Accuracy: 9911/10000 (99%)\n",
            "\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.011142\n",
            "Train Epoch: 12 [2560/60000 (4%)]\tLoss: 0.000298\n",
            "Train Epoch: 12 [5120/60000 (9%)]\tLoss: 0.004606\n",
            "Train Epoch: 12 [7680/60000 (13%)]\tLoss: 0.007073\n",
            "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 0.007496\n",
            "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.013871\n",
            "Train Epoch: 12 [15360/60000 (26%)]\tLoss: 0.000764\n",
            "Train Epoch: 12 [17920/60000 (30%)]\tLoss: 0.003678\n",
            "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 0.019518\n",
            "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 0.004954\n",
            "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.002885\n",
            "Train Epoch: 12 [28160/60000 (47%)]\tLoss: 0.013644\n",
            "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 0.004792\n",
            "Train Epoch: 12 [33280/60000 (55%)]\tLoss: 0.012747\n",
            "Train Epoch: 12 [35840/60000 (60%)]\tLoss: 0.003560\n",
            "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.001909\n",
            "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.006008\n",
            "Train Epoch: 12 [43520/60000 (72%)]\tLoss: 0.011605\n",
            "Train Epoch: 12 [46080/60000 (77%)]\tLoss: 0.001010\n",
            "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 0.005986\n",
            "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.053503\n",
            "Train Epoch: 12 [53760/60000 (89%)]\tLoss: 0.008992\n",
            "Train Epoch: 12 [56320/60000 (94%)]\tLoss: 0.000600\n",
            "Train Epoch: 12 [58880/60000 (98%)]\tLoss: 0.001947\n",
            "\n",
            "Test set: Average loss: 0.0348, Accuracy: 9905/10000 (99%)\n",
            "\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.008512\n",
            "Train Epoch: 13 [2560/60000 (4%)]\tLoss: 0.004487\n",
            "Train Epoch: 13 [5120/60000 (9%)]\tLoss: 0.001539\n",
            "Train Epoch: 13 [7680/60000 (13%)]\tLoss: 0.001340\n",
            "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 0.003095\n",
            "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.003006\n",
            "Train Epoch: 13 [15360/60000 (26%)]\tLoss: 0.005426\n",
            "Train Epoch: 13 [17920/60000 (30%)]\tLoss: 0.023585\n",
            "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.011196\n",
            "Train Epoch: 13 [23040/60000 (38%)]\tLoss: 0.004857\n",
            "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.000155\n",
            "Train Epoch: 13 [28160/60000 (47%)]\tLoss: 0.042997\n",
            "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 0.003372\n",
            "Train Epoch: 13 [33280/60000 (55%)]\tLoss: 0.012766\n",
            "Train Epoch: 13 [35840/60000 (60%)]\tLoss: 0.004589\n",
            "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.001979\n",
            "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.009385\n",
            "Train Epoch: 13 [43520/60000 (72%)]\tLoss: 0.000645\n",
            "Train Epoch: 13 [46080/60000 (77%)]\tLoss: 0.006022\n",
            "Train Epoch: 13 [48640/60000 (81%)]\tLoss: 0.004332\n",
            "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.003115\n",
            "Train Epoch: 13 [53760/60000 (89%)]\tLoss: 0.001263\n",
            "Train Epoch: 13 [56320/60000 (94%)]\tLoss: 0.002326\n",
            "Train Epoch: 13 [58880/60000 (98%)]\tLoss: 0.003569\n",
            "\n",
            "Test set: Average loss: 0.0345, Accuracy: 9917/10000 (99%)\n",
            "\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.000244\n",
            "Train Epoch: 14 [2560/60000 (4%)]\tLoss: 0.003146\n",
            "Train Epoch: 14 [5120/60000 (9%)]\tLoss: 0.022441\n",
            "Train Epoch: 14 [7680/60000 (13%)]\tLoss: 0.006240\n",
            "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 0.002941\n",
            "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.022416\n",
            "Train Epoch: 14 [15360/60000 (26%)]\tLoss: 0.001784\n",
            "Train Epoch: 14 [17920/60000 (30%)]\tLoss: 0.000201\n",
            "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 0.026767\n",
            "Train Epoch: 14 [23040/60000 (38%)]\tLoss: 0.001387\n",
            "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.007732\n",
            "Train Epoch: 14 [28160/60000 (47%)]\tLoss: 0.000396\n",
            "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 0.001476\n",
            "Train Epoch: 14 [33280/60000 (55%)]\tLoss: 0.000410\n",
            "Train Epoch: 14 [35840/60000 (60%)]\tLoss: 0.003333\n",
            "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.001699\n",
            "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 0.000518\n",
            "Train Epoch: 14 [43520/60000 (72%)]\tLoss: 0.000642\n",
            "Train Epoch: 14 [46080/60000 (77%)]\tLoss: 0.033355\n",
            "Train Epoch: 14 [48640/60000 (81%)]\tLoss: 0.015444\n",
            "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.002869\n",
            "Train Epoch: 14 [53760/60000 (89%)]\tLoss: 0.007704\n",
            "Train Epoch: 14 [56320/60000 (94%)]\tLoss: 0.004559\n",
            "Train Epoch: 14 [58880/60000 (98%)]\tLoss: 0.007484\n",
            "\n",
            "Test set: Average loss: 0.0362, Accuracy: 9904/10000 (99%)\n",
            "\n",
            "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.009217\n",
            "Train Epoch: 15 [2560/60000 (4%)]\tLoss: 0.001820\n",
            "Train Epoch: 15 [5120/60000 (9%)]\tLoss: 0.014777\n",
            "Train Epoch: 15 [7680/60000 (13%)]\tLoss: 0.003299\n",
            "Train Epoch: 15 [10240/60000 (17%)]\tLoss: 0.007176\n",
            "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.021965\n",
            "Train Epoch: 15 [15360/60000 (26%)]\tLoss: 0.001015\n",
            "Train Epoch: 15 [17920/60000 (30%)]\tLoss: 0.004362\n",
            "Train Epoch: 15 [20480/60000 (34%)]\tLoss: 0.000656\n",
            "Train Epoch: 15 [23040/60000 (38%)]\tLoss: 0.001874\n",
            "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.014196\n",
            "Train Epoch: 15 [28160/60000 (47%)]\tLoss: 0.002509\n",
            "Train Epoch: 15 [30720/60000 (51%)]\tLoss: 0.001096\n",
            "Train Epoch: 15 [33280/60000 (55%)]\tLoss: 0.010221\n",
            "Train Epoch: 15 [35840/60000 (60%)]\tLoss: 0.005156\n",
            "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.002560\n",
            "Train Epoch: 15 [40960/60000 (68%)]\tLoss: 0.004878\n",
            "Train Epoch: 15 [43520/60000 (72%)]\tLoss: 0.001624\n",
            "Train Epoch: 15 [46080/60000 (77%)]\tLoss: 0.002260\n",
            "Train Epoch: 15 [48640/60000 (81%)]\tLoss: 0.000781\n",
            "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.008049\n",
            "Train Epoch: 15 [53760/60000 (89%)]\tLoss: 0.002060\n",
            "Train Epoch: 15 [56320/60000 (94%)]\tLoss: 0.003388\n",
            "Train Epoch: 15 [58880/60000 (98%)]\tLoss: 0.001139\n",
            "\n",
            "Test set: Average loss: 0.0380, Accuracy: 9905/10000 (99%)\n",
            "\n",
            "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.004475\n",
            "Train Epoch: 16 [2560/60000 (4%)]\tLoss: 0.000374\n",
            "Train Epoch: 16 [5120/60000 (9%)]\tLoss: 0.000779\n",
            "Train Epoch: 16 [7680/60000 (13%)]\tLoss: 0.000253\n",
            "Train Epoch: 16 [10240/60000 (17%)]\tLoss: 0.006302\n",
            "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 0.001954\n",
            "Train Epoch: 16 [15360/60000 (26%)]\tLoss: 0.001532\n",
            "Train Epoch: 16 [17920/60000 (30%)]\tLoss: 0.001814\n",
            "Train Epoch: 16 [20480/60000 (34%)]\tLoss: 0.017462\n",
            "Train Epoch: 16 [23040/60000 (38%)]\tLoss: 0.000382\n",
            "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.023316\n",
            "Train Epoch: 16 [28160/60000 (47%)]\tLoss: 0.001373\n",
            "Train Epoch: 16 [30720/60000 (51%)]\tLoss: 0.000152\n",
            "Train Epoch: 16 [33280/60000 (55%)]\tLoss: 0.003947\n",
            "Train Epoch: 16 [35840/60000 (60%)]\tLoss: 0.008119\n",
            "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 0.000251\n",
            "Train Epoch: 16 [40960/60000 (68%)]\tLoss: 0.003458\n",
            "Train Epoch: 16 [43520/60000 (72%)]\tLoss: 0.006686\n",
            "Train Epoch: 16 [46080/60000 (77%)]\tLoss: 0.003639\n",
            "Train Epoch: 16 [48640/60000 (81%)]\tLoss: 0.003033\n",
            "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.010221\n",
            "Train Epoch: 16 [53760/60000 (89%)]\tLoss: 0.005691\n",
            "Train Epoch: 16 [56320/60000 (94%)]\tLoss: 0.001249\n",
            "Train Epoch: 16 [58880/60000 (98%)]\tLoss: 0.002877\n",
            "\n",
            "Test set: Average loss: 0.0343, Accuracy: 9915/10000 (99%)\n",
            "\n",
            "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.000621\n",
            "Train Epoch: 17 [2560/60000 (4%)]\tLoss: 0.003128\n",
            "Train Epoch: 17 [5120/60000 (9%)]\tLoss: 0.000339\n",
            "Train Epoch: 17 [7680/60000 (13%)]\tLoss: 0.001594\n",
            "Train Epoch: 17 [10240/60000 (17%)]\tLoss: 0.002055\n",
            "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 0.004339\n",
            "Train Epoch: 17 [15360/60000 (26%)]\tLoss: 0.000400\n",
            "Train Epoch: 17 [17920/60000 (30%)]\tLoss: 0.017488\n",
            "Train Epoch: 17 [20480/60000 (34%)]\tLoss: 0.001063\n",
            "Train Epoch: 17 [23040/60000 (38%)]\tLoss: 0.008124\n",
            "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.004782\n",
            "Train Epoch: 17 [28160/60000 (47%)]\tLoss: 0.000128\n",
            "Train Epoch: 17 [30720/60000 (51%)]\tLoss: 0.005804\n",
            "Train Epoch: 17 [33280/60000 (55%)]\tLoss: 0.004394\n",
            "Train Epoch: 17 [35840/60000 (60%)]\tLoss: 0.000047\n",
            "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 0.007217\n",
            "Train Epoch: 17 [40960/60000 (68%)]\tLoss: 0.000119\n",
            "Train Epoch: 17 [43520/60000 (72%)]\tLoss: 0.000651\n",
            "Train Epoch: 17 [46080/60000 (77%)]\tLoss: 0.000312\n",
            "Train Epoch: 17 [48640/60000 (81%)]\tLoss: 0.018159\n",
            "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.000159\n",
            "Train Epoch: 17 [53760/60000 (89%)]\tLoss: 0.002900\n",
            "Train Epoch: 17 [56320/60000 (94%)]\tLoss: 0.000249\n",
            "Train Epoch: 17 [58880/60000 (98%)]\tLoss: 0.000671\n",
            "\n",
            "Test set: Average loss: 0.0363, Accuracy: 9925/10000 (99%)\n",
            "\n",
            "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.000106\n",
            "Train Epoch: 18 [2560/60000 (4%)]\tLoss: 0.000861\n",
            "Train Epoch: 18 [5120/60000 (9%)]\tLoss: 0.016679\n",
            "Train Epoch: 18 [7680/60000 (13%)]\tLoss: 0.002198\n",
            "Train Epoch: 18 [10240/60000 (17%)]\tLoss: 0.000368\n",
            "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 0.023926\n",
            "Train Epoch: 18 [15360/60000 (26%)]\tLoss: 0.001439\n",
            "Train Epoch: 18 [17920/60000 (30%)]\tLoss: 0.009838\n",
            "Train Epoch: 18 [20480/60000 (34%)]\tLoss: 0.005922\n",
            "Train Epoch: 18 [23040/60000 (38%)]\tLoss: 0.005598\n",
            "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.022296\n",
            "Train Epoch: 18 [28160/60000 (47%)]\tLoss: 0.000160\n",
            "Train Epoch: 18 [30720/60000 (51%)]\tLoss: 0.001484\n",
            "Train Epoch: 18 [33280/60000 (55%)]\tLoss: 0.000325\n",
            "Train Epoch: 18 [35840/60000 (60%)]\tLoss: 0.004206\n",
            "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 0.000143\n",
            "Train Epoch: 18 [40960/60000 (68%)]\tLoss: 0.002741\n",
            "Train Epoch: 18 [43520/60000 (72%)]\tLoss: 0.000350\n",
            "Train Epoch: 18 [46080/60000 (77%)]\tLoss: 0.000731\n",
            "Train Epoch: 18 [48640/60000 (81%)]\tLoss: 0.000566\n",
            "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.001243\n",
            "Train Epoch: 18 [53760/60000 (89%)]\tLoss: 0.000894\n",
            "Train Epoch: 18 [56320/60000 (94%)]\tLoss: 0.008817\n",
            "Train Epoch: 18 [58880/60000 (98%)]\tLoss: 0.000369\n",
            "\n",
            "Test set: Average loss: 0.0366, Accuracy: 9919/10000 (99%)\n",
            "\n",
            "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.000801\n",
            "Train Epoch: 19 [2560/60000 (4%)]\tLoss: 0.017678\n",
            "Train Epoch: 19 [5120/60000 (9%)]\tLoss: 0.007760\n",
            "Train Epoch: 19 [7680/60000 (13%)]\tLoss: 0.000056\n",
            "Train Epoch: 19 [10240/60000 (17%)]\tLoss: 0.000319\n",
            "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 0.009214\n",
            "Train Epoch: 19 [15360/60000 (26%)]\tLoss: 0.001346\n",
            "Train Epoch: 19 [17920/60000 (30%)]\tLoss: 0.001199\n",
            "Train Epoch: 19 [20480/60000 (34%)]\tLoss: 0.001477\n",
            "Train Epoch: 19 [23040/60000 (38%)]\tLoss: 0.006657\n",
            "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.000122\n",
            "Train Epoch: 19 [28160/60000 (47%)]\tLoss: 0.008761\n",
            "Train Epoch: 19 [30720/60000 (51%)]\tLoss: 0.001081\n",
            "Train Epoch: 19 [33280/60000 (55%)]\tLoss: 0.000965\n",
            "Train Epoch: 19 [35840/60000 (60%)]\tLoss: 0.001515\n",
            "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 0.000071\n",
            "Train Epoch: 19 [40960/60000 (68%)]\tLoss: 0.000833\n",
            "Train Epoch: 19 [43520/60000 (72%)]\tLoss: 0.000578\n",
            "Train Epoch: 19 [46080/60000 (77%)]\tLoss: 0.010569\n",
            "Train Epoch: 19 [48640/60000 (81%)]\tLoss: 0.000115\n",
            "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.001038\n",
            "Train Epoch: 19 [53760/60000 (89%)]\tLoss: 0.001521\n",
            "Train Epoch: 19 [56320/60000 (94%)]\tLoss: 0.006733\n",
            "Train Epoch: 19 [58880/60000 (98%)]\tLoss: 0.000481\n",
            "\n",
            "Test set: Average loss: 0.0389, Accuracy: 9919/10000 (99%)\n",
            "\n",
            "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.000656\n",
            "Train Epoch: 20 [2560/60000 (4%)]\tLoss: 0.001905\n",
            "Train Epoch: 20 [5120/60000 (9%)]\tLoss: 0.004517\n",
            "Train Epoch: 20 [7680/60000 (13%)]\tLoss: 0.001898\n",
            "Train Epoch: 20 [10240/60000 (17%)]\tLoss: 0.000833\n",
            "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 0.004308\n",
            "Train Epoch: 20 [15360/60000 (26%)]\tLoss: 0.002255\n",
            "Train Epoch: 20 [17920/60000 (30%)]\tLoss: 0.001715\n",
            "Train Epoch: 20 [20480/60000 (34%)]\tLoss: 0.009648\n",
            "Train Epoch: 20 [23040/60000 (38%)]\tLoss: 0.007039\n",
            "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.000764\n",
            "Train Epoch: 20 [28160/60000 (47%)]\tLoss: 0.004432\n",
            "Train Epoch: 20 [30720/60000 (51%)]\tLoss: 0.001004\n",
            "Train Epoch: 20 [33280/60000 (55%)]\tLoss: 0.001121\n",
            "Train Epoch: 20 [35840/60000 (60%)]\tLoss: 0.000601\n",
            "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 0.000294\n",
            "Train Epoch: 20 [40960/60000 (68%)]\tLoss: 0.004034\n",
            "Train Epoch: 20 [43520/60000 (72%)]\tLoss: 0.000938\n",
            "Train Epoch: 20 [46080/60000 (77%)]\tLoss: 0.001244\n",
            "Train Epoch: 20 [48640/60000 (81%)]\tLoss: 0.000177\n",
            "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.000437\n",
            "Train Epoch: 20 [53760/60000 (89%)]\tLoss: 0.019520\n",
            "Train Epoch: 20 [56320/60000 (94%)]\tLoss: 0.001357\n",
            "Train Epoch: 20 [58880/60000 (98%)]\tLoss: 0.000773\n",
            "\n",
            "Test set: Average loss: 0.0388, Accuracy: 9911/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SUd2FLfgZGq"
      },
      "source": [
        "# **Batch Normalization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWz-PcS0QwlT"
      },
      "source": [
        "* BatchNorm2d: https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\n",
        "* BatchNorm1d: https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8xTryDKSMJH",
        "outputId": "1a1da8b5-54da-4662-bbaf-b972cb291f5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Without Learnable Parameters\n",
        "BatchNorm1d_layer = nn.BatchNorm1d(10, affine=False)\n",
        "# With Learnable Parameters\n",
        "BatchNorm1d_layer = nn.BatchNorm1d(10)\n",
        "input = torch.randn(20, 10)\n",
        "output = BatchNorm1d_layer(input)\n",
        "print(input)\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 6.5536e-02,  1.2142e+00,  4.9243e-01,  1.2853e+00,  4.6950e-01,\n",
            "          2.1183e+00,  2.4904e+00, -1.1194e+00,  5.9126e-01,  2.0532e-01],\n",
            "        [ 4.4115e-01,  3.7207e-01,  1.6566e-01, -2.0475e+00,  1.7869e+00,\n",
            "          3.5882e-01,  1.5658e+00, -1.7263e-01,  2.6234e-01, -1.1125e-01],\n",
            "        [ 1.3818e+00, -9.7413e-02, -3.8611e-01, -4.2693e-01, -2.3497e-01,\n",
            "         -5.8059e-01,  1.4155e+00,  1.1241e+00, -8.7430e-01, -1.3307e+00],\n",
            "        [ 1.1377e+00,  3.5718e-01,  1.4732e+00, -2.2605e-01,  5.4620e-01,\n",
            "         -3.1961e-01, -2.2507e-01, -1.5210e+00, -4.4925e-01, -2.3442e-01],\n",
            "        [ 3.4726e-01, -6.2492e-01,  5.8816e-01,  1.1174e+00,  1.0310e+00,\n",
            "          5.3202e-01,  2.3731e-01,  5.0414e-01, -4.2485e-01, -1.8069e+00],\n",
            "        [ 7.0126e-01, -1.2281e+00, -4.7729e-01,  5.2481e-01,  1.4993e+00,\n",
            "          5.6660e-01,  3.9388e-01,  1.8554e-01,  5.8759e-01,  1.9689e+00],\n",
            "        [-1.1635e+00,  6.4588e-01,  3.9377e-01, -5.4564e-01,  2.3864e+00,\n",
            "          4.6088e-01, -8.7527e-01, -8.7290e-01,  1.5425e-01,  3.5459e-01],\n",
            "        [ 1.9537e+00, -4.5789e-01,  1.4342e+00,  8.7260e-02,  8.2131e-01,\n",
            "          3.5787e-01,  6.4418e-01,  2.6948e-01, -1.0686e+00, -3.0092e-01],\n",
            "        [ 6.3367e-02, -4.0598e-02, -8.0229e-01,  1.1337e+00, -4.6432e-01,\n",
            "         -4.7029e-03,  1.0521e+00,  1.9039e+00, -3.6517e-01, -6.8402e-01],\n",
            "        [-1.2968e-01,  1.0286e+00, -1.6527e+00, -1.4464e+00, -8.9669e-01,\n",
            "         -5.8946e-01, -1.7561e+00, -5.8611e-01,  5.4563e-01, -1.2356e+00],\n",
            "        [ 1.9672e+00, -1.3242e+00, -7.8214e-01, -4.7700e-01, -1.4935e+00,\n",
            "          2.5329e+00,  6.0756e-01, -1.2156e+00,  1.0946e+00, -5.8701e-01],\n",
            "        [-1.1808e-01,  4.5325e-02, -7.1873e-01, -1.7084e+00, -4.7446e-01,\n",
            "          1.7663e-01, -1.6746e-02,  1.2856e+00, -2.5425e-01, -1.1357e-03],\n",
            "        [-1.2397e+00, -5.3851e-01, -9.1687e-01,  2.5465e-01,  1.7476e+00,\n",
            "         -2.6674e-01, -4.9295e-01, -5.0914e-01, -7.0208e-01,  6.4329e-01],\n",
            "        [-2.1603e+00, -9.4931e-01, -5.1874e-01,  2.5355e-01, -4.8990e-02,\n",
            "         -1.4073e-01, -8.4831e-01,  7.8827e-01, -3.9812e-01,  1.1176e+00],\n",
            "        [ 1.1627e+00, -7.2117e-01, -3.7654e-01, -7.4781e-01,  7.7163e-03,\n",
            "         -2.8365e-01, -4.9450e-01,  1.4932e+00, -1.0212e+00,  1.1161e+00],\n",
            "        [ 1.5603e+00,  1.6831e+00, -5.0095e-01,  1.3903e+00,  2.0063e+00,\n",
            "         -7.8599e-01, -3.0195e-01,  1.9133e+00,  8.0497e-01, -1.6926e-01],\n",
            "        [-1.5221e+00,  1.1354e+00, -3.1559e+00, -5.6954e-01, -7.8872e-01,\n",
            "          1.1384e+00, -4.8372e-01,  1.6573e+00, -1.7018e+00,  1.8752e-01],\n",
            "        [ 8.6151e-02, -1.7511e-01, -7.7037e-01, -5.6124e-01, -1.5785e-01,\n",
            "          7.2778e-01, -3.8951e-01,  6.1730e-01, -2.9456e-01,  1.1489e+00],\n",
            "        [ 3.0131e-01, -4.6700e-01, -1.1060e+00, -2.5486e+00, -7.1125e-02,\n",
            "          1.0271e+00,  8.6082e-02, -3.2970e-01, -8.6301e-01, -5.6984e-01],\n",
            "        [ 1.9063e+00, -1.0915e-01, -1.4614e-01, -2.7914e-01,  8.7359e-01,\n",
            "         -5.5809e-02,  5.4236e-01, -4.5282e-01,  4.4644e-01,  6.4984e-01]])\n",
            "tensor([[-2.3436e-01,  1.5112e+00,  8.7697e-01,  1.4653e+00,  4.0494e-02,\n",
            "          2.1124e+00,  2.4488e+00, -1.3020e+00,  1.0995e+00,  2.0550e-01],\n",
            "        [ 8.9775e-02,  4.7381e-01,  5.5155e-01, -1.6609e+00,  1.3033e+00,\n",
            "          1.2315e-02,  1.4783e+00, -4.0061e-01,  6.4041e-01, -1.4188e-01],\n",
            "        [ 9.0149e-01, -1.0449e-01,  2.0499e-03, -1.4076e-01, -6.3475e-01,\n",
            "         -1.1090e+00,  1.3205e+00,  8.3400e-01, -9.4600e-01, -1.4800e+00],\n",
            "        [ 6.9085e-01,  4.5546e-01,  1.8537e+00,  4.7659e-02,  1.1401e-01,\n",
            "         -7.9746e-01, -4.0165e-01, -1.6844e+00, -3.5276e-01, -2.7704e-01],\n",
            "        [ 8.7571e-03, -7.5427e-01,  9.7232e-01,  1.3078e+00,  5.7866e-01,\n",
            "          2.1906e-01,  8.3729e-02,  2.4374e-01, -3.1871e-01, -2.0025e+00],\n",
            "        [ 3.1424e-01, -1.4973e+00, -8.8754e-02,  7.5196e-01,  1.0275e+00,\n",
            "          2.6033e-01,  2.4808e-01, -5.9603e-02,  1.0944e+00,  2.1407e+00],\n",
            "        [-1.2950e+00,  8.1108e-01,  7.7873e-01, -2.5212e-01,  1.8779e+00,\n",
            "          1.3414e-01, -1.0842e+00, -1.0673e+00,  4.8955e-01,  3.6930e-01],\n",
            "        [ 1.3950e+00, -5.4852e-01,  1.8149e+00,  3.4154e-01,  3.7770e-01,\n",
            "          1.1186e-02,  5.1082e-01,  2.0322e-02, -1.2171e+00, -3.5001e-01],\n",
            "        [-2.3623e-01, -3.4510e-02, -4.1242e-01,  1.3231e+00, -8.5458e-01,\n",
            "         -4.2159e-01,  9.3907e-01,  1.5765e+00, -2.3540e-01, -7.7040e-01],\n",
            "        [-4.0282e-01,  1.2825e+00, -1.2593e+00, -1.0970e+00, -1.2690e+00,\n",
            "         -1.1196e+00, -2.0088e+00, -7.9428e-01,  1.0358e+00, -1.3756e+00],\n",
            "        [ 1.4067e+00, -1.6156e+00, -3.9235e-01, -1.8773e-01, -1.8411e+00,\n",
            "          2.6074e+00,  4.7238e-01, -1.3936e+00,  1.8020e+00, -6.6394e-01],\n",
            "        [-3.9282e-01,  7.1329e-02, -3.2920e-01, -1.3428e+00, -8.6430e-01,\n",
            "         -2.0514e-01, -1.8297e-01,  9.8778e-01, -8.0592e-02, -2.1048e-02],\n",
            "        [-1.3607e+00, -6.4783e-01, -5.2652e-01,  4.9855e-01,  1.2656e+00,\n",
            "         -7.3435e-01, -6.8285e-01, -7.2101e-01, -7.0564e-01,  6.8609e-01],\n",
            "        [-2.1551e+00, -1.1538e+00, -1.3004e-01,  4.9752e-01, -4.5649e-01,\n",
            "         -5.8394e-01, -1.0559e+00,  5.1426e-01, -2.8139e-01,  1.2065e+00],\n",
            "        [ 7.1248e-01, -8.7282e-01,  1.1587e-02, -4.4175e-01, -4.0213e-01,\n",
            "         -7.5454e-01, -6.8447e-01,  1.1854e+00, -1.1511e+00,  1.2049e+00],\n",
            "        [ 1.0556e+00,  2.0887e+00, -1.1231e-01,  1.5638e+00,  1.5135e+00,\n",
            "         -1.3541e+00, -4.8235e-01,  1.5854e+00,  1.3978e+00, -2.0554e-01],\n",
            "        [-1.6045e+00,  1.4140e+00, -2.7564e+00, -2.7453e-01, -1.1655e+00,\n",
            "          9.4278e-01, -6.7317e-01,  1.3416e+00, -2.1009e+00,  1.8597e-01],\n",
            "        [-2.1657e-01, -2.0020e-01, -3.8062e-01, -2.6674e-01, -5.6083e-01,\n",
            "          4.5272e-01, -5.7426e-01,  3.5148e-01, -1.3686e-01,  1.2409e+00],\n",
            "        [-3.0903e-02, -5.5975e-01, -7.1491e-01, -2.1309e+00, -4.7770e-01,\n",
            "          8.0995e-01, -7.5023e-02, -5.5015e-01, -9.3025e-01, -6.4510e-01],\n",
            "        [ 1.3542e+00, -1.1895e-01,  2.4103e-01, -2.1399e-03,  4.2781e-01,\n",
            "         -4.8259e-01,  4.0395e-01, -6.6739e-01,  8.9736e-01,  6.9329e-01]],\n",
            "       grad_fn=<NativeBatchNormBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzSQpzFPRQND",
        "outputId": "ae5aa109-2fb8-43d7-9609-c29612e5ff8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Without Learnable Parameters\n",
        "BatchNorm2d_layer = nn.BatchNorm2d(10, affine=False)\n",
        "# With Learnable Parameters\n",
        "BatchNorm2d_layer = nn.BatchNorm2d(10)\n",
        "input = torch.randn(20, 10, 35, 45)\n",
        "output = BatchNorm2d_layer(input)\n",
        "print(input)\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[ 3.2498e+00, -1.1361e+00, -1.1437e+00,  ..., -1.5524e-01,\n",
            "           -1.2541e+00,  2.3745e-01],\n",
            "          [-1.8886e-01,  1.0084e+00,  1.1396e+00,  ...,  1.9136e+00,\n",
            "            2.6322e-02,  1.0527e+00],\n",
            "          [ 6.4040e-01,  7.0705e-01,  8.4769e-02,  ..., -6.8769e-01,\n",
            "            2.4693e-01, -2.0326e-01],\n",
            "          ...,\n",
            "          [ 1.8197e+00, -4.5865e-01,  1.7775e+00,  ..., -1.9022e-01,\n",
            "           -1.0356e-01, -2.0211e-01],\n",
            "          [-9.5955e-01, -2.6303e-01,  4.4190e-01,  ..., -2.0048e+00,\n",
            "           -9.1773e-01,  1.8528e+00],\n",
            "          [ 2.0193e+00,  2.2863e-01,  3.6503e-01,  ..., -1.4027e+00,\n",
            "           -7.3854e-01, -8.8414e-01]],\n",
            "\n",
            "         [[-1.3142e+00, -4.2268e-01,  3.2736e+00,  ..., -3.4826e-01,\n",
            "            1.2777e+00, -3.6433e+00],\n",
            "          [-8.1349e-01, -5.1899e-01,  3.0408e-01,  ...,  1.4168e+00,\n",
            "            4.9975e-02,  3.7498e-02],\n",
            "          [ 4.8382e-01, -7.7343e-02, -5.5220e-02,  ...,  5.5978e-01,\n",
            "            8.6317e-01, -5.6798e-01],\n",
            "          ...,\n",
            "          [-4.3502e-01,  1.0795e+00, -2.7452e+00,  ..., -1.9961e-01,\n",
            "           -6.3600e-01,  1.4283e+00],\n",
            "          [ 3.8515e-01,  6.6858e-02,  9.8094e-01,  ..., -5.2657e-01,\n",
            "            5.1668e-01,  5.1726e-01],\n",
            "          [ 9.4510e-01,  2.3854e-01,  5.5164e-01,  ...,  1.7185e-02,\n",
            "            5.3263e-01, -8.4757e-01]],\n",
            "\n",
            "         [[-1.9404e-01,  2.7702e-01,  1.4052e+00,  ...,  8.9155e-02,\n",
            "            7.0243e-01,  1.4864e-02],\n",
            "          [-6.8489e-02,  7.5443e-01,  1.0357e+00,  ..., -4.5829e-01,\n",
            "            3.9207e-01,  4.4187e-01],\n",
            "          [ 8.6975e-01, -2.6096e-01,  3.5474e-01,  ...,  1.4564e+00,\n",
            "            7.1966e-01,  8.4278e-02],\n",
            "          ...,\n",
            "          [-6.5788e-01, -9.1249e-01, -1.8888e+00,  ...,  1.5930e+00,\n",
            "            7.8425e-01,  8.9358e-02],\n",
            "          [ 2.5015e+00, -5.2740e-01,  6.8437e-01,  ...,  4.0258e-03,\n",
            "           -3.8461e-01,  1.4359e+00],\n",
            "          [-8.8555e-01,  3.0584e-01,  5.0441e-01,  ..., -1.8545e-01,\n",
            "           -3.2300e-01, -4.6528e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.6332e+00, -1.7683e+00,  2.2901e-01,  ..., -3.0849e-01,\n",
            "            9.7977e-01, -1.6196e-01],\n",
            "          [-8.1972e-01, -1.9855e-01,  1.1320e+00,  ...,  7.7373e-02,\n",
            "            7.1182e-01, -1.4125e-01],\n",
            "          [-1.4865e+00,  9.7887e-01, -1.3370e-01,  ...,  2.1544e+00,\n",
            "           -8.3486e-01, -1.5974e+00],\n",
            "          ...,\n",
            "          [ 1.3259e+00,  1.4544e+00,  1.9710e+00,  ...,  5.4099e-02,\n",
            "           -1.5803e+00, -2.5768e-01],\n",
            "          [-2.8560e-01,  1.7666e+00,  1.4347e+00,  ...,  8.5181e-02,\n",
            "            1.3839e+00,  7.1546e-01],\n",
            "          [-2.1457e+00,  4.0509e-01,  1.6658e+00,  ...,  5.9802e-01,\n",
            "           -1.9498e-01,  1.1176e-01]],\n",
            "\n",
            "         [[ 9.0620e-01,  1.6413e+00, -9.9141e-01,  ..., -1.5318e+00,\n",
            "           -7.9698e-02,  5.2903e-01],\n",
            "          [-1.1043e+00, -2.9003e+00,  1.4461e+00,  ..., -1.3360e+00,\n",
            "           -9.2115e-01,  5.7463e-02],\n",
            "          [-1.3024e-01, -7.6902e-01, -5.6202e-01,  ..., -5.6754e-02,\n",
            "            7.9593e-01, -8.5014e-01],\n",
            "          ...,\n",
            "          [ 6.3793e-02, -1.2584e-01,  1.6771e+00,  ...,  8.6691e-01,\n",
            "            4.7341e-01, -1.3567e+00],\n",
            "          [-2.0748e-01,  6.7043e-01, -9.6239e-02,  ...,  1.1196e+00,\n",
            "           -6.5254e-01,  6.8170e-01],\n",
            "          [-1.2054e+00,  2.0607e+00,  8.3571e-01,  ..., -9.1332e-01,\n",
            "            1.2676e+00,  6.6073e-01]],\n",
            "\n",
            "         [[ 1.3361e+00, -7.2501e-01,  9.5843e-01,  ..., -4.1948e-01,\n",
            "            2.1214e+00,  1.6125e+00],\n",
            "          [ 1.9086e-01,  1.1311e+00, -6.3753e-01,  ...,  4.2058e-01,\n",
            "            3.9898e-01, -7.3418e-01],\n",
            "          [ 4.5863e-01,  5.6193e-01, -1.2223e+00,  ...,  1.9775e-01,\n",
            "           -3.3188e-01, -7.8956e-01],\n",
            "          ...,\n",
            "          [ 5.2107e-01, -1.4515e+00, -1.2443e+00,  ...,  7.5684e-01,\n",
            "           -1.0817e-01, -6.1581e-02],\n",
            "          [ 1.2059e-01,  1.9155e-01,  1.3143e+00,  ..., -1.6960e+00,\n",
            "           -8.1372e-01, -2.4829e-01],\n",
            "          [ 1.6411e+00, -4.5818e-03, -8.7878e-01,  ...,  1.3940e+00,\n",
            "           -1.1600e+00,  1.5576e+00]]],\n",
            "\n",
            "\n",
            "        [[[-1.2263e+00,  8.2337e-01,  9.8404e-01,  ..., -7.8010e-01,\n",
            "           -9.1092e-01,  1.8790e-01],\n",
            "          [-5.7768e-01, -1.2889e+00, -5.6513e-01,  ...,  1.6252e+00,\n",
            "           -1.0202e+00,  2.0798e-01],\n",
            "          [-3.1568e-01,  3.4153e-01,  1.2555e+00,  ...,  2.1708e+00,\n",
            "            2.7257e-01,  5.4000e-01],\n",
            "          ...,\n",
            "          [-4.3740e-01, -9.0226e-01,  3.0111e-02,  ...,  3.9739e-01,\n",
            "            3.9012e-01,  3.6491e-01],\n",
            "          [-2.1419e+00, -1.4111e-01, -5.4030e-01,  ..., -4.0765e-01,\n",
            "           -4.6066e-01,  1.7362e+00],\n",
            "          [ 1.8245e-01, -8.1001e-02, -1.4899e-01,  ..., -5.7374e-01,\n",
            "           -2.8591e-01,  8.0135e-01]],\n",
            "\n",
            "         [[-1.0109e+00,  1.9948e+00, -7.3477e-01,  ..., -4.8996e-01,\n",
            "            9.9969e-02, -1.6321e-01],\n",
            "          [-4.4319e-01,  6.4492e-01,  7.4073e-01,  ..., -3.2248e-01,\n",
            "           -9.6977e-01,  2.7327e-01],\n",
            "          [ 1.6576e+00,  1.5273e+00,  1.3003e-01,  ...,  1.5376e+00,\n",
            "            2.6001e-01, -5.5333e-01],\n",
            "          ...,\n",
            "          [ 1.8938e+00, -3.4827e-03,  1.0241e-01,  ..., -3.0284e+00,\n",
            "           -9.0394e-01, -1.3948e+00],\n",
            "          [ 4.2813e-01,  5.8494e-01,  8.6816e-01,  ..., -8.5534e-02,\n",
            "            1.7159e+00, -5.2989e-01],\n",
            "          [-5.7560e-01,  1.0479e+00,  5.7050e-01,  ..., -1.3637e+00,\n",
            "           -1.5930e+00, -1.0652e+00]],\n",
            "\n",
            "         [[ 8.6677e-02,  1.0992e+00,  1.4630e+00,  ...,  7.1745e-01,\n",
            "            1.4972e+00,  6.7029e-01],\n",
            "          [-2.3985e-01, -1.0919e+00,  4.1722e-01,  ..., -3.4372e-01,\n",
            "           -1.1483e-02, -1.1801e+00],\n",
            "          [-6.4833e-01,  2.0782e-01, -2.0406e+00,  ..., -2.9205e-01,\n",
            "           -4.6099e-01,  8.4554e-01],\n",
            "          ...,\n",
            "          [-2.8231e-01, -7.3690e-02, -1.9898e+00,  ...,  1.4955e+00,\n",
            "           -1.9755e-01, -4.5374e-01],\n",
            "          [ 2.5293e-01, -9.3706e-01, -1.6619e+00,  ..., -1.1216e+00,\n",
            "           -2.6565e+00, -2.9831e-01],\n",
            "          [ 6.9593e-01,  1.7472e-01, -2.1561e-01,  ..., -1.0072e+00,\n",
            "            9.4745e-01, -2.6041e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.1903e+00,  9.8872e-01,  6.4368e-01,  ...,  1.3591e-01,\n",
            "           -4.3189e-01, -7.9425e-01],\n",
            "          [-4.7860e-02, -3.8325e-01,  7.4855e-01,  ..., -5.7564e-01,\n",
            "           -4.3591e-01,  4.0289e-01],\n",
            "          [-6.0646e-01,  2.3535e-01, -6.2894e-01,  ..., -2.2939e-01,\n",
            "            5.5254e-01,  3.7505e-01],\n",
            "          ...,\n",
            "          [ 6.3997e-01, -1.8684e+00, -6.5514e-01,  ..., -9.9075e-02,\n",
            "            5.5370e-02, -2.1836e-01],\n",
            "          [-3.0359e-01, -9.7533e-02, -1.1811e+00,  ...,  1.0660e+00,\n",
            "           -1.9786e-01,  1.2600e+00],\n",
            "          [ 7.2178e-01,  1.9341e-02, -9.5963e-01,  ..., -3.0031e-01,\n",
            "            1.8895e+00,  1.4956e+00]],\n",
            "\n",
            "         [[-4.3640e-01,  7.2289e-02,  1.1875e-01,  ...,  3.1283e-01,\n",
            "           -4.6433e-01,  6.0524e-01],\n",
            "          [-1.2154e-01, -4.2100e-01, -6.6648e-01,  ..., -2.8969e-01,\n",
            "            1.1547e+00,  1.8704e-01],\n",
            "          [-1.2430e+00,  1.0202e+00,  7.3685e-01,  ...,  1.2022e+00,\n",
            "            5.1535e-01, -9.7480e-01],\n",
            "          ...,\n",
            "          [ 8.9026e-01, -1.2281e-01,  3.5439e-01,  ...,  6.7048e-01,\n",
            "           -3.3307e-01,  1.0515e+00],\n",
            "          [-1.5313e+00, -9.4999e-01, -1.5425e+00,  ...,  1.7996e+00,\n",
            "           -5.0319e-02,  7.3512e-01],\n",
            "          [ 1.1148e+00,  3.3952e-01, -1.2365e+00,  ..., -1.0289e+00,\n",
            "            1.1648e+00, -4.1847e-01]],\n",
            "\n",
            "         [[ 3.0559e-02,  9.1841e-02, -1.1520e+00,  ..., -1.0951e+00,\n",
            "           -4.8390e-01, -4.2778e-01],\n",
            "          [ 8.3836e-01, -2.2109e+00, -5.1219e-01,  ..., -4.3211e-01,\n",
            "            2.6443e-01, -1.0050e+00],\n",
            "          [ 1.9896e-02,  3.9431e-02,  1.8440e+00,  ...,  1.8961e-01,\n",
            "           -8.0886e-01, -6.0512e-03],\n",
            "          ...,\n",
            "          [-4.2816e-01,  2.1990e-01,  1.5700e+00,  ...,  5.1907e-02,\n",
            "            1.4553e+00, -3.3085e-01],\n",
            "          [ 1.7615e+00,  3.1668e-01, -1.4083e-01,  ...,  1.1535e+00,\n",
            "            2.8038e-01,  1.9380e-01],\n",
            "          [ 1.1695e-02, -2.9011e-01,  2.1089e+00,  ...,  1.1848e+00,\n",
            "           -1.2525e-01,  5.6995e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 2.4197e-01,  2.2533e+00,  1.0559e+00,  ...,  1.2107e+00,\n",
            "            9.3398e-01,  3.9849e-01],\n",
            "          [-5.7890e-01,  9.4577e-01,  1.4186e+00,  ...,  4.8812e-01,\n",
            "           -8.0466e-01,  1.0326e-01],\n",
            "          [-1.1220e-01, -1.4541e+00, -4.2430e-01,  ...,  7.9017e-02,\n",
            "           -6.8613e-01,  1.1600e+00],\n",
            "          ...,\n",
            "          [ 5.3697e-01,  8.3555e-01,  4.0060e-01,  ...,  3.7946e-01,\n",
            "           -7.6155e-01,  1.6260e+00],\n",
            "          [ 1.5251e+00,  3.8907e-01, -2.6883e-03,  ..., -1.7558e-01,\n",
            "            7.4260e-01,  9.5043e-01],\n",
            "          [-1.9737e-02, -1.7356e+00, -1.2075e-01,  ..., -6.3574e-01,\n",
            "           -3.5331e-02,  3.2758e-01]],\n",
            "\n",
            "         [[-6.6614e-01, -1.0892e-01, -1.7176e-01,  ...,  5.2786e-01,\n",
            "            6.6283e-01,  5.1629e-01],\n",
            "          [-3.1663e-01, -6.5814e-01,  6.7146e-01,  ...,  5.6764e-02,\n",
            "           -5.5828e-01,  1.0914e+00],\n",
            "          [ 1.1396e+00,  8.8336e-01,  1.1276e+00,  ..., -2.9600e+00,\n",
            "            1.0648e+00, -5.1989e-03],\n",
            "          ...,\n",
            "          [-1.0728e+00,  5.9063e-01, -1.4957e+00,  ...,  4.1268e-01,\n",
            "            1.6160e-01, -9.3790e-01],\n",
            "          [ 9.4996e-02, -1.3333e+00, -1.7376e+00,  ...,  1.2544e+00,\n",
            "           -1.0555e+00,  2.0664e-01],\n",
            "          [-1.3028e-01, -3.2512e-01, -2.5852e+00,  ..., -1.1432e+00,\n",
            "            1.3151e+00,  5.3509e-01]],\n",
            "\n",
            "         [[-1.5896e+00,  1.7926e+00, -2.3535e+00,  ...,  6.0118e-01,\n",
            "            2.0429e-01,  1.7691e+00],\n",
            "          [-7.8045e-01, -1.2195e+00, -3.4552e-01,  ..., -1.7139e+00,\n",
            "           -2.4840e+00,  1.3533e+00],\n",
            "          [ 4.8853e-01, -8.9458e-01,  1.1939e+00,  ..., -8.1897e-01,\n",
            "            3.1662e-01, -3.8422e-01],\n",
            "          ...,\n",
            "          [-1.2009e-01,  1.0788e+00,  3.6965e-01,  ...,  2.8528e-01,\n",
            "           -3.8985e-04, -1.3474e+00],\n",
            "          [ 6.3744e-01, -1.1864e+00,  1.6285e+00,  ...,  1.6669e+00,\n",
            "            6.6288e-01,  7.9806e-01],\n",
            "          [ 2.6443e-01,  2.2878e-01, -1.5184e-01,  ..., -3.9174e-01,\n",
            "           -1.2388e+00, -1.6466e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.3455e-01,  9.8817e-01, -5.7582e-01,  ..., -1.3235e+00,\n",
            "           -3.3076e-01,  1.2582e+00],\n",
            "          [ 7.0887e-01, -2.2928e+00, -1.5041e+00,  ...,  6.9564e-01,\n",
            "           -6.4035e-01,  3.4742e-01],\n",
            "          [ 1.5580e+00, -1.2470e-01, -9.6938e-01,  ..., -1.6952e-01,\n",
            "           -1.2420e+00,  9.3921e-02],\n",
            "          ...,\n",
            "          [ 6.5070e-01, -1.6022e+00,  1.0954e+00,  ...,  3.3396e-01,\n",
            "            1.2035e+00,  2.0226e-01],\n",
            "          [ 2.5734e-01, -6.4543e-01, -4.9527e-01,  ..., -4.9629e-01,\n",
            "           -9.2186e-01, -2.3754e+00],\n",
            "          [-2.5199e-01, -5.8611e-01,  4.6246e-01,  ...,  3.7750e-01,\n",
            "            2.4466e+00, -2.0475e-01]],\n",
            "\n",
            "         [[-1.1974e-01, -4.3693e-01,  1.1774e-01,  ..., -1.2115e-02,\n",
            "           -9.4766e-02,  3.5634e-01],\n",
            "          [-8.6119e-01,  7.4751e-01,  4.8740e-01,  ..., -7.9104e-01,\n",
            "           -1.5089e+00,  1.0738e+00],\n",
            "          [ 1.2130e+00,  3.6918e-01, -6.9622e-02,  ...,  6.0812e-01,\n",
            "           -1.8760e-01, -9.6137e-01],\n",
            "          ...,\n",
            "          [-2.0405e-01, -5.1849e-01,  1.2553e+00,  ...,  2.0441e+00,\n",
            "            1.9454e+00, -1.3621e-01],\n",
            "          [-2.1828e+00, -9.1634e-01, -3.0410e-01,  ..., -7.9966e-01,\n",
            "            6.8476e-01, -5.0134e-01],\n",
            "          [-3.1969e-01,  8.5019e-01, -3.0519e-01,  ..., -7.9367e-01,\n",
            "           -7.7717e-02,  1.9136e-02]],\n",
            "\n",
            "         [[ 7.1076e-01,  7.9922e-01,  1.2477e-01,  ..., -1.5213e-01,\n",
            "            7.9168e-01,  2.3750e+00],\n",
            "          [ 9.0750e-01, -2.2423e-01,  7.0303e-01,  ..., -1.5681e+00,\n",
            "            4.4969e-01, -6.3567e-01],\n",
            "          [ 1.0110e+00, -8.0923e-01, -1.7039e+00,  ...,  6.6848e-01,\n",
            "           -3.3785e-01, -7.0555e-01],\n",
            "          ...,\n",
            "          [ 8.1799e-01, -1.6216e+00,  1.9062e+00,  ...,  1.2794e+00,\n",
            "           -6.6931e-01,  1.2353e+00],\n",
            "          [ 7.4636e-01, -1.7847e-01, -7.0999e-01,  ...,  1.8862e+00,\n",
            "            1.0655e+00, -6.1112e-01],\n",
            "          [ 4.3985e-01,  8.8091e-01, -7.2391e-01,  ..., -1.0296e+00,\n",
            "            3.5980e-01,  2.4257e-01]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1.0692e+00, -4.7146e-01, -4.0104e-01,  ..., -1.7347e+00,\n",
            "            7.6303e-01,  1.3726e+00],\n",
            "          [ 8.8856e-02,  1.0586e-02,  1.6715e-01,  ..., -2.8912e-01,\n",
            "           -2.9242e-01, -2.6667e+00],\n",
            "          [-1.4337e+00,  1.1583e+00, -1.3778e+00,  ...,  4.0041e-01,\n",
            "            1.7412e-01, -7.1235e-01],\n",
            "          ...,\n",
            "          [-1.9149e+00, -1.0726e+00,  5.8905e-01,  ..., -1.2260e+00,\n",
            "           -1.1298e-01, -8.3246e-01],\n",
            "          [ 9.0130e-02, -9.1144e-01,  3.8518e-01,  ...,  1.7512e+00,\n",
            "           -4.1214e-01, -1.4220e-01],\n",
            "          [ 1.6639e+00, -3.3702e+00, -1.3516e+00,  ..., -6.7678e-02,\n",
            "           -4.3344e-01,  7.4378e-01]],\n",
            "\n",
            "         [[-1.2542e-01,  1.2001e-01,  1.2811e-01,  ...,  1.6143e+00,\n",
            "            2.2905e+00, -4.4896e-01],\n",
            "          [ 2.9118e-01,  1.4378e+00, -5.3425e-01,  ...,  9.3751e-01,\n",
            "            9.9798e-01, -1.4008e+00],\n",
            "          [-1.2398e+00,  6.1168e-01,  9.7392e-01,  ..., -6.5179e-01,\n",
            "           -4.3537e-01, -1.0850e-01],\n",
            "          ...,\n",
            "          [ 3.8533e-01,  1.0842e+00, -1.8836e-01,  ...,  9.0679e-01,\n",
            "           -7.9977e-01,  6.1405e-01],\n",
            "          [-4.8143e-01,  2.3187e-01, -4.5527e-01,  ..., -1.4515e+00,\n",
            "            2.6861e-01,  8.4570e-01],\n",
            "          [ 4.3301e-01,  5.2230e-01, -9.7139e-01,  ..., -5.8664e-01,\n",
            "            7.3116e-01,  5.3345e-01]],\n",
            "\n",
            "         [[ 1.1756e+00, -3.8523e-01, -6.4191e-01,  ..., -1.6751e-01,\n",
            "           -8.4619e-01,  2.5170e-01],\n",
            "          [ 5.1427e-02,  2.2388e+00, -1.0355e+00,  ..., -2.0302e+00,\n",
            "            3.1226e-01, -7.6095e-02],\n",
            "          [-7.5429e-01,  6.3975e-01,  2.3148e-01,  ...,  1.5091e+00,\n",
            "            6.4106e-01, -1.1996e+00],\n",
            "          ...,\n",
            "          [-2.3102e+00, -1.3903e-01, -1.1627e+00,  ..., -5.4751e-02,\n",
            "            1.0868e-01,  1.1127e+00],\n",
            "          [ 4.3418e-01,  1.9238e+00, -1.5645e+00,  ..., -1.2868e+00,\n",
            "           -5.1422e-01,  2.4025e-01],\n",
            "          [-8.6950e-03,  7.3226e-01, -7.4060e-01,  ..., -5.8668e-01,\n",
            "            1.8893e-01, -1.3201e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.3370e+00, -7.0352e-01,  1.2603e-01,  ...,  1.9320e-01,\n",
            "           -8.0599e-01,  9.2323e-02],\n",
            "          [ 8.6924e-01,  2.3629e-01,  9.5779e-04,  ...,  2.7270e-01,\n",
            "            5.2418e-02,  2.5199e-01],\n",
            "          [-1.5426e-01, -9.2265e-02,  1.1343e+00,  ...,  1.0036e+00,\n",
            "            1.6988e+00, -1.2512e+00],\n",
            "          ...,\n",
            "          [-1.0546e-01,  1.9720e-02, -1.4886e+00,  ..., -2.5676e-01,\n",
            "            1.0409e+00, -8.7001e-01],\n",
            "          [-6.0548e-01, -1.6365e+00,  3.2571e-01,  ...,  1.2231e-01,\n",
            "           -6.1867e-02,  1.4547e+00],\n",
            "          [ 1.6926e+00,  1.0087e+00,  2.9251e-01,  ...,  4.4949e-01,\n",
            "           -5.6123e-01,  1.0304e+00]],\n",
            "\n",
            "         [[ 8.6007e-01,  2.7644e-01,  1.9293e-01,  ..., -2.4696e+00,\n",
            "           -4.1385e-01,  7.6351e-01],\n",
            "          [-4.4464e-01,  6.2771e-01, -2.1997e+00,  ...,  8.4576e-01,\n",
            "            2.6534e-01,  2.0545e+00],\n",
            "          [-8.4874e-02,  9.3691e-01, -3.6129e-01,  ..., -7.0453e-01,\n",
            "           -5.6210e-01, -1.9112e+00],\n",
            "          ...,\n",
            "          [ 4.4272e-01, -1.1586e+00,  1.9462e+00,  ..., -7.1039e-01,\n",
            "            7.3129e-02,  1.5206e-01],\n",
            "          [ 1.5430e+00,  1.2676e+00,  6.8583e-01,  ..., -1.5419e+00,\n",
            "            3.0195e-01, -1.5803e+00],\n",
            "          [ 3.6187e-01, -7.0027e-01, -1.7851e-01,  ...,  1.0361e+00,\n",
            "           -8.8058e-01, -1.0549e+00]],\n",
            "\n",
            "         [[-1.3996e+00, -1.7751e-01,  1.2279e-01,  ..., -1.6031e+00,\n",
            "            6.3649e-01,  7.0877e-01],\n",
            "          [-1.1448e+00,  1.3411e+00,  3.7952e-02,  ...,  1.5583e+00,\n",
            "            3.5547e-02,  3.7273e-01],\n",
            "          [-8.7509e-01,  2.9161e+00, -2.8204e-01,  ...,  1.3324e+00,\n",
            "            6.2894e-01, -8.4729e-01],\n",
            "          ...,\n",
            "          [-9.3451e-01, -1.5374e+00,  1.8993e+00,  ..., -4.0226e-01,\n",
            "            3.3677e-01, -1.0862e-01],\n",
            "          [ 1.5973e+00, -3.8797e-01,  1.6598e+00,  ..., -2.9855e-01,\n",
            "            7.9280e-01,  1.0930e-01],\n",
            "          [ 4.7906e-01, -4.7336e-02,  5.2085e-01,  ...,  6.9678e-01,\n",
            "           -5.7662e-01, -2.8383e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 4.7428e-01, -6.0481e-01,  8.9728e-01,  ...,  5.1108e-01,\n",
            "           -4.3269e-01,  4.9581e-01],\n",
            "          [-7.3123e-01,  9.0022e-01, -5.8169e-01,  ...,  2.3830e-01,\n",
            "           -3.6335e-01,  6.4895e-01],\n",
            "          [ 1.9861e-01, -2.1431e-01, -1.5717e+00,  ..., -6.0413e-01,\n",
            "           -2.2426e+00, -8.8239e-02],\n",
            "          ...,\n",
            "          [-1.9337e+00,  1.6824e+00,  4.5277e-01,  ..., -2.0550e-01,\n",
            "           -3.0646e-01,  1.0289e+00],\n",
            "          [ 1.1593e+00,  6.8724e-01,  7.7930e-01,  ...,  1.0942e+00,\n",
            "           -2.4901e+00, -2.3201e-01],\n",
            "          [-8.6292e-02, -1.1052e+00, -4.8619e-01,  ..., -3.3077e-01,\n",
            "           -2.5814e+00,  1.0927e+00]],\n",
            "\n",
            "         [[-6.1215e-01,  1.1894e+00,  1.0427e-02,  ...,  1.7235e-01,\n",
            "           -8.1168e-01, -3.8894e-01],\n",
            "          [-1.5438e+00,  1.9349e+00, -2.3242e+00,  ..., -5.1739e-02,\n",
            "            1.5512e-01, -2.3929e+00],\n",
            "          [-3.3244e-01, -5.0912e-01, -3.7000e-02,  ...,  9.9613e-01,\n",
            "            6.5762e-04,  9.1423e-01],\n",
            "          ...,\n",
            "          [ 1.0051e+00, -1.7977e-02,  1.5011e+00,  ...,  3.2492e-01,\n",
            "           -6.6201e-01,  2.2043e-01],\n",
            "          [ 2.3004e-01,  3.5845e-01,  7.8902e-01,  ...,  1.7823e+00,\n",
            "            1.4672e+00,  1.1557e+00],\n",
            "          [ 6.2262e-01,  1.0965e-02, -6.5136e-01,  ...,  4.7504e-01,\n",
            "            4.7106e-01, -1.2478e+00]],\n",
            "\n",
            "         [[ 3.2583e-01, -4.3776e-01,  5.0479e-01,  ...,  7.0253e-01,\n",
            "            2.0761e+00, -1.5627e+00],\n",
            "          [ 8.8250e-01,  5.9745e-01,  3.6933e-01,  ..., -5.8944e-01,\n",
            "            5.5956e-01, -7.7937e-02],\n",
            "          [-3.6473e-01,  4.8131e-01,  6.1268e-01,  ...,  6.5565e-01,\n",
            "           -3.8956e-02, -6.8699e-01],\n",
            "          ...,\n",
            "          [ 2.2557e-02, -1.6604e+00, -1.9163e+00,  ...,  2.8241e-01,\n",
            "           -1.4490e+00,  1.2908e+00],\n",
            "          [ 5.6906e-01,  8.4790e-01,  8.8025e-01,  ...,  1.2721e-01,\n",
            "           -3.5546e-01, -2.1559e+00],\n",
            "          [ 2.3368e-01,  4.1604e-01, -2.9679e-01,  ..., -1.6171e+00,\n",
            "            7.5432e-01, -2.5645e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.8039e-01,  1.0166e+00, -1.9968e+00,  ...,  5.2648e-01,\n",
            "           -1.3073e+00, -2.5629e-01],\n",
            "          [-1.3349e+00, -6.9624e-02, -9.3911e-02,  ...,  9.8580e-01,\n",
            "            1.2035e+00,  5.1740e-01],\n",
            "          [-1.2060e+00,  1.1911e+00,  1.1373e+00,  ...,  6.7474e-01,\n",
            "           -6.4775e-01, -3.8667e-01],\n",
            "          ...,\n",
            "          [-2.0740e+00, -8.1535e-01, -2.5305e+00,  ...,  4.7654e-01,\n",
            "            3.8928e-01, -5.4497e-01],\n",
            "          [ 5.7676e-01,  8.0787e-02, -5.6956e-01,  ..., -1.1259e+00,\n",
            "           -1.6109e+00, -1.2946e+00],\n",
            "          [ 5.1059e-03, -7.4994e-01,  4.3954e-01,  ..., -9.4220e-01,\n",
            "           -2.7758e-04,  1.4872e+00]],\n",
            "\n",
            "         [[-4.0913e-01,  1.5664e+00, -1.1392e+00,  ..., -6.1765e-01,\n",
            "            3.5468e-02,  1.2486e+00],\n",
            "          [-1.4390e-01,  8.8626e-01, -1.8894e+00,  ..., -2.9202e-01,\n",
            "           -6.1582e-01, -1.8902e-01],\n",
            "          [ 1.4231e+00, -7.0181e-02, -2.2128e-02,  ...,  2.5203e-01,\n",
            "            9.1478e-01,  1.8181e+00],\n",
            "          ...,\n",
            "          [-2.9916e-01, -1.1928e+00, -4.2653e-01,  ...,  1.0149e+00,\n",
            "            6.4538e-01,  1.4862e+00],\n",
            "          [-3.8223e-01,  1.7675e-01, -1.5458e+00,  ..., -1.2645e+00,\n",
            "            1.8920e+00,  2.8988e-01],\n",
            "          [-1.2271e+00, -2.0275e+00,  5.9558e-01,  ..., -2.2615e+00,\n",
            "            7.7672e-01, -5.1870e-01]],\n",
            "\n",
            "         [[ 1.0038e+00,  8.2879e-01, -1.3653e+00,  ...,  2.7359e-01,\n",
            "            2.3782e+00,  5.2419e-01],\n",
            "          [-1.0487e+00, -5.5894e-01,  2.3631e+00,  ..., -1.4516e+00,\n",
            "            1.4345e-01,  1.5000e+00],\n",
            "          [ 4.2899e-01, -1.1840e+00,  2.5998e-01,  ..., -6.3309e-01,\n",
            "           -6.0239e-01,  7.6984e-02],\n",
            "          ...,\n",
            "          [-9.0040e-01,  2.0245e-01,  2.6257e-01,  ..., -9.7111e-01,\n",
            "           -7.5070e-01, -9.1662e-01],\n",
            "          [ 2.9223e-01, -1.1067e+00,  1.9894e+00,  ...,  1.7058e+00,\n",
            "            3.6222e-01,  1.3179e+00],\n",
            "          [ 7.0073e-01,  2.1043e-01,  1.0150e+00,  ...,  8.8679e-01,\n",
            "            1.9382e+00, -9.9359e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.3323e+00,  1.6126e+00, -8.6707e-01,  ...,  7.0597e-01,\n",
            "            6.5236e-01,  8.5260e-01],\n",
            "          [ 1.9990e-01, -2.3137e+00, -2.3431e-01,  ...,  3.5312e-01,\n",
            "           -7.0006e-01, -1.1615e+00],\n",
            "          [-9.5913e-01, -1.2251e+00, -9.1880e-01,  ...,  3.9920e-02,\n",
            "           -8.6269e-01,  1.0799e+00],\n",
            "          ...,\n",
            "          [-7.5630e-01, -6.9422e-01,  9.9434e-01,  ...,  5.1363e-02,\n",
            "            8.2154e-02, -3.7094e-01],\n",
            "          [-3.7883e-01,  9.2957e-01,  7.9440e-01,  ..., -2.5524e-01,\n",
            "           -5.2168e-01,  1.0791e+00],\n",
            "          [ 5.2615e-01, -6.9039e-01,  1.1990e+00,  ...,  1.0083e+00,\n",
            "           -4.2718e-01, -1.0600e+00]],\n",
            "\n",
            "         [[-5.9427e-01,  7.5054e-01,  5.6250e-01,  ..., -7.0769e-01,\n",
            "           -2.8057e-01,  4.4514e-01],\n",
            "          [ 2.2317e+00, -1.2151e+00,  2.0584e+00,  ..., -9.1981e-01,\n",
            "           -5.5762e-01,  7.6021e-01],\n",
            "          [ 6.0565e-01, -1.4914e+00, -5.4571e-01,  ...,  2.1853e-01,\n",
            "            3.5959e-01,  1.8852e-01],\n",
            "          ...,\n",
            "          [-6.4490e-02, -4.1400e-02, -5.8571e-01,  ..., -2.4688e-01,\n",
            "           -9.0625e-01, -9.9852e-01],\n",
            "          [-1.0943e+00,  6.9866e-02,  2.6598e-01,  ...,  1.2561e+00,\n",
            "           -1.6237e+00, -1.3460e-01],\n",
            "          [-8.5040e-01, -5.6206e-01,  1.3485e+00,  ...,  3.2917e-02,\n",
            "           -1.8616e+00, -1.6733e-01]],\n",
            "\n",
            "         [[ 1.1265e+00,  1.0159e+00, -6.3143e-01,  ...,  1.2877e+00,\n",
            "            5.5066e-01, -3.9836e-01],\n",
            "          [ 1.0764e-01,  1.7284e+00, -5.1613e-01,  ...,  1.3137e+00,\n",
            "            1.9602e+00, -2.5376e+00],\n",
            "          [ 7.5766e-02, -1.2396e-02,  1.1505e+00,  ...,  4.5946e-01,\n",
            "            8.1357e-01, -2.7042e-01],\n",
            "          ...,\n",
            "          [-2.4624e-01,  1.9957e+00,  4.4599e-01,  ..., -3.9642e-01,\n",
            "            9.5189e-01,  7.4126e-01],\n",
            "          [ 1.2415e-01, -3.8191e-01,  1.2628e+00,  ..., -1.3440e+00,\n",
            "            1.7807e-01,  7.6470e-02],\n",
            "          [-8.9984e-01,  1.4241e+00, -1.3348e+00,  ...,  2.1992e+00,\n",
            "           -4.1464e-01, -7.4146e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.5929e+00,  2.4565e-01,  2.0179e+00,  ..., -1.6074e+00,\n",
            "           -1.0241e+00,  3.1137e-01],\n",
            "          [-1.1347e+00, -4.1343e-02,  3.3225e-03,  ..., -9.6510e-01,\n",
            "           -2.5983e-01,  7.3052e-01],\n",
            "          [-1.8901e-01, -6.7274e-01, -9.3140e-01,  ..., -1.3187e+00,\n",
            "           -2.5781e-01, -2.0254e+00],\n",
            "          ...,\n",
            "          [ 1.1621e+00,  8.1792e-01,  3.6188e-01,  ..., -6.4775e-01,\n",
            "            3.9657e-01, -2.5672e-01],\n",
            "          [-6.2884e-01, -1.5232e+00,  3.0439e-01,  ...,  1.1711e+00,\n",
            "            1.5135e-01, -6.7313e-02],\n",
            "          [-7.6301e-02,  5.0187e-01, -6.7782e-01,  ..., -7.7525e-02,\n",
            "           -8.3962e-02, -8.3745e-01]],\n",
            "\n",
            "         [[-1.4420e+00, -2.0778e-01, -1.3439e-01,  ...,  1.4709e+00,\n",
            "            7.0300e-01, -1.7007e+00],\n",
            "          [-8.0094e-01, -3.1161e-01, -2.1076e+00,  ..., -1.3376e+00,\n",
            "           -1.1945e+00,  9.7013e-01],\n",
            "          [-4.5531e-01, -8.3563e-01,  7.1118e-01,  ...,  1.0536e+00,\n",
            "           -1.7854e+00, -6.9985e-01],\n",
            "          ...,\n",
            "          [-1.8670e+00,  4.9451e-03,  1.8499e-01,  ..., -1.3137e-01,\n",
            "           -6.2500e-01, -5.1776e-01],\n",
            "          [ 1.0616e-01,  1.3474e+00, -4.1602e-01,  ..., -1.1396e+00,\n",
            "           -2.7182e+00, -2.7767e-01],\n",
            "          [-1.3193e+00,  1.5584e+00, -2.7278e-01,  ..., -8.4565e-01,\n",
            "            5.0022e-01, -1.3210e+00]],\n",
            "\n",
            "         [[ 3.3610e-01, -1.3911e+00,  7.0696e-01,  ...,  7.8875e-01,\n",
            "           -5.1218e-01, -8.2945e-01],\n",
            "          [ 1.0026e+00,  7.2688e-01,  4.0054e-01,  ..., -7.6571e-01,\n",
            "           -6.9136e-01, -1.1512e-01],\n",
            "          [-5.6701e-01, -3.0772e-01, -3.4675e-01,  ...,  4.6319e-01,\n",
            "           -3.4607e-01,  2.3965e+00],\n",
            "          ...,\n",
            "          [ 7.1189e-01, -5.6300e-01, -8.4290e-01,  ...,  6.8910e-01,\n",
            "            7.5020e-01, -2.4183e-01],\n",
            "          [ 3.1988e-01, -3.4480e-01,  1.3081e+00,  ...,  1.3173e-01,\n",
            "            8.2553e-01, -2.8226e+00],\n",
            "          [ 1.0737e-01, -1.9859e+00,  1.1551e+00,  ..., -4.2486e-02,\n",
            "            6.6728e-01, -6.5850e-01]]]])\n",
            "tensor([[[[ 3.2235e+00, -1.1383e+00, -1.1459e+00,  ..., -1.6284e-01,\n",
            "           -1.2557e+00,  2.2769e-01],\n",
            "          [-1.9627e-01,  9.9446e-01,  1.1249e+00,  ...,  1.8946e+00,\n",
            "            1.7727e-02,  1.0385e+00],\n",
            "          [ 6.2843e-01,  6.9471e-01,  7.5852e-02,  ..., -6.9237e-01,\n",
            "            2.3712e-01, -2.1059e-01],\n",
            "          ...,\n",
            "          [ 1.8013e+00, -4.6459e-01,  1.7593e+00,  ..., -1.9762e-01,\n",
            "           -1.1144e-01, -2.0946e-01],\n",
            "          [-9.6273e-01, -2.7004e-01,  4.3102e-01,  ..., -2.0023e+00,\n",
            "           -9.2114e-01,  1.8342e+00],\n",
            "          [ 1.9998e+00,  2.1892e-01,  3.5457e-01,  ..., -1.4035e+00,\n",
            "           -7.4293e-01, -8.8774e-01]],\n",
            "\n",
            "         [[-1.3098e+00, -4.2239e-01,  3.2568e+00,  ..., -3.4831e-01,\n",
            "            1.2702e+00, -3.6281e+00],\n",
            "          [-8.1138e-01, -5.1825e-01,  3.0100e-01,  ...,  1.4085e+00,\n",
            "            4.8078e-02,  3.5660e-02],\n",
            "          [ 4.7991e-01, -7.8650e-02, -5.6629e-02,  ...,  5.5552e-01,\n",
            "            8.5750e-01, -5.6701e-01],\n",
            "          ...,\n",
            "          [-4.3467e-01,  1.0728e+00, -2.7341e+00,  ..., -2.0035e-01,\n",
            "           -6.3471e-01,  1.4200e+00],\n",
            "          [ 3.8170e-01,  6.4883e-02,  9.7472e-01,  ..., -5.2579e-01,\n",
            "            5.1262e-01,  5.1319e-01],\n",
            "          [ 9.3905e-01,  2.3577e-01,  5.4741e-01,  ...,  1.5440e-02,\n",
            "            5.2849e-01, -8.4530e-01]],\n",
            "\n",
            "         [[-1.9344e-01,  2.7579e-01,  1.3996e+00,  ...,  8.8654e-02,\n",
            "            6.9955e-01,  1.4651e-02],\n",
            "          [-6.8378e-02,  7.5135e-01,  1.0316e+00,  ..., -4.5667e-01,\n",
            "            3.9039e-01,  4.4000e-01],\n",
            "          [ 8.6622e-01, -2.6010e-01,  3.5321e-01,  ...,  1.4506e+00,\n",
            "            7.1671e-01,  8.3795e-02],\n",
            "          ...,\n",
            "          [-6.5548e-01, -9.0910e-01, -1.8816e+00,  ...,  1.5867e+00,\n",
            "            7.8105e-01,  8.8856e-02],\n",
            "          [ 2.4917e+00, -5.2551e-01,  6.8155e-01,  ...,  3.8549e-03,\n",
            "           -3.8328e-01,  1.4302e+00],\n",
            "          [-8.8227e-01,  3.0449e-01,  5.0230e-01,  ..., -1.8488e-01,\n",
            "           -3.2190e-01, -4.6363e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.6321e+00, -1.7578e+00,  2.3275e-01,  ..., -3.0293e-01,\n",
            "            9.8096e-01, -1.5690e-01],\n",
            "          [-8.1243e-01, -1.9337e-01,  1.1327e+00,  ...,  8.1624e-02,\n",
            "            7.1392e-01, -1.3626e-01],\n",
            "          [-1.4769e+00,  9.8006e-01, -1.2874e-01,  ...,  2.1516e+00,\n",
            "           -8.2751e-01, -1.5875e+00],\n",
            "          ...,\n",
            "          [ 1.3259e+00,  1.4540e+00,  1.9688e+00,  ...,  5.8429e-02,\n",
            "           -1.5705e+00, -2.5230e-01],\n",
            "          [-2.8012e-01,  1.7652e+00,  1.4343e+00,  ...,  8.9406e-02,\n",
            "            1.3837e+00,  7.1755e-01],\n",
            "          [-2.1339e+00,  4.0823e-01,  1.6647e+00,  ...,  6.0051e-01,\n",
            "           -1.8980e-01,  1.1589e-01]],\n",
            "\n",
            "         [[ 9.1502e-01,  1.6519e+00, -9.8731e-01,  ..., -1.5291e+00,\n",
            "           -7.3330e-02,  5.3691e-01],\n",
            "          [-1.1005e+00, -2.9009e+00,  1.4563e+00,  ..., -1.3328e+00,\n",
            "           -9.1688e-01,  6.4171e-02],\n",
            "          [-1.2399e-01, -7.6436e-01, -5.5686e-01,  ..., -5.0329e-02,\n",
            "            8.0447e-01, -8.4569e-01],\n",
            "          ...,\n",
            "          [ 7.0517e-02, -1.1959e-01,  1.6879e+00,  ...,  8.7563e-01,\n",
            "            4.8115e-01, -1.3535e+00],\n",
            "          [-2.0143e-01,  6.7866e-01, -8.9912e-02,  ...,  1.1290e+00,\n",
            "           -6.4760e-01,  6.8996e-01],\n",
            "          [-1.2019e+00,  2.0724e+00,  8.4435e-01,  ..., -9.0903e-01,\n",
            "            1.2773e+00,  6.6894e-01]],\n",
            "\n",
            "         [[ 1.3366e+00, -7.3539e-01,  9.5692e-01,  ..., -4.2825e-01,\n",
            "            2.1260e+00,  1.6144e+00],\n",
            "          [ 1.8531e-01,  1.1305e+00, -6.4745e-01,  ...,  4.1623e-01,\n",
            "            3.9452e-01, -7.4461e-01],\n",
            "          [ 4.5448e-01,  5.5833e-01, -1.2353e+00,  ...,  1.9223e-01,\n",
            "           -3.4018e-01, -8.0027e-01],\n",
            "          ...,\n",
            "          [ 5.1726e-01, -1.4657e+00, -1.2574e+00,  ...,  7.5427e-01,\n",
            "           -1.1530e-01, -6.8466e-02],\n",
            "          [ 1.1467e-01,  1.8600e-01,  1.3147e+00,  ..., -1.7114e+00,\n",
            "           -8.2457e-01, -2.5616e-01],\n",
            "          [ 1.6432e+00, -1.1166e-02, -8.8997e-01,  ...,  1.3948e+00,\n",
            "           -1.1726e+00,  1.5592e+00]]],\n",
            "\n",
            "\n",
            "        [[[-1.2280e+00,  8.1040e-01,  9.7018e-01,  ..., -7.8427e-01,\n",
            "           -9.1437e-01,  1.7842e-01],\n",
            "          [-5.8296e-01, -1.2903e+00, -5.7047e-01,  ...,  1.6078e+00,\n",
            "           -1.0231e+00,  1.9839e-01],\n",
            "          [-3.2240e-01,  3.3120e-01,  1.2402e+00,  ...,  2.1505e+00,\n",
            "            2.6263e-01,  5.2858e-01],\n",
            "          ...,\n",
            "          [-4.4345e-01, -9.0576e-01,  2.1495e-02,  ...,  3.8676e-01,\n",
            "            3.7953e-01,  3.5446e-01],\n",
            "          [-2.1386e+00, -1.4879e-01, -5.4578e-01,  ..., -4.1386e-01,\n",
            "           -4.6658e-01,  1.7182e+00],\n",
            "          [ 1.7300e-01, -8.9007e-02, -1.5662e-01,  ..., -5.7904e-01,\n",
            "           -2.9279e-01,  7.8850e-01]],\n",
            "\n",
            "         [[-1.0079e+00,  1.9839e+00, -7.3303e-01,  ..., -4.8936e-01,\n",
            "            9.7840e-02, -1.6412e-01],\n",
            "          [-4.4280e-01,  6.4027e-01,  7.3563e-01,  ..., -3.2265e-01,\n",
            "           -9.6694e-01,  2.7034e-01],\n",
            "          [ 1.6482e+00,  1.5186e+00,  1.2776e-01,  ...,  1.5288e+00,\n",
            "            2.5714e-01, -5.5243e-01],\n",
            "          ...,\n",
            "          [ 1.8834e+00, -5.1316e-03,  1.0027e-01,  ..., -3.0160e+00,\n",
            "           -9.0142e-01, -1.3900e+00],\n",
            "          [ 4.2448e-01,  5.8056e-01,  8.6247e-01,  ..., -8.6803e-02,\n",
            "            1.7063e+00, -5.2910e-01],\n",
            "          [-5.7459e-01,  1.0414e+00,  5.6619e-01,  ..., -1.3590e+00,\n",
            "           -1.5872e+00, -1.0620e+00]],\n",
            "\n",
            "         [[ 8.6186e-02,  1.0948e+00,  1.4572e+00,  ...,  7.1451e-01,\n",
            "            1.4913e+00,  6.6753e-01],\n",
            "          [-2.3907e-01, -1.0878e+00,  4.1545e-01,  ..., -3.4254e-01,\n",
            "           -1.1593e-02, -1.1757e+00],\n",
            "          [-6.4597e-01,  2.0686e-01, -2.0328e+00,  ..., -2.9107e-01,\n",
            "           -4.5935e-01,  8.4210e-01],\n",
            "          ...,\n",
            "          [-2.8137e-01, -7.3559e-02, -1.9823e+00,  ...,  1.4895e+00,\n",
            "           -1.9694e-01, -4.5213e-01],\n",
            "          [ 2.5179e-01, -9.3358e-01, -1.6556e+00,  ..., -1.1175e+00,\n",
            "           -2.6463e+00, -2.9731e-01],\n",
            "          [ 6.9307e-01,  1.7388e-01, -2.1493e-01,  ..., -1.0034e+00,\n",
            "            9.4362e-01, -2.5956e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.1817e+00,  9.8988e-01,  6.4601e-01,  ...,  1.3996e-01,\n",
            "           -4.2591e-01, -7.8704e-01],\n",
            "          [-4.3184e-02, -3.7744e-01,  7.5052e-01,  ..., -5.6917e-01,\n",
            "           -4.2992e-01,  4.0603e-01],\n",
            "          [-5.9989e-01,  2.3906e-01, -6.2230e-01,  ..., -2.2409e-01,\n",
            "            5.5518e-01,  3.7830e-01],\n",
            "          ...,\n",
            "          [ 6.4231e-01, -1.8575e+00, -6.4841e-01,  ..., -9.4225e-02,\n",
            "            5.9696e-02, -2.1310e-01],\n",
            "          [-2.9805e-01, -9.2688e-02, -1.1726e+00,  ...,  1.0669e+00,\n",
            "           -1.9267e-01,  1.2602e+00],\n",
            "          [ 7.2384e-01,  2.3789e-02, -9.5187e-01,  ..., -2.9478e-01,\n",
            "            1.8876e+00,  1.4950e+00]],\n",
            "\n",
            "         [[-4.3092e-01,  7.9034e-02,  1.2561e-01,  ...,  3.2017e-01,\n",
            "           -4.5892e-01,  6.1331e-01],\n",
            "          [-1.1528e-01, -4.1548e-01, -6.6157e-01,  ..., -2.8384e-01,\n",
            "            1.1642e+00,  1.9407e-01],\n",
            "          [-1.2395e+00,  1.0293e+00,  7.4524e-01,  ...,  1.2118e+00,\n",
            "            5.2319e-01, -9.7066e-01],\n",
            "          ...,\n",
            "          [ 8.9904e-01, -1.1655e-01,  3.6184e-01,  ...,  6.7872e-01,\n",
            "           -3.2734e-01,  1.0606e+00],\n",
            "          [-1.5286e+00, -9.4579e-01, -1.5398e+00,  ...,  1.8107e+00,\n",
            "           -4.3878e-02,  7.4351e-01],\n",
            "          [ 1.1241e+00,  3.4693e-01, -1.2330e+00,  ..., -1.0249e+00,\n",
            "            1.1742e+00, -4.1295e-01]],\n",
            "\n",
            "         [[ 2.4160e-02,  8.5765e-02, -1.1646e+00,  ..., -1.1074e+00,\n",
            "           -4.9301e-01, -4.3659e-01],\n",
            "          [ 8.3622e-01, -2.2291e+00, -5.2144e-01,  ..., -4.4095e-01,\n",
            "            2.5926e-01, -1.0169e+00],\n",
            "          [ 1.3441e-02,  3.3079e-02,  1.8471e+00,  ...,  1.8405e-01,\n",
            "           -8.1968e-01, -1.2643e-02],\n",
            "          ...,\n",
            "          [-4.3698e-01,  2.1450e-01,  1.5717e+00,  ...,  4.5620e-02,\n",
            "            1.4564e+00, -3.3915e-01],\n",
            "          [ 1.7642e+00,  3.1179e-01, -1.4813e-01,  ...,  1.1530e+00,\n",
            "            2.7529e-01,  1.8826e-01],\n",
            "          [ 5.1963e-03, -2.9819e-01,  2.1134e+00,  ...,  1.1844e+00,\n",
            "           -1.3247e-01,  5.6639e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 2.3219e-01,  2.2324e+00,  1.0417e+00,  ...,  1.1956e+00,\n",
            "            9.2040e-01,  3.8785e-01],\n",
            "          [-5.8417e-01,  9.3213e-01,  1.4023e+00,  ...,  4.7699e-01,\n",
            "           -8.0870e-01,  9.4241e-02],\n",
            "          [-1.2003e-01, -1.4546e+00, -4.3042e-01,  ...,  7.0132e-02,\n",
            "           -6.9081e-01,  1.1452e+00],\n",
            "          ...,\n",
            "          [ 5.2557e-01,  8.2251e-01,  3.8995e-01,  ...,  3.6893e-01,\n",
            "           -7.6582e-01,  1.6086e+00],\n",
            "          [ 1.5083e+00,  3.7848e-01, -1.1125e-02,  ..., -1.8307e-01,\n",
            "            7.3007e-01,  9.3676e-01],\n",
            "          [-2.8080e-02, -1.7345e+00, -1.2853e-01,  ..., -6.4070e-01,\n",
            "           -4.3588e-02,  3.1733e-01]],\n",
            "\n",
            "         [[-6.6471e-01, -1.1008e-01, -1.7263e-01,  ...,  5.2375e-01,\n",
            "            6.5810e-01,  5.1223e-01],\n",
            "          [-3.1683e-01, -6.5676e-01,  6.6668e-01,  ...,  5.4836e-02,\n",
            "           -5.5736e-01,  1.0847e+00],\n",
            "          [ 1.1327e+00,  8.7760e-01,  1.1207e+00,  ..., -2.9479e+00,\n",
            "            1.0582e+00, -6.8399e-03],\n",
            "          ...,\n",
            "          [-1.0695e+00,  5.8622e-01, -1.4905e+00,  ...,  4.0910e-01,\n",
            "            1.5919e-01, -9.3522e-01],\n",
            "          [ 9.2890e-02, -1.3288e+00, -1.7312e+00,  ...,  1.2469e+00,\n",
            "           -1.0523e+00,  2.0402e-01],\n",
            "          [-1.3134e-01, -3.2528e-01, -2.5749e+00,  ..., -1.1396e+00,\n",
            "            1.3073e+00,  5.3095e-01]],\n",
            "\n",
            "         [[-1.5836e+00,  1.7855e+00, -2.3445e+00,  ...,  5.9869e-01,\n",
            "            2.0334e-01,  1.7620e+00],\n",
            "          [-7.7757e-01, -1.2150e+00, -3.4434e-01,  ..., -1.7074e+00,\n",
            "           -2.4745e+00,  1.3479e+00],\n",
            "          [ 4.8648e-01, -8.9126e-01,  1.1891e+00,  ..., -8.1594e-01,\n",
            "            3.1523e-01, -3.8288e-01],\n",
            "          ...,\n",
            "          [-1.1978e-01,  1.0745e+00,  3.6806e-01,  ...,  2.8402e-01,\n",
            "           -5.4362e-04, -1.3423e+00],\n",
            "          [ 6.3481e-01, -1.1819e+00,  1.6220e+00,  ...,  1.6603e+00,\n",
            "            6.6015e-01,  7.9481e-01],\n",
            "          [ 2.6325e-01,  2.2774e-01, -1.5141e-01,  ..., -3.9038e-01,\n",
            "           -1.2342e+00, -1.6404e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.2924e-01,  9.8933e-01, -5.6935e-01,  ..., -1.3145e+00,\n",
            "           -3.2513e-01,  1.2584e+00],\n",
            "          [ 7.1098e-01, -2.2805e+00, -1.4945e+00,  ...,  6.9779e-01,\n",
            "           -6.3366e-01,  3.5076e-01],\n",
            "          [ 1.5572e+00, -1.1977e-01, -9.6158e-01,  ..., -1.6443e-01,\n",
            "           -1.2333e+00,  9.8116e-02],\n",
            "          ...,\n",
            "          [ 6.5301e-01, -1.5923e+00,  1.0962e+00,  ...,  3.3734e-01,\n",
            "            1.2039e+00,  2.0608e-01],\n",
            "          [ 2.6098e-01, -6.3873e-01, -4.8908e-01,  ..., -4.9009e-01,\n",
            "           -9.1421e-01, -2.3629e+00],\n",
            "          [-2.4662e-01, -5.7961e-01,  4.6541e-01,  ...,  3.8074e-01,\n",
            "            2.4428e+00, -1.9954e-01]],\n",
            "\n",
            "         [[-1.1348e-01, -4.3145e-01,  1.2460e-01,  ..., -5.5796e-03,\n",
            "           -8.8436e-02,  3.6379e-01],\n",
            "          [-8.5676e-01,  7.5593e-01,  4.9517e-01,  ..., -7.8644e-01,\n",
            "           -1.5061e+00,  1.0830e+00],\n",
            "          [ 1.2226e+00,  3.7667e-01, -6.3229e-02,  ...,  6.1620e-01,\n",
            "           -1.8150e-01, -9.5719e-01],\n",
            "          ...,\n",
            "          [-1.9800e-01, -5.1322e-01,  1.2650e+00,  ...,  2.0557e+00,\n",
            "            1.9568e+00, -1.2998e-01],\n",
            "          [-2.1817e+00, -9.1205e-01, -2.9829e-01,  ..., -7.9508e-01,\n",
            "            6.9303e-01, -4.9602e-01],\n",
            "          [-3.1392e-01,  8.5887e-01, -2.9938e-01,  ..., -7.8908e-01,\n",
            "           -7.1344e-02,  2.5749e-02]],\n",
            "\n",
            "         [[ 7.0795e-01,  7.9687e-01,  1.1887e-01,  ..., -1.5950e-01,\n",
            "            7.8929e-01,  2.3810e+00],\n",
            "          [ 9.0572e-01, -2.3197e-01,  7.0017e-01,  ..., -1.5829e+00,\n",
            "            4.4550e-01, -6.4558e-01],\n",
            "          [ 1.0097e+00, -8.2005e-01, -1.7194e+00,  ...,  6.6544e-01,\n",
            "           -3.4619e-01, -7.1583e-01],\n",
            "          ...,\n",
            "          [ 8.1573e-01, -1.6367e+00,  1.9097e+00,  ...,  1.2795e+00,\n",
            "           -6.7939e-01,  1.2353e+00],\n",
            "          [ 7.4373e-01, -1.8597e-01, -7.2029e-01,  ...,  1.8895e+00,\n",
            "            1.0646e+00, -6.2089e-01],\n",
            "          [ 4.3560e-01,  8.7899e-01, -7.3428e-01,  ..., -1.0416e+00,\n",
            "            3.5513e-01,  2.3729e-01]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-1.0718e+00, -4.7732e-01, -4.0729e-01,  ..., -1.7337e+00,\n",
            "            7.5039e-01,  1.3566e+00],\n",
            "          [ 7.9917e-02,  2.0767e-03,  1.5778e-01,  ..., -2.9599e-01,\n",
            "           -2.9927e-01, -2.6605e+00],\n",
            "          [-1.4342e+00,  1.1435e+00, -1.3787e+00,  ...,  3.8976e-01,\n",
            "            1.6472e-01, -7.1689e-01],\n",
            "          ...,\n",
            "          [-1.9128e+00, -1.0752e+00,  5.7736e-01,  ..., -1.2277e+00,\n",
            "           -1.2081e-01, -8.3635e-01],\n",
            "          [ 8.1184e-02, -9.1489e-01,  3.7461e-01,  ...,  1.7332e+00,\n",
            "           -4.1833e-01, -1.4987e-01],\n",
            "          [ 1.6463e+00, -3.3602e+00, -1.3526e+00,  ..., -7.5758e-02,\n",
            "           -4.3951e-01,  7.3124e-01]],\n",
            "\n",
            "         [[-1.2651e-01,  1.1779e-01,  1.2585e-01,  ...,  1.6051e+00,\n",
            "            2.2782e+00, -4.4854e-01],\n",
            "          [ 2.8816e-01,  1.4295e+00, -5.3344e-01,  ...,  9.3150e-01,\n",
            "            9.9169e-01, -1.3960e+00],\n",
            "          [-1.2357e+00,  6.0718e-01,  9.6774e-01,  ..., -6.5043e-01,\n",
            "           -4.3502e-01, -1.0966e-01],\n",
            "          ...,\n",
            "          [ 3.8188e-01,  1.0775e+00, -1.8916e-01,  ...,  9.0092e-01,\n",
            "           -7.9773e-01,  6.0954e-01],\n",
            "          [-4.8086e-01,  2.2913e-01, -4.5483e-01,  ..., -1.4464e+00,\n",
            "            2.6570e-01,  8.4012e-01],\n",
            "          [ 4.2934e-01,  5.1822e-01, -9.6855e-01,  ..., -5.8559e-01,\n",
            "            7.2610e-01,  5.2931e-01]],\n",
            "\n",
            "         [[ 1.1709e+00, -3.8389e-01, -6.3957e-01,  ..., -1.6701e-01,\n",
            "           -8.4306e-01,  2.5057e-01],\n",
            "          [ 5.1072e-02,  2.2300e+00, -1.0317e+00,  ..., -2.0225e+00,\n",
            "            3.1090e-01, -7.5955e-02],\n",
            "          [-7.5152e-01,  6.3711e-01,  2.3042e-01,  ...,  1.5031e+00,\n",
            "            6.3841e-01, -1.1951e+00],\n",
            "          ...,\n",
            "          [-2.3014e+00, -1.3865e-01, -1.1584e+00,  ..., -5.4694e-02,\n",
            "            1.0810e-01,  1.1082e+00],\n",
            "          [ 4.3234e-01,  1.9162e+00, -1.5585e+00,  ..., -1.2820e+00,\n",
            "           -5.1238e-01,  2.3916e-01],\n",
            "          [-8.8166e-03,  7.2926e-01, -7.3788e-01,  ..., -5.8456e-01,\n",
            "            1.8804e-01, -1.3152e+00]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.3335e+00, -6.9662e-01,  1.3012e-01,  ...,  1.9706e-01,\n",
            "           -7.9874e-01,  9.6523e-02],\n",
            "          [ 8.7081e-01,  2.4000e-01,  5.4681e-03,  ...,  2.7629e-01,\n",
            "            5.6754e-02,  2.5565e-01],\n",
            "          [-1.4922e-01, -8.7439e-02,  1.1349e+00,  ...,  1.0047e+00,\n",
            "            1.6976e+00, -1.2424e+00],\n",
            "          ...,\n",
            "          [-1.0059e-01,  2.4167e-02, -1.4791e+00,  ..., -2.5138e-01,\n",
            "            1.0419e+00, -8.6254e-01],\n",
            "          [-5.9892e-01, -1.6264e+00,  3.2911e-01,  ...,  1.2641e-01,\n",
            "           -5.7144e-02,  1.4543e+00],\n",
            "          [ 1.6914e+00,  1.0097e+00,  2.9603e-01,  ...,  4.5248e-01,\n",
            "           -5.5481e-01,  1.0315e+00]],\n",
            "\n",
            "         [[ 8.6878e-01,  2.8370e-01,  1.9997e-01,  ..., -2.4691e+00,\n",
            "           -4.0832e-01,  7.7197e-01],\n",
            "          [-4.3918e-01,  6.3584e-01, -2.1986e+00,  ...,  8.5443e-01,\n",
            "            2.7257e-01,  2.0662e+00],\n",
            "          [-7.8519e-02,  9.4580e-01, -3.5563e-01,  ..., -6.9971e-01,\n",
            "           -5.5693e-01, -1.9094e+00],\n",
            "          ...,\n",
            "          [ 4.5039e-01, -1.1549e+00,  1.9576e+00,  ..., -7.0559e-01,\n",
            "            7.9876e-02,  1.5901e-01],\n",
            "          [ 1.5534e+00,  1.2773e+00,  6.9410e-01,  ..., -1.5391e+00,\n",
            "            3.0926e-01, -1.5776e+00],\n",
            "          [ 3.6934e-01, -6.9544e-01, -1.7238e-01,  ...,  1.0453e+00,\n",
            "           -8.7620e-01, -1.0510e+00]],\n",
            "\n",
            "         [[-1.4135e+00, -1.8501e-01,  1.1688e-01,  ..., -1.6181e+00,\n",
            "            6.3328e-01,  7.0595e-01],\n",
            "          [-1.1574e+00,  1.3416e+00,  3.1592e-02,  ...,  1.5600e+00,\n",
            "            2.9175e-02,  3.6813e-01],\n",
            "          [-8.8625e-01,  2.9249e+00, -2.9008e-01,  ...,  1.3329e+00,\n",
            "            6.2570e-01, -8.5831e-01],\n",
            "          ...,\n",
            "          [-9.4599e-01, -1.5521e+00,  1.9027e+00,  ..., -4.1094e-01,\n",
            "            3.3198e-01, -1.1575e-01],\n",
            "          [ 1.5992e+00, -3.9657e-01,  1.6619e+00,  ..., -3.0668e-01,\n",
            "            7.9041e-01,  1.0331e-01],\n",
            "          [ 4.7502e-01, -5.4145e-02,  5.1704e-01,  ...,  6.9389e-01,\n",
            "           -5.8621e-01, -2.9188e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 4.6322e-01, -6.0995e-01,  8.8390e-01,  ...,  4.9983e-01,\n",
            "           -4.3877e-01,  4.8463e-01],\n",
            "          [-7.3566e-01,  8.8683e-01, -5.8695e-01,  ...,  2.2854e-01,\n",
            "           -3.6981e-01,  6.3693e-01],\n",
            "          [ 1.8906e-01, -2.2159e-01, -1.5715e+00,  ..., -6.0926e-01,\n",
            "           -2.2387e+00, -9.6205e-02],\n",
            "          ...,\n",
            "          [-1.9316e+00,  1.6647e+00,  4.4183e-01,  ..., -2.1282e-01,\n",
            "           -3.1323e-01,  1.0148e+00],\n",
            "          [ 1.1445e+00,  6.7501e-01,  7.6657e-01,  ...,  1.0798e+00,\n",
            "           -2.4849e+00, -2.3919e-01],\n",
            "          [-9.4270e-02, -1.1076e+00, -4.9197e-01,  ..., -3.3741e-01,\n",
            "           -2.5757e+00,  1.0783e+00]],\n",
            "\n",
            "         [[-6.1098e-01,  1.1822e+00,  8.7134e-03,  ...,  1.6989e-01,\n",
            "           -8.0958e-01, -3.8880e-01],\n",
            "          [-1.5384e+00,  1.9243e+00, -2.3150e+00,  ..., -5.3164e-02,\n",
            "            1.5274e-01, -2.3834e+00],\n",
            "          [-3.3257e-01, -5.0843e-01, -3.8493e-02,  ...,  9.8985e-01,\n",
            "           -1.0106e-03,  9.0833e-01],\n",
            "          ...,\n",
            "          [ 9.9880e-01, -1.9559e-02,  1.4925e+00,  ...,  3.2175e-01,\n",
            "           -6.6061e-01,  2.1775e-01],\n",
            "          [ 2.2731e-01,  3.5513e-01,  7.8370e-01,  ...,  1.7724e+00,\n",
            "            1.4587e+00,  1.1486e+00],\n",
            "          [ 6.1807e-01,  9.2494e-03, -6.5001e-01,  ...,  4.7117e-01,\n",
            "            4.6721e-01, -1.2437e+00]],\n",
            "\n",
            "         [[ 3.2441e-01, -4.3622e-01,  5.0268e-01,  ...,  6.9964e-01,\n",
            "            2.0678e+00, -1.5568e+00],\n",
            "          [ 8.7892e-01,  5.9498e-01,  3.6774e-01,  ..., -5.8731e-01,\n",
            "            5.5723e-01, -7.7790e-02],\n",
            "          [-3.6347e-01,  4.7929e-01,  6.1015e-01,  ...,  6.5295e-01,\n",
            "           -3.8960e-02, -6.8448e-01],\n",
            "          ...,\n",
            "          [ 2.2314e-02, -1.6541e+00, -1.9090e+00,  ...,  2.8116e-01,\n",
            "           -1.4435e+00,  1.2856e+00],\n",
            "          [ 5.6670e-01,  8.4446e-01,  8.7668e-01,  ...,  1.2656e-01,\n",
            "           -3.5423e-01, -2.1477e+00],\n",
            "          [ 2.3262e-01,  4.1427e-01, -2.9579e-01,  ..., -1.6110e+00,\n",
            "            7.5124e-01, -2.5561e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 7.8225e-01,  1.0177e+00, -1.9855e+00,  ...,  5.2921e-01,\n",
            "           -1.2983e+00, -2.5091e-01],\n",
            "          [-1.3259e+00, -6.4874e-02, -8.9079e-02,  ...,  9.8697e-01,\n",
            "            1.2040e+00,  5.2015e-01],\n",
            "          [-1.1974e+00,  1.1915e+00,  1.1380e+00,  ...,  6.7697e-01,\n",
            "           -6.4104e-01, -3.8084e-01],\n",
            "          ...,\n",
            "          [-2.0624e+00, -8.0807e-01, -2.5174e+00,  ...,  4.7943e-01,\n",
            "            3.9247e-01, -5.3860e-01],\n",
            "          [ 5.7932e-01,  8.5026e-02, -5.6312e-01,  ..., -1.1176e+00,\n",
            "           -1.6009e+00, -1.2857e+00],\n",
            "          [ 9.6022e-03, -7.4288e-01,  4.4257e-01,  ..., -9.3449e-01,\n",
            "            4.2369e-03,  1.4866e+00]],\n",
            "\n",
            "         [[-4.0358e-01,  1.5768e+00, -1.1355e+00,  ..., -6.1262e-01,\n",
            "            4.2122e-02,  1.2582e+00],\n",
            "          [-1.3769e-01,  8.9503e-01, -1.8875e+00,  ..., -2.8618e-01,\n",
            "           -6.1078e-01, -1.8293e-01],\n",
            "          [ 1.4332e+00, -6.3790e-02, -1.5617e-02,  ...,  2.5922e-01,\n",
            "            9.2362e-01,  1.8292e+00],\n",
            "          ...,\n",
            "          [-2.9333e-01, -1.1892e+00, -4.2102e-01,  ...,  1.0240e+00,\n",
            "            6.5355e-01,  1.4965e+00],\n",
            "          [-3.7662e-01,  1.8375e-01, -1.5431e+00,  ..., -1.2611e+00,\n",
            "            1.9033e+00,  2.9717e-01],\n",
            "          [-1.2235e+00, -2.0260e+00,  6.0362e-01,  ..., -2.2605e+00,\n",
            "            7.8522e-01, -5.1342e-01]],\n",
            "\n",
            "         [[ 1.0025e+00,  8.2660e-01, -1.3790e+00,  ...,  2.6847e-01,\n",
            "            2.3842e+00,  5.2039e-01],\n",
            "          [-1.0608e+00, -5.6844e-01,  2.3690e+00,  ..., -1.4659e+00,\n",
            "            1.3765e-01,  1.5013e+00],\n",
            "          [ 4.2469e-01, -1.1968e+00,  2.5479e-01,  ..., -6.4299e-01,\n",
            "           -6.1212e-01,  7.0829e-02],\n",
            "          ...,\n",
            "          [-9.1170e-01,  1.9696e-01,  2.5739e-01,  ..., -9.8278e-01,\n",
            "           -7.6121e-01, -9.2801e-01],\n",
            "          [ 2.8721e-01, -1.1191e+00,  1.9934e+00,  ...,  1.7083e+00,\n",
            "            3.5756e-01,  1.3183e+00],\n",
            "          [ 6.9786e-01,  2.0498e-01,  1.0138e+00,  ...,  8.8490e-01,\n",
            "            1.9419e+00, -1.0054e+00]]],\n",
            "\n",
            "\n",
            "        [[[ 1.3165e+00,  1.5953e+00, -8.7076e-01,  ...,  6.9364e-01,\n",
            "            6.4033e-01,  8.3947e-01],\n",
            "          [ 1.9036e-01, -2.3095e+00, -2.4147e-01,  ...,  3.4273e-01,\n",
            "           -7.0466e-01, -1.1636e+00],\n",
            "          [-9.6231e-01, -1.2269e+00, -9.2220e-01,  ...,  3.1250e-02,\n",
            "           -8.6640e-01,  1.0655e+00],\n",
            "          ...,\n",
            "          [-7.6060e-01, -6.9886e-01,  9.8043e-01,  ...,  4.2630e-02,\n",
            "            7.3252e-02, -3.7736e-01],\n",
            "          [-3.8520e-01,  9.1602e-01,  7.8159e-01,  ..., -2.6229e-01,\n",
            "           -5.2726e-01,  1.0648e+00],\n",
            "          [ 5.1481e-01, -6.9505e-01,  1.1840e+00,  ...,  9.9430e-01,\n",
            "           -4.3329e-01, -1.0627e+00]],\n",
            "\n",
            "         [[-5.9318e-01,  7.4540e-01,  5.5822e-01,  ..., -7.0608e-01,\n",
            "           -2.8093e-01,  4.4141e-01],\n",
            "          [ 2.2197e+00, -1.2111e+00,  2.0471e+00,  ..., -9.1721e-01,\n",
            "           -5.5670e-01,  7.5502e-01],\n",
            "          [ 6.0117e-01, -1.4862e+00, -5.4485e-01,  ...,  2.1586e-01,\n",
            "            3.5625e-01,  1.8599e-01],\n",
            "          ...,\n",
            "          [-6.5856e-02, -4.2873e-02, -5.8466e-01,  ..., -2.4740e-01,\n",
            "           -9.0371e-01, -9.9555e-01],\n",
            "          [-1.0909e+00,  6.7877e-02,  2.6308e-01,  ...,  1.2487e+00,\n",
            "           -1.6179e+00, -1.3565e-01],\n",
            "          [-8.4813e-01, -5.6112e-01,  1.3406e+00,  ...,  3.1099e-02,\n",
            "           -1.8546e+00, -1.6822e-01]],\n",
            "\n",
            "         [[ 1.1220e+00,  1.0118e+00, -6.2913e-01,  ...,  1.2826e+00,\n",
            "            5.4837e-01, -3.9697e-01],\n",
            "          [ 1.0707e-01,  1.7215e+00, -5.1428e-01,  ...,  1.3085e+00,\n",
            "            1.9524e+00, -2.5279e+00],\n",
            "          [ 7.5316e-02, -1.2503e-02,  1.1459e+00,  ...,  4.5753e-01,\n",
            "            8.1025e-01, -2.6952e-01],\n",
            "          ...,\n",
            "          [-2.4544e-01,  1.9878e+00,  4.4410e-01,  ..., -3.9503e-01,\n",
            "            9.4804e-01,  7.3823e-01],\n",
            "          [ 1.2351e-01, -3.8058e-01,  1.2578e+00,  ..., -1.3389e+00,\n",
            "            1.7723e-01,  7.6018e-02],\n",
            "          [-8.9650e-01,  1.4185e+00, -1.3298e+00,  ...,  2.1905e+00,\n",
            "           -4.1319e-01, -7.5411e-03]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 1.5920e+00,  2.4933e-01,  2.0156e+00,  ..., -1.5975e+00,\n",
            "           -1.0161e+00,  3.1482e-01],\n",
            "          [-1.1263e+00, -3.6689e-02,  7.8248e-03,  ..., -9.5731e-01,\n",
            "           -2.5443e-01,  7.3255e-01],\n",
            "          [-1.8386e-01, -6.6594e-01, -9.2372e-01,  ..., -1.3097e+00,\n",
            "           -2.5242e-01, -2.0140e+00],\n",
            "          ...,\n",
            "          [ 1.1627e+00,  8.1966e-01,  3.6517e-01,  ..., -6.4104e-01,\n",
            "            3.9974e-01, -2.5134e-01],\n",
            "          [-6.2219e-01, -1.5135e+00,  3.0787e-01,  ...,  1.1716e+00,\n",
            "            1.5535e-01, -6.2571e-02],\n",
            "          [-7.1529e-02,  5.0468e-01, -6.7101e-01,  ..., -7.2749e-02,\n",
            "           -7.9164e-02, -8.3009e-01]],\n",
            "\n",
            "         [[-1.4390e+00, -2.0173e-01, -1.2816e-01,  ...,  1.4811e+00,\n",
            "            7.1131e-01, -1.6984e+00],\n",
            "          [-7.9636e-01, -3.0582e-01, -2.1062e+00,  ..., -1.3344e+00,\n",
            "           -1.1909e+00,  9.7911e-01],\n",
            "          [-4.4987e-01, -8.3115e-01,  7.1951e-01,  ...,  1.0627e+00,\n",
            "           -1.7833e+00, -6.9503e-01],\n",
            "          ...,\n",
            "          [-1.8651e+00,  1.1523e-02,  1.9202e-01,  ..., -1.2513e-01,\n",
            "           -6.1999e-01, -5.1248e-01],\n",
            "          [ 1.1299e-01,  1.3573e+00, -4.1049e-01,  ..., -1.1359e+00,\n",
            "           -2.7184e+00, -2.7179e-01],\n",
            "          [-1.3160e+00,  1.5688e+00, -2.6689e-01,  ..., -8.4119e-01,\n",
            "            5.0803e-01, -1.3177e+00]],\n",
            "\n",
            "         [[ 3.3131e-01, -1.4050e+00,  7.0413e-01,  ...,  7.8634e-01,\n",
            "           -5.2144e-01, -8.4038e-01],\n",
            "          [ 1.0013e+00,  7.2415e-01,  3.9609e-01,  ..., -7.7631e-01,\n",
            "           -7.0156e-01, -1.2229e-01],\n",
            "          [-5.7656e-01, -3.1590e-01, -3.5514e-01,  ...,  4.5907e-01,\n",
            "           -3.5445e-01,  2.4025e+00],\n",
            "          ...,\n",
            "          [ 7.0908e-01, -5.7252e-01, -8.5390e-01,  ...,  6.8617e-01,\n",
            "            7.4759e-01, -2.4966e-01],\n",
            "          [ 3.1500e-01, -3.5318e-01,  1.3084e+00,  ...,  1.2586e-01,\n",
            "            8.2332e-01, -2.8441e+00],\n",
            "          [ 1.0137e-01, -2.0029e+00,  1.1547e+00,  ..., -4.9269e-02,\n",
            "            6.6423e-01, -6.6853e-01]]]], grad_fn=<NativeBatchNormBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gc4kYMw8gf7l"
      },
      "source": [
        "# Modified based on https://github.com/pytorch/examples/tree/master/mnist\n",
        "\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.BatchNorm2d=nn.BatchNorm2d(64) ###\n",
        "        #self.dropout1 = nn.Dropout(0.25)\n",
        "        #self.dropout2 = nn.Dropout(0.5)\n",
        "        #self.BatchNorm1d_2=nn.BatchNorm1d(9216)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.BatchNorm1d=nn.BatchNorm1d(128) ###\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.BatchNorm2d(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "       # x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "       # x = self.dropout2(x)\n",
        "        x = self.BatchNorm1d(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "\n",
        "def train(args, model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "            100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyT0DVFLgf7v"
      },
      "source": [
        "    torch.manual_seed(112)\n",
        "    device =torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciMDL2zygf74",
        "outputId": "8ab1ed99-d1ba-4edf-faa6-97625c120031",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391,
          "referenced_widgets": [
            "f00d271f42bb4f33b63b2f27887c8744",
            "315e1ee9c0594839be3e4628034a044b",
            "b7babe159cbc4a0eaf8369599794ad16",
            "fab37ec1f4d84b79a0b6e538ab643ddd",
            "2bf7720ffd214a40959476daf84988dd",
            "06c31d079ad447f5887c0917a2d39838",
            "410e25dc6a43478e85928b275c5897c4",
            "8abe1edb80d047df90b0126090375d59",
            "2544e7f5c4d44055b02afa8c319f9ed5",
            "5351b99637724f71ac0335ada08ee0d2",
            "fd89f7df4e40445c85a2505cbd76455e",
            "fb74dcc767f9430184e472f4e7f32cc0",
            "8173ed0defe7457387688675ca87c634",
            "98890ac0548347fd920bbde4f6f626db",
            "d625c188f6b24825b0611f4fd628942c",
            "c07f4019be004ba4aa48d210ee2ccea4",
            "7d51647254244a34be1eee6f18fcb653",
            "08bf426b61ec4430b1545019d3862f71",
            "32e6a9968fcc4e3484af5dfe3c1a61ae",
            "caa9a22cb83946c8806866dbb9f0bf3a",
            "414d7410db6a43c3afc3453f59030c42",
            "fc86f9f16f9e41aaa7848cbdfde897d9",
            "8374f3261220442f8922692b0ed8582a",
            "2e5863a1d6c745669e368592c47ca6fe",
            "f8bb6cb4edf54a26856478040eff8c1c",
            "a2b9076230f24124bee9e47994617725",
            "7dff99e3202447b78a3b6a2b34caeda7",
            "efd4720977bd455490204833638eafc6",
            "d5218f14da664116934dd07347d8d9bc",
            "26683ba5d89847d2b08a6424b8cedafc",
            "4cde42a5e391444fb7ee1a817901243f",
            "858ccdc73d814cc292499cc7d8d776af"
          ]
        }
      },
      "source": [
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "    dataset_train = datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transform)\n",
        "    dataset_test = datasets.MNIST('../data', train=False,\n",
        "                       transform=transform)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f00d271f42bb4f33b63b2f27887c8744",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2544e7f5c4d44055b02afa8c319f9ed5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d51647254244a34be1eee6f18fcb653",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8bb6cb4edf54a26856478040eff8c1c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BynoQT9gf8C",
        "outputId": "7716db44-132d-447f-e21b-713a1d137a80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataset_train, dataset_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Dataset MNIST\n",
              "     Number of datapoints: 60000\n",
              "     Root location: ../data\n",
              "     Split: Train\n",
              "     StandardTransform\n",
              " Transform: Compose(\n",
              "                ToTensor()\n",
              "                Normalize(mean=(0.1307,), std=(0.3081,))\n",
              "            ), Dataset MNIST\n",
              "     Number of datapoints: 10000\n",
              "     Root location: ../data\n",
              "     Split: Test\n",
              "     StandardTransform\n",
              " Transform: Compose(\n",
              "                ToTensor()\n",
              "                Normalize(mean=(0.1307,), std=(0.3081,))\n",
              "            ))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WP74bm7Rgf8L",
        "outputId": "08c7d15a-b708-4eec-b56b-13945e3b3213",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "torch.manual_seed(123)\n",
        "transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transform)\n",
        "dataset2 = datasets.MNIST('../data', train=False,\n",
        "                       transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=256 , shuffle=True,\n",
        "        num_workers=1,pin_memory=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=256 , shuffle=True,\n",
        "        num_workers=1,pin_memory=True)\n",
        "\n",
        "model = Net().to(device)\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=0.01)\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.1 )\n",
        "for epoch in range(1, 20 + 1):\n",
        "  train(10, model, device, train_loader, optimizer, epoch)\n",
        "  test(model, device, test_loader)\n",
        "  scheduler.step()\n",
        "\n",
        "torch.save(model.state_dict(), \"mnist_1.pt\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.395790\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 1.134064\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.779052\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.663981\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.574635\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.552791\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.471058\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.451472\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.393252\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.381005\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.357808\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.335644\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.302188\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.321290\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.285373\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.339933\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.240519\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.238403\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.257119\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.215105\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.241381\n",
            "Train Epoch: 1 [53760/60000 (89%)]\tLoss: 0.190403\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.239746\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.248817\n",
            "\n",
            "Test set: Average loss: 0.2006, Accuracy: 9662/10000 (97%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.206920\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.189404\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.171547\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.221273\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.186894\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.165258\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.193421\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.183515\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.157949\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.190085\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.190016\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.165090\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.169343\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.157645\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.175037\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.117876\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.157343\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.139666\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.150223\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.122695\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.117551\n",
            "Train Epoch: 2 [53760/60000 (89%)]\tLoss: 0.108700\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.150954\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.115706\n",
            "\n",
            "Test set: Average loss: 0.1255, Accuracy: 9770/10000 (98%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.131208\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.108149\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.154623\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.090112\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.122972\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.114416\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.131132\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.160107\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.117419\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.124526\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.125485\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.077389\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.118114\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.093600\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.113652\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.126516\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.081542\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.106661\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.083685\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.150431\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.119742\n",
            "Train Epoch: 3 [53760/60000 (89%)]\tLoss: 0.103090\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.094042\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.100122\n",
            "\n",
            "Test set: Average loss: 0.0989, Accuracy: 9830/10000 (98%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.093156\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.082595\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.125696\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.104668\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.076412\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.107616\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.106090\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.077866\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.116579\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.100052\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.071124\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.095139\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.093916\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.091857\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.093732\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.099923\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.052305\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.064531\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.075087\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.110868\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.082949\n",
            "Train Epoch: 4 [53760/60000 (89%)]\tLoss: 0.085105\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.073221\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.077264\n",
            "\n",
            "Test set: Average loss: 0.0801, Accuracy: 9838/10000 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.086069\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.074684\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.065143\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.078423\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.057676\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.049223\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.092220\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.115553\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.077263\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.046807\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.068677\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.069005\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.096313\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.058845\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.111996\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.063004\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.075965\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.060485\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.072566\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.057608\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.055495\n",
            "Train Epoch: 5 [53760/60000 (89%)]\tLoss: 0.067099\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.066835\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.066756\n",
            "\n",
            "Test set: Average loss: 0.0707, Accuracy: 9855/10000 (99%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.054883\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.061365\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.068802\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.057256\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.071479\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.053130\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.048699\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.051491\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.050020\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.067763\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.058634\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.057822\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.049456\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.082797\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.064431\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.057911\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.067124\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.077434\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.043501\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.068218\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.060712\n",
            "Train Epoch: 6 [53760/60000 (89%)]\tLoss: 0.077792\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.068315\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.093634\n",
            "\n",
            "Test set: Average loss: 0.0672, Accuracy: 9852/10000 (99%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.065452\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.064101\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.052399\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.069717\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.053650\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.072419\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.057385\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.077426\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.058250\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.064460\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.060868\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.062978\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.085469\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.063625\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.062681\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.085541\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.060106\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.050141\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.039349\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.047468\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.060320\n",
            "Train Epoch: 7 [53760/60000 (89%)]\tLoss: 0.079065\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.049315\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.068381\n",
            "\n",
            "Test set: Average loss: 0.0665, Accuracy: 9860/10000 (99%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.051552\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.047691\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.082664\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.079730\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.044611\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.056127\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.054671\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.064213\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.058745\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.070762\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.090208\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.069766\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.059048\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.058331\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.069231\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.064114\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.065990\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.059891\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.061688\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.056103\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.056240\n",
            "Train Epoch: 8 [53760/60000 (89%)]\tLoss: 0.062920\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.056936\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.060251\n",
            "\n",
            "Test set: Average loss: 0.0655, Accuracy: 9855/10000 (99%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.058940\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.093741\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.052454\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.057154\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.052600\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.047801\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.094425\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.058572\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.043663\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.050293\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.056591\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.050645\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.091778\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.064580\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.043308\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.032858\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.048154\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.037283\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.086880\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.038995\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.042114\n",
            "Train Epoch: 9 [53760/60000 (89%)]\tLoss: 0.059791\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.071112\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.057565\n",
            "\n",
            "Test set: Average loss: 0.0657, Accuracy: 9857/10000 (99%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.064900\n",
            "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.057912\n",
            "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.060286\n",
            "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.063007\n",
            "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.057027\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.055557\n",
            "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.050144\n",
            "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.059873\n",
            "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.045807\n",
            "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.062198\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.058595\n",
            "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.058466\n",
            "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.058363\n",
            "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.045327\n",
            "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.076072\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.045757\n",
            "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.059558\n",
            "Train Epoch: 10 [43520/60000 (72%)]\tLoss: 0.052281\n",
            "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.057252\n",
            "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.057053\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.045692\n",
            "Train Epoch: 10 [53760/60000 (89%)]\tLoss: 0.042688\n",
            "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.080519\n",
            "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.051296\n",
            "\n",
            "Test set: Average loss: 0.0640, Accuracy: 9856/10000 (99%)\n",
            "\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.031104\n",
            "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 0.065405\n",
            "Train Epoch: 11 [5120/60000 (9%)]\tLoss: 0.058292\n",
            "Train Epoch: 11 [7680/60000 (13%)]\tLoss: 0.049757\n",
            "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 0.040565\n",
            "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.044154\n",
            "Train Epoch: 11 [15360/60000 (26%)]\tLoss: 0.059984\n",
            "Train Epoch: 11 [17920/60000 (30%)]\tLoss: 0.091407\n",
            "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.084724\n",
            "Train Epoch: 11 [23040/60000 (38%)]\tLoss: 0.043786\n",
            "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.042564\n",
            "Train Epoch: 11 [28160/60000 (47%)]\tLoss: 0.039403\n",
            "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.062772\n",
            "Train Epoch: 11 [33280/60000 (55%)]\tLoss: 0.049353\n",
            "Train Epoch: 11 [35840/60000 (60%)]\tLoss: 0.062747\n",
            "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.065449\n",
            "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 0.053027\n",
            "Train Epoch: 11 [43520/60000 (72%)]\tLoss: 0.079420\n",
            "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 0.053549\n",
            "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 0.067425\n",
            "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.072152\n",
            "Train Epoch: 11 [53760/60000 (89%)]\tLoss: 0.050820\n",
            "Train Epoch: 11 [56320/60000 (94%)]\tLoss: 0.080361\n",
            "Train Epoch: 11 [58880/60000 (98%)]\tLoss: 0.039286\n",
            "\n",
            "Test set: Average loss: 0.0643, Accuracy: 9861/10000 (99%)\n",
            "\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.056286\n",
            "Train Epoch: 12 [2560/60000 (4%)]\tLoss: 0.040439\n",
            "Train Epoch: 12 [5120/60000 (9%)]\tLoss: 0.065647\n",
            "Train Epoch: 12 [7680/60000 (13%)]\tLoss: 0.077438\n",
            "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 0.073786\n",
            "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.044747\n",
            "Train Epoch: 12 [15360/60000 (26%)]\tLoss: 0.056677\n",
            "Train Epoch: 12 [17920/60000 (30%)]\tLoss: 0.053786\n",
            "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 0.079561\n",
            "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 0.053605\n",
            "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.056930\n",
            "Train Epoch: 12 [28160/60000 (47%)]\tLoss: 0.059053\n",
            "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 0.054034\n",
            "Train Epoch: 12 [33280/60000 (55%)]\tLoss: 0.045762\n",
            "Train Epoch: 12 [35840/60000 (60%)]\tLoss: 0.065986\n",
            "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.044086\n",
            "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.058402\n",
            "Train Epoch: 12 [43520/60000 (72%)]\tLoss: 0.052621\n",
            "Train Epoch: 12 [46080/60000 (77%)]\tLoss: 0.050598\n",
            "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 0.034317\n",
            "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.064572\n",
            "Train Epoch: 12 [53760/60000 (89%)]\tLoss: 0.052484\n",
            "Train Epoch: 12 [56320/60000 (94%)]\tLoss: 0.053605\n",
            "Train Epoch: 12 [58880/60000 (98%)]\tLoss: 0.060799\n",
            "\n",
            "Test set: Average loss: 0.0644, Accuracy: 9855/10000 (99%)\n",
            "\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.079180\n",
            "Train Epoch: 13 [2560/60000 (4%)]\tLoss: 0.077252\n",
            "Train Epoch: 13 [5120/60000 (9%)]\tLoss: 0.055583\n",
            "Train Epoch: 13 [7680/60000 (13%)]\tLoss: 0.050417\n",
            "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 0.058683\n",
            "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.066989\n",
            "Train Epoch: 13 [15360/60000 (26%)]\tLoss: 0.054489\n",
            "Train Epoch: 13 [17920/60000 (30%)]\tLoss: 0.054732\n",
            "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.051740\n",
            "Train Epoch: 13 [23040/60000 (38%)]\tLoss: 0.046715\n",
            "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.066875\n",
            "Train Epoch: 13 [28160/60000 (47%)]\tLoss: 0.052065\n",
            "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 0.051800\n",
            "Train Epoch: 13 [33280/60000 (55%)]\tLoss: 0.083797\n",
            "Train Epoch: 13 [35840/60000 (60%)]\tLoss: 0.076824\n",
            "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.057879\n",
            "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.061740\n",
            "Train Epoch: 13 [43520/60000 (72%)]\tLoss: 0.041054\n",
            "Train Epoch: 13 [46080/60000 (77%)]\tLoss: 0.057577\n",
            "Train Epoch: 13 [48640/60000 (81%)]\tLoss: 0.062648\n",
            "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.066999\n",
            "Train Epoch: 13 [53760/60000 (89%)]\tLoss: 0.081010\n",
            "Train Epoch: 13 [56320/60000 (94%)]\tLoss: 0.048548\n",
            "Train Epoch: 13 [58880/60000 (98%)]\tLoss: 0.044277\n",
            "\n",
            "Test set: Average loss: 0.0640, Accuracy: 9860/10000 (99%)\n",
            "\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.031239\n",
            "Train Epoch: 14 [2560/60000 (4%)]\tLoss: 0.058763\n",
            "Train Epoch: 14 [5120/60000 (9%)]\tLoss: 0.050583\n",
            "Train Epoch: 14 [7680/60000 (13%)]\tLoss: 0.046541\n",
            "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 0.054239\n",
            "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.052724\n",
            "Train Epoch: 14 [15360/60000 (26%)]\tLoss: 0.044460\n",
            "Train Epoch: 14 [17920/60000 (30%)]\tLoss: 0.046010\n",
            "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 0.047021\n",
            "Train Epoch: 14 [23040/60000 (38%)]\tLoss: 0.039932\n",
            "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.048639\n",
            "Train Epoch: 14 [28160/60000 (47%)]\tLoss: 0.062120\n",
            "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 0.046071\n",
            "Train Epoch: 14 [33280/60000 (55%)]\tLoss: 0.048740\n",
            "Train Epoch: 14 [35840/60000 (60%)]\tLoss: 0.048509\n",
            "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.049384\n",
            "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 0.053017\n",
            "Train Epoch: 14 [43520/60000 (72%)]\tLoss: 0.056550\n",
            "Train Epoch: 14 [46080/60000 (77%)]\tLoss: 0.067560\n",
            "Train Epoch: 14 [48640/60000 (81%)]\tLoss: 0.076382\n",
            "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.060523\n",
            "Train Epoch: 14 [53760/60000 (89%)]\tLoss: 0.057193\n",
            "Train Epoch: 14 [56320/60000 (94%)]\tLoss: 0.041614\n",
            "Train Epoch: 14 [58880/60000 (98%)]\tLoss: 0.037696\n",
            "\n",
            "Test set: Average loss: 0.0641, Accuracy: 9857/10000 (99%)\n",
            "\n",
            "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.068153\n",
            "Train Epoch: 15 [2560/60000 (4%)]\tLoss: 0.071288\n",
            "Train Epoch: 15 [5120/60000 (9%)]\tLoss: 0.056808\n",
            "Train Epoch: 15 [7680/60000 (13%)]\tLoss: 0.046975\n",
            "Train Epoch: 15 [10240/60000 (17%)]\tLoss: 0.075562\n",
            "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.053647\n",
            "Train Epoch: 15 [15360/60000 (26%)]\tLoss: 0.051741\n",
            "Train Epoch: 15 [17920/60000 (30%)]\tLoss: 0.084471\n",
            "Train Epoch: 15 [20480/60000 (34%)]\tLoss: 0.060888\n",
            "Train Epoch: 15 [23040/60000 (38%)]\tLoss: 0.071815\n",
            "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.054318\n",
            "Train Epoch: 15 [28160/60000 (47%)]\tLoss: 0.052369\n",
            "Train Epoch: 15 [30720/60000 (51%)]\tLoss: 0.061346\n",
            "Train Epoch: 15 [33280/60000 (55%)]\tLoss: 0.108100\n",
            "Train Epoch: 15 [35840/60000 (60%)]\tLoss: 0.033932\n",
            "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.066887\n",
            "Train Epoch: 15 [40960/60000 (68%)]\tLoss: 0.059484\n",
            "Train Epoch: 15 [43520/60000 (72%)]\tLoss: 0.036673\n",
            "Train Epoch: 15 [46080/60000 (77%)]\tLoss: 0.066610\n",
            "Train Epoch: 15 [48640/60000 (81%)]\tLoss: 0.049840\n",
            "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.073179\n",
            "Train Epoch: 15 [53760/60000 (89%)]\tLoss: 0.065204\n",
            "Train Epoch: 15 [56320/60000 (94%)]\tLoss: 0.046945\n",
            "Train Epoch: 15 [58880/60000 (98%)]\tLoss: 0.044948\n",
            "\n",
            "Test set: Average loss: 0.0639, Accuracy: 9857/10000 (99%)\n",
            "\n",
            "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.071477\n",
            "Train Epoch: 16 [2560/60000 (4%)]\tLoss: 0.065289\n",
            "Train Epoch: 16 [5120/60000 (9%)]\tLoss: 0.049568\n",
            "Train Epoch: 16 [7680/60000 (13%)]\tLoss: 0.059168\n",
            "Train Epoch: 16 [10240/60000 (17%)]\tLoss: 0.054835\n",
            "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 0.046911\n",
            "Train Epoch: 16 [15360/60000 (26%)]\tLoss: 0.042538\n",
            "Train Epoch: 16 [17920/60000 (30%)]\tLoss: 0.040333\n",
            "Train Epoch: 16 [20480/60000 (34%)]\tLoss: 0.085392\n",
            "Train Epoch: 16 [23040/60000 (38%)]\tLoss: 0.059875\n",
            "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.070260\n",
            "Train Epoch: 16 [28160/60000 (47%)]\tLoss: 0.052904\n",
            "Train Epoch: 16 [30720/60000 (51%)]\tLoss: 0.049977\n",
            "Train Epoch: 16 [33280/60000 (55%)]\tLoss: 0.073565\n",
            "Train Epoch: 16 [35840/60000 (60%)]\tLoss: 0.065444\n",
            "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 0.057821\n",
            "Train Epoch: 16 [40960/60000 (68%)]\tLoss: 0.038117\n",
            "Train Epoch: 16 [43520/60000 (72%)]\tLoss: 0.058979\n",
            "Train Epoch: 16 [46080/60000 (77%)]\tLoss: 0.070319\n",
            "Train Epoch: 16 [48640/60000 (81%)]\tLoss: 0.049676\n",
            "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.061028\n",
            "Train Epoch: 16 [53760/60000 (89%)]\tLoss: 0.050538\n",
            "Train Epoch: 16 [56320/60000 (94%)]\tLoss: 0.046238\n",
            "Train Epoch: 16 [58880/60000 (98%)]\tLoss: 0.072415\n",
            "\n",
            "Test set: Average loss: 0.0639, Accuracy: 9860/10000 (99%)\n",
            "\n",
            "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.060573\n",
            "Train Epoch: 17 [2560/60000 (4%)]\tLoss: 0.061722\n",
            "Train Epoch: 17 [5120/60000 (9%)]\tLoss: 0.044516\n",
            "Train Epoch: 17 [7680/60000 (13%)]\tLoss: 0.055954\n",
            "Train Epoch: 17 [10240/60000 (17%)]\tLoss: 0.063256\n",
            "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 0.066129\n",
            "Train Epoch: 17 [15360/60000 (26%)]\tLoss: 0.058513\n",
            "Train Epoch: 17 [17920/60000 (30%)]\tLoss: 0.037232\n",
            "Train Epoch: 17 [20480/60000 (34%)]\tLoss: 0.057551\n",
            "Train Epoch: 17 [23040/60000 (38%)]\tLoss: 0.077470\n",
            "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.057171\n",
            "Train Epoch: 17 [28160/60000 (47%)]\tLoss: 0.042655\n",
            "Train Epoch: 17 [30720/60000 (51%)]\tLoss: 0.038431\n",
            "Train Epoch: 17 [33280/60000 (55%)]\tLoss: 0.045971\n",
            "Train Epoch: 17 [35840/60000 (60%)]\tLoss: 0.073755\n",
            "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 0.046978\n",
            "Train Epoch: 17 [40960/60000 (68%)]\tLoss: 0.048683\n",
            "Train Epoch: 17 [43520/60000 (72%)]\tLoss: 0.054836\n",
            "Train Epoch: 17 [46080/60000 (77%)]\tLoss: 0.036680\n",
            "Train Epoch: 17 [48640/60000 (81%)]\tLoss: 0.054062\n",
            "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.052111\n",
            "Train Epoch: 17 [53760/60000 (89%)]\tLoss: 0.057817\n",
            "Train Epoch: 17 [56320/60000 (94%)]\tLoss: 0.054934\n",
            "Train Epoch: 17 [58880/60000 (98%)]\tLoss: 0.063519\n",
            "\n",
            "Test set: Average loss: 0.0643, Accuracy: 9856/10000 (99%)\n",
            "\n",
            "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.039967\n",
            "Train Epoch: 18 [2560/60000 (4%)]\tLoss: 0.048099\n",
            "Train Epoch: 18 [5120/60000 (9%)]\tLoss: 0.078730\n",
            "Train Epoch: 18 [7680/60000 (13%)]\tLoss: 0.072621\n",
            "Train Epoch: 18 [10240/60000 (17%)]\tLoss: 0.060292\n",
            "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 0.040808\n",
            "Train Epoch: 18 [15360/60000 (26%)]\tLoss: 0.065700\n",
            "Train Epoch: 18 [17920/60000 (30%)]\tLoss: 0.042919\n",
            "Train Epoch: 18 [20480/60000 (34%)]\tLoss: 0.056424\n",
            "Train Epoch: 18 [23040/60000 (38%)]\tLoss: 0.042554\n",
            "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.068281\n",
            "Train Epoch: 18 [28160/60000 (47%)]\tLoss: 0.040940\n",
            "Train Epoch: 18 [30720/60000 (51%)]\tLoss: 0.049647\n",
            "Train Epoch: 18 [33280/60000 (55%)]\tLoss: 0.062037\n",
            "Train Epoch: 18 [35840/60000 (60%)]\tLoss: 0.041845\n",
            "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 0.066512\n",
            "Train Epoch: 18 [40960/60000 (68%)]\tLoss: 0.042439\n",
            "Train Epoch: 18 [43520/60000 (72%)]\tLoss: 0.056358\n",
            "Train Epoch: 18 [46080/60000 (77%)]\tLoss: 0.058492\n",
            "Train Epoch: 18 [48640/60000 (81%)]\tLoss: 0.055566\n",
            "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.062570\n",
            "Train Epoch: 18 [53760/60000 (89%)]\tLoss: 0.059731\n",
            "Train Epoch: 18 [56320/60000 (94%)]\tLoss: 0.054748\n",
            "Train Epoch: 18 [58880/60000 (98%)]\tLoss: 0.047651\n",
            "\n",
            "Test set: Average loss: 0.0639, Accuracy: 9856/10000 (99%)\n",
            "\n",
            "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.049493\n",
            "Train Epoch: 19 [2560/60000 (4%)]\tLoss: 0.087600\n",
            "Train Epoch: 19 [5120/60000 (9%)]\tLoss: 0.071893\n",
            "Train Epoch: 19 [7680/60000 (13%)]\tLoss: 0.069451\n",
            "Train Epoch: 19 [10240/60000 (17%)]\tLoss: 0.059985\n",
            "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 0.068826\n",
            "Train Epoch: 19 [15360/60000 (26%)]\tLoss: 0.059336\n",
            "Train Epoch: 19 [17920/60000 (30%)]\tLoss: 0.081924\n",
            "Train Epoch: 19 [20480/60000 (34%)]\tLoss: 0.057606\n",
            "Train Epoch: 19 [23040/60000 (38%)]\tLoss: 0.066211\n",
            "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.054810\n",
            "Train Epoch: 19 [28160/60000 (47%)]\tLoss: 0.054040\n",
            "Train Epoch: 19 [30720/60000 (51%)]\tLoss: 0.061928\n",
            "Train Epoch: 19 [33280/60000 (55%)]\tLoss: 0.054731\n",
            "Train Epoch: 19 [35840/60000 (60%)]\tLoss: 0.045328\n",
            "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 0.047275\n",
            "Train Epoch: 19 [40960/60000 (68%)]\tLoss: 0.067061\n",
            "Train Epoch: 19 [43520/60000 (72%)]\tLoss: 0.066012\n",
            "Train Epoch: 19 [46080/60000 (77%)]\tLoss: 0.039524\n",
            "Train Epoch: 19 [48640/60000 (81%)]\tLoss: 0.046161\n",
            "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.034405\n",
            "Train Epoch: 19 [53760/60000 (89%)]\tLoss: 0.054048\n",
            "Train Epoch: 19 [56320/60000 (94%)]\tLoss: 0.040305\n",
            "Train Epoch: 19 [58880/60000 (98%)]\tLoss: 0.080379\n",
            "\n",
            "Test set: Average loss: 0.0639, Accuracy: 9855/10000 (99%)\n",
            "\n",
            "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.050505\n",
            "Train Epoch: 20 [2560/60000 (4%)]\tLoss: 0.048210\n",
            "Train Epoch: 20 [5120/60000 (9%)]\tLoss: 0.064338\n",
            "Train Epoch: 20 [7680/60000 (13%)]\tLoss: 0.052437\n",
            "Train Epoch: 20 [10240/60000 (17%)]\tLoss: 0.058879\n",
            "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 0.039822\n",
            "Train Epoch: 20 [15360/60000 (26%)]\tLoss: 0.051404\n",
            "Train Epoch: 20 [17920/60000 (30%)]\tLoss: 0.058111\n",
            "Train Epoch: 20 [20480/60000 (34%)]\tLoss: 0.073651\n",
            "Train Epoch: 20 [23040/60000 (38%)]\tLoss: 0.077301\n",
            "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.037893\n",
            "Train Epoch: 20 [28160/60000 (47%)]\tLoss: 0.046219\n",
            "Train Epoch: 20 [30720/60000 (51%)]\tLoss: 0.062742\n",
            "Train Epoch: 20 [33280/60000 (55%)]\tLoss: 0.057058\n",
            "Train Epoch: 20 [35840/60000 (60%)]\tLoss: 0.054197\n",
            "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 0.041319\n",
            "Train Epoch: 20 [40960/60000 (68%)]\tLoss: 0.047877\n",
            "Train Epoch: 20 [43520/60000 (72%)]\tLoss: 0.055089\n",
            "Train Epoch: 20 [46080/60000 (77%)]\tLoss: 0.050689\n",
            "Train Epoch: 20 [48640/60000 (81%)]\tLoss: 0.051005\n",
            "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.047645\n",
            "Train Epoch: 20 [53760/60000 (89%)]\tLoss: 0.057759\n",
            "Train Epoch: 20 [56320/60000 (94%)]\tLoss: 0.051174\n",
            "Train Epoch: 20 [58880/60000 (98%)]\tLoss: 0.053294\n",
            "\n",
            "Test set: Average loss: 0.0639, Accuracy: 9854/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}